{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EiBVUr0dbnzX",
        "r9W86xBtDpVV",
        "p8DlXvbjcuJg",
        "xNizvLRTyJEL",
        "cnmgIneSEeHh",
        "7iC_d-tS0hm0",
        "jLc6fLFlFRrP",
        "2BOyke0IGXPk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiBVUr0dbnzX"
      },
      "source": [
        "# set-up the enviroment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPongHmUcNgd"
      },
      "source": [
        "download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I5qE_eEjK48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd7fab7-da5c-4542-da75-b603a197012a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jH6nYkkcpwSyELOe-U3JgOOdA4mGHiJx\n",
            "To: /content/data.csv\n",
            "100% 8.31M/8.31M [00:00<00:00, 62.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1jH6nYkkcpwSyELOe-U3JgOOdA4mGHiJx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDywhhGgb8Hr"
      },
      "source": [
        "set seed for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0bxD0TRNdKk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(1)\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ_6YTP8cnc4"
      },
      "source": [
        "read the csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNysE1eZk41e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data exploration"
      ],
      "metadata": {
        "id": "r9W86xBtDpVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "display the first few rows of the dataset"
      ],
      "metadata": {
        "id": "E7WjgPusOwwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "uh62-9oHDvMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "basic statistics of numerical columns"
      ],
      "metadata": {
        "id": "HrubJ_nwO1Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "id": "bRZHZuh0O8Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "info about the dataset, including data types and missing values"
      ],
      "metadata": {
        "id": "UO0qU2K_P3US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "YaTcqK3qP0ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explore categorical variables"
      ],
      "metadata": {
        "id": "jPSUyY6yQMMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# omit OCCUPATION_TYPE because the charts gets messy\n",
        "categorical_cols = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE',\n",
        "                    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE']\n",
        "\n",
        "\n",
        "for col in categorical_cols:\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    sns.countplot(x=col, data=df, hue='status', palette='viridis')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4hFwTpbMQJlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "it can be easily seen that class \"0\" is the predominant one. An address of unbalanced classes"
      ],
      "metadata": {
        "id": "xh-jUL5VRnJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "explore numerical columns"
      ],
      "metadata": {
        "id": "Inth6xhkRx7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'CNT_FAM_MEMBERS']\n",
        "\n",
        "# Set up a grid of subplots\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(numerical_cols), figsize=(18, 5))\n",
        "fig.suptitle('Distribution of Numerical Columns')\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    # Plot histograms for numerical columns\n",
        "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to prevent title overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qkBPwJYUS2mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap for numerical columns\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Columns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zOhd5coHTwf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Countplot for the target variable \"status\"\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='status', data=df, palette='viridis')\n",
        "plt.title('Distribution of Target Variable (status)')\n",
        "plt.xlabel('Status')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g59IH8jqT9Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned before, we have a strong imbalance of classes"
      ],
      "metadata": {
        "id": "o9PpJqsOUAKN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DlXvbjcuJg"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1OXDOy1dGYy"
      },
      "source": [
        "replace the NAs from *OCCUPATION_TYPE* column and drop *ID* column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htRvfK2Qm412"
      },
      "outputs": [],
      "source": [
        "df['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].fillna('jobless')\n",
        "\n",
        "df = df.drop('ID', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6l1fqEydTRM"
      },
      "source": [
        "make all labels to predict, numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C80dbTCNqLE_"
      },
      "outputs": [],
      "source": [
        "status_replacement = {'X': 7, 'C': 6}\n",
        "df['status'] = df['status'].replace(status_replacement).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtRQesw7dX7Y"
      },
      "source": [
        "one encode categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AvrWSsqrV_u"
      },
      "outputs": [],
      "source": [
        "columns_to_one_encode = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "df = pd.get_dummies(df, columns=columns_to_one_encode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXM7sQQEy2G"
      },
      "source": [
        "create new features (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ7Bq7dzEx7f"
      },
      "outputs": [],
      "source": [
        "# division by zero is not handle because there is always at least 1 member in a family\n",
        "#df['income_per_family'] = df['AMT_INCOME_TOTAL']/df['CNT_FAM_MEMBERS']\n",
        "\n",
        "#df['total_balance_children'] = df['income_per_family'] * (1+(df['CNT_CHILDREN']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fTJ-vY0dnAw"
      },
      "source": [
        "divide the dataset into training/testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-xr_W4SEiqx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df['status'].to_numpy()\n",
        "x = df.loc[:, df.columns != 'status']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJiKZIX_zIDp"
      },
      "source": [
        "standardize numerical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWFbn3hKzHka"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "columns_to_standardize = ['AMT_INCOME_TOTAL', 'CNT_FAM_MEMBERS', 'CNT_CHILDREN', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train[columns_to_standardize] = scaler.fit_transform(x_train[columns_to_standardize])\n",
        "\n",
        "x_test[columns_to_standardize] = scaler.transform(x_test[columns_to_standardize])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNizvLRTyJEL"
      },
      "source": [
        "# oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhuSl4ur52Kz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Find unique values and their counts\n",
        "unique_labels, counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "# Sort the labels based on their counts\n",
        "sorted_indices = np.argsort(-counts)  # Sort in descending order\n",
        "sorted_labels = unique_labels[sorted_indices]\n",
        "sorted_counts = counts[sorted_indices]\n",
        "\n",
        "# Undersample the majority class to the second majority class\n",
        "#undersampler = RandomUnderSampler(sampling_strategy={sorted_labels[0]:sorted_counts[1]}, random_state=42)\n",
        "#x_train, y_train = undersampler.fit_resample(x_train, y_train)\n",
        "\n",
        "# Oversample\n",
        "smote = SMOTE(sampling_strategy={sorted_labels[4]:sorted_counts[3],sorted_labels[5]:sorted_counts[3],sorted_labels[6]:sorted_counts[3],sorted_labels[7]:sorted_counts[3]},random_state=42)\n",
        "x_train, y_train = smote.fit_resample(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLviGuTK1uM-"
      },
      "source": [
        "print the occurrences for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlJsu-hx0pqi",
        "outputId": "aa8f3581-39c2-4c4a-c958-7c0c442004b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 41752 occurrences\n",
            "1: 5141 occurrences\n",
            "2: 1457 occurrences\n",
            "3: 1457 occurrences\n",
            "4: 1457 occurrences\n",
            "5: 1457 occurrences\n",
            "6: 1457 occurrences\n",
            "7: 4629 occurrences\n"
          ]
        }
      ],
      "source": [
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"{value}: {count} occurrences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEBJtNRMsI4r"
      },
      "source": [
        "let's see the percentage of each class in the test dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NikgjN1YsOIG",
        "outputId": "6b17269c-8297-469c-8ed1-78cf9ebf5ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: Count = 41752, Percentage = 71.00%\n",
            "Class 1: Count = 5141, Percentage = 8.74%\n",
            "Class 2: Count = 1457, Percentage = 2.48%\n",
            "Class 3: Count = 1457, Percentage = 2.48%\n",
            "Class 4: Count = 1457, Percentage = 2.48%\n",
            "Class 5: Count = 1457, Percentage = 2.48%\n",
            "Class 6: Count = 1457, Percentage = 2.48%\n",
            "Class 7: Count = 4629, Percentage = 7.87%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get unique values and their counts\n",
        "unique_values, counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "# Calculate the percentage of each class\n",
        "total_instances = len(y_train)\n",
        "percentage_per_class = counts / total_instances * 100\n",
        "\n",
        "# Display the results\n",
        "for value, count, percentage in zip(unique_values, counts, percentage_per_class):\n",
        "    print(f\"Class {value}: Count = {count}, Percentage = {percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnmgIneSEeHh"
      },
      "source": [
        "# cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gWdjdY0Ei2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b47b18-431b-4ce2-f21c-ed542d891f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n",
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
            "[CV 1/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=  11.2s\n",
            "[CV 2/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   8.6s\n",
            "[CV 3/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   9.4s\n",
            "[CV 4/10] END batch_size=512, model__dropout=0.1;, score=0.709 total time=   8.9s\n",
            "[CV 5/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   6.2s\n",
            "[CV 6/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   7.6s\n",
            "[CV 7/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   8.7s\n",
            "[CV 8/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=  10.1s\n",
            "[CV 9/10] END batch_size=512, model__dropout=0.1;, score=0.710 total time=   9.6s\n",
            "[CV 10/10] END batch_size=512, model__dropout=0.1;, score=0.701 total time=   7.2s\n",
            "[CV 1/10] END batch_size=512, model__dropout=0.2;, score=0.709 total time=   6.2s\n",
            "[CV 2/10] END batch_size=512, model__dropout=0.2;, score=0.709 total time=   8.2s\n",
            "[CV 3/10] END batch_size=512, model__dropout=0.2;, score=0.710 total time=  10.3s\n",
            "[CV 4/10] END batch_size=512, model__dropout=0.2;, score=0.708 total time=  15.9s\n",
            "[CV 5/10] END batch_size=512, model__dropout=0.2;, score=0.710 total time=  13.9s\n",
            "[CV 6/10] END batch_size=512, model__dropout=0.2;, score=0.709 total time=  10.6s\n",
            "[CV 7/10] END batch_size=512, model__dropout=0.2;, score=0.650 total time=  11.5s\n",
            "[CV 8/10] END batch_size=512, model__dropout=0.2;, score=0.555 total time=  12.0s\n",
            "[CV 9/10] END batch_size=512, model__dropout=0.2;, score=0.709 total time=  10.5s\n",
            "[CV 10/10] END batch_size=512, model__dropout=0.2;, score=0.710 total time=   8.0s\n",
            "[CV 1/10] END batch_size=512, model__dropout=0.3;, score=0.682 total time=  11.9s\n",
            "[CV 2/10] END batch_size=512, model__dropout=0.3;, score=0.696 total time=  10.2s\n",
            "[CV 3/10] END batch_size=512, model__dropout=0.3;, score=0.709 total time=   6.4s\n",
            "[CV 4/10] END batch_size=512, model__dropout=0.3;, score=0.710 total time=  13.8s\n",
            "[CV 5/10] END batch_size=512, model__dropout=0.3;, score=0.709 total time=  12.3s\n",
            "[CV 6/10] END batch_size=512, model__dropout=0.3;, score=0.709 total time=   7.4s\n",
            "[CV 7/10] END batch_size=512, model__dropout=0.3;, score=0.709 total time=   7.2s\n",
            "[CV 8/10] END batch_size=512, model__dropout=0.3;, score=0.710 total time=   7.2s\n",
            "[CV 9/10] END batch_size=512, model__dropout=0.3;, score=0.710 total time=   6.2s\n",
            "[CV 10/10] END batch_size=512, model__dropout=0.3;, score=0.710 total time=   8.5s\n",
            "[CV 1/10] END batch_size=1024, model__dropout=0.1;, score=0.691 total time=   5.9s\n",
            "[CV 2/10] END batch_size=1024, model__dropout=0.1;, score=0.694 total time=   9.1s\n",
            "[CV 3/10] END batch_size=1024, model__dropout=0.1;, score=0.710 total time=   5.9s\n",
            "[CV 4/10] END batch_size=1024, model__dropout=0.1;, score=0.710 total time=   6.6s\n",
            "[CV 5/10] END batch_size=1024, model__dropout=0.1;, score=0.710 total time=  12.8s\n",
            "[CV 6/10] END batch_size=1024, model__dropout=0.1;, score=0.584 total time=  10.0s\n",
            "[CV 7/10] END batch_size=1024, model__dropout=0.1;, score=0.645 total time=   7.8s\n",
            "[CV 8/10] END batch_size=1024, model__dropout=0.1;, score=0.709 total time=  11.9s\n",
            "[CV 9/10] END batch_size=1024, model__dropout=0.1;, score=0.709 total time=   6.8s\n",
            "[CV 10/10] END batch_size=1024, model__dropout=0.1;, score=0.710 total time=   6.0s\n",
            "[CV 1/10] END batch_size=1024, model__dropout=0.2;, score=0.710 total time=   6.7s\n",
            "[CV 2/10] END batch_size=1024, model__dropout=0.2;, score=0.710 total time=   5.6s\n",
            "[CV 3/10] END batch_size=1024, model__dropout=0.2;, score=0.709 total time=   8.5s\n",
            "[CV 4/10] END batch_size=1024, model__dropout=0.2;, score=0.701 total time=   5.9s\n",
            "[CV 5/10] END batch_size=1024, model__dropout=0.2;, score=0.710 total time=   7.9s\n",
            "[CV 6/10] END batch_size=1024, model__dropout=0.2;, score=0.710 total time=  11.5s\n",
            "[CV 7/10] END batch_size=1024, model__dropout=0.2;, score=0.514 total time=  11.6s\n",
            "[CV 8/10] END batch_size=1024, model__dropout=0.2;, score=0.707 total time=   5.7s\n",
            "[CV 9/10] END batch_size=1024, model__dropout=0.2;, score=0.362 total time=   8.4s\n",
            "[CV 10/10] END batch_size=1024, model__dropout=0.2;, score=0.583 total time=   6.0s\n",
            "[CV 1/10] END batch_size=1024, model__dropout=0.3;, score=0.636 total time=   7.3s\n",
            "[CV 2/10] END batch_size=1024, model__dropout=0.3;, score=0.678 total time=   9.8s\n",
            "[CV 3/10] END batch_size=1024, model__dropout=0.3;, score=0.319 total time=   5.7s\n",
            "[CV 4/10] END batch_size=1024, model__dropout=0.3;, score=0.505 total time=   6.7s\n",
            "[CV 5/10] END batch_size=1024, model__dropout=0.3;, score=0.710 total time=   6.0s\n",
            "[CV 6/10] END batch_size=1024, model__dropout=0.3;, score=0.710 total time=   8.9s\n",
            "[CV 7/10] END batch_size=1024, model__dropout=0.3;, score=0.710 total time=  10.2s\n",
            "[CV 8/10] END batch_size=1024, model__dropout=0.3;, score=0.710 total time=   7.6s\n",
            "[CV 9/10] END batch_size=1024, model__dropout=0.3;, score=0.119 total time=   9.1s\n",
            "[CV 10/10] END batch_size=1024, model__dropout=0.3;, score=0.502 total time=   8.7s\n",
            "[CV 1/10] END batch_size=2048, model__dropout=0.1;, score=0.687 total time=   7.8s\n",
            "[CV 2/10] END batch_size=2048, model__dropout=0.1;, score=0.708 total time=   8.8s\n",
            "[CV 3/10] END batch_size=2048, model__dropout=0.1;, score=0.709 total time=   5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d70aac9f250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/10] END batch_size=2048, model__dropout=0.1;, score=0.710 total time=   6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d7087cd7a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/10] END batch_size=2048, model__dropout=0.1;, score=0.405 total time=   5.4s\n",
            "[CV 6/10] END batch_size=2048, model__dropout=0.1;, score=0.709 total time=   7.4s\n",
            "[CV 7/10] END batch_size=2048, model__dropout=0.1;, score=0.220 total time=   5.5s\n",
            "[CV 8/10] END batch_size=2048, model__dropout=0.1;, score=0.663 total time=   6.4s\n",
            "[CV 9/10] END batch_size=2048, model__dropout=0.1;, score=0.709 total time=   5.9s\n",
            "[CV 10/10] END batch_size=2048, model__dropout=0.1;, score=0.669 total time=   6.5s\n",
            "[CV 1/10] END batch_size=2048, model__dropout=0.2;, score=0.710 total time=   7.7s\n",
            "[CV 2/10] END batch_size=2048, model__dropout=0.2;, score=0.709 total time=   6.3s\n",
            "[CV 3/10] END batch_size=2048, model__dropout=0.2;, score=0.709 total time=   6.0s\n",
            "[CV 4/10] END batch_size=2048, model__dropout=0.2;, score=0.027 total time=   6.3s\n",
            "[CV 5/10] END batch_size=2048, model__dropout=0.2;, score=0.709 total time=  13.6s\n",
            "[CV 6/10] END batch_size=2048, model__dropout=0.2;, score=0.707 total time=   5.4s\n",
            "[CV 7/10] END batch_size=2048, model__dropout=0.2;, score=0.032 total time=   6.6s\n",
            "[CV 8/10] END batch_size=2048, model__dropout=0.2;, score=0.706 total time=   5.4s\n",
            "[CV 9/10] END batch_size=2048, model__dropout=0.2;, score=0.666 total time=   6.5s\n",
            "[CV 10/10] END batch_size=2048, model__dropout=0.2;, score=0.709 total time=   5.5s\n",
            "[CV 1/10] END batch_size=2048, model__dropout=0.3;, score=0.707 total time=   8.6s\n",
            "[CV 2/10] END batch_size=2048, model__dropout=0.3;, score=0.710 total time=   5.9s\n",
            "[CV 3/10] END batch_size=2048, model__dropout=0.3;, score=0.121 total time=   6.5s\n",
            "[CV 4/10] END batch_size=2048, model__dropout=0.3;, score=0.680 total time=   5.5s\n",
            "[CV 5/10] END batch_size=2048, model__dropout=0.3;, score=0.709 total time=   6.6s\n",
            "[CV 6/10] END batch_size=2048, model__dropout=0.3;, score=0.710 total time=   6.0s\n",
            "[CV 7/10] END batch_size=2048, model__dropout=0.3;, score=0.209 total time=   8.7s\n",
            "[CV 8/10] END batch_size=2048, model__dropout=0.3;, score=0.708 total time=  12.6s\n",
            "[CV 9/10] END batch_size=2048, model__dropout=0.3;, score=0.710 total time=   5.6s\n",
            "[CV 10/10] END batch_size=2048, model__dropout=0.3;, score=0.710 total time=   6.7s\n",
            "Best Hyperparameters: {'batch_size': 512, 'model__dropout': 0.1}\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Define the function to create the model\n",
        "def create_model(dropout):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=256, activation='relu', input_dim=x_train.shape[1]))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=16, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=8, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a KerasClassifier for use with GridSearchCV\n",
        "model = KerasClassifier(model=create_model, epochs=1000, verbose=4)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'batch_size': [512,1024,2048],\n",
        "    'model__dropout': [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold for cross-validation if dealing with imbalanced classes\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=cv,verbose = 4)\n",
        "\n",
        "# Perform the grid search on the data\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape[1])"
      ],
      "metadata": {
        "id": "HqHZxPhyfkG1",
        "outputId": "725d6e8a-c728-4631-b687-9bbf03e5ef0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iC_d-tS0hm0"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VYA_Kfl0eB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fb8bde-a164-4a77-dee7-ff6526fcbc08"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 2.1688 - accuracy: 0.2939\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71433, saving model to best_weights.h5\n",
            "92/92 [==============================] - 7s 17ms/step - loss: 2.1628 - accuracy: 0.2976 - val_loss: 1.6065 - val_accuracy: 0.7143\n",
            "Epoch 2/15000\n",
            " 5/92 [>.............................] - ETA: 1s - loss: 1.8336 - accuracy: 0.4949"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2933 - accuracy: 0.8992 - val_loss: 0.4955 - val_accuracy: 0.8612\n",
            "Epoch 4665/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9008\n",
            "Epoch 4665: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2892 - accuracy: 0.9006 - val_loss: 0.4988 - val_accuracy: 0.8604\n",
            "Epoch 4666/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8984\n",
            "Epoch 4666: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2940 - accuracy: 0.8987 - val_loss: 0.4989 - val_accuracy: 0.8609\n",
            "Epoch 4667/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9031\n",
            "Epoch 4667: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2870 - accuracy: 0.9027 - val_loss: 0.5017 - val_accuracy: 0.8619\n",
            "Epoch 4668/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9013\n",
            "Epoch 4668: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2895 - accuracy: 0.9013 - val_loss: 0.5028 - val_accuracy: 0.8611\n",
            "Epoch 4669/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9010\n",
            "Epoch 4669: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2932 - accuracy: 0.9012 - val_loss: 0.4942 - val_accuracy: 0.8626\n",
            "Epoch 4670/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9021\n",
            "Epoch 4670: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2863 - accuracy: 0.9021 - val_loss: 0.4996 - val_accuracy: 0.8624\n",
            "Epoch 4671/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9019\n",
            "Epoch 4671: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2911 - accuracy: 0.9017 - val_loss: 0.4972 - val_accuracy: 0.8624\n",
            "Epoch 4672/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8985\n",
            "Epoch 4672: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2953 - accuracy: 0.8986 - val_loss: 0.4966 - val_accuracy: 0.8612\n",
            "Epoch 4673/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9005\n",
            "Epoch 4673: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2895 - accuracy: 0.9005 - val_loss: 0.4966 - val_accuracy: 0.8611\n",
            "Epoch 4674/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9002\n",
            "Epoch 4674: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2894 - accuracy: 0.9002 - val_loss: 0.5011 - val_accuracy: 0.8629\n",
            "Epoch 4675/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8995\n",
            "Epoch 4675: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2899 - accuracy: 0.8995 - val_loss: 0.5063 - val_accuracy: 0.8600\n",
            "Epoch 4676/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9000\n",
            "Epoch 4676: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2910 - accuracy: 0.9001 - val_loss: 0.4972 - val_accuracy: 0.8597\n",
            "Epoch 4677/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9018\n",
            "Epoch 4677: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2854 - accuracy: 0.9019 - val_loss: 0.5042 - val_accuracy: 0.8594\n",
            "Epoch 4678/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9017\n",
            "Epoch 4678: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2938 - accuracy: 0.9016 - val_loss: 0.4937 - val_accuracy: 0.8631\n",
            "Epoch 4679/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9013\n",
            "Epoch 4679: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2893 - accuracy: 0.9011 - val_loss: 0.4969 - val_accuracy: 0.8624\n",
            "Epoch 4680/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9008\n",
            "Epoch 4680: val_accuracy did not improve from 0.86405\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2899 - accuracy: 0.9008 - val_loss: 0.4970 - val_accuracy: 0.8628\n",
            "Epoch 4681/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8986\n",
            "Epoch 4681: val_accuracy improved from 0.86405 to 0.86465, saving model to best_weights.h5\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2938 - accuracy: 0.8985 - val_loss: 0.4885 - val_accuracy: 0.8646\n",
            "Epoch 4682/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.9005\n",
            "Epoch 4682: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2931 - accuracy: 0.9006 - val_loss: 0.4947 - val_accuracy: 0.8617\n",
            "Epoch 4683/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9006\n",
            "Epoch 4683: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2919 - accuracy: 0.9006 - val_loss: 0.4917 - val_accuracy: 0.8621\n",
            "Epoch 4684/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9016\n",
            "Epoch 4684: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2915 - accuracy: 0.9015 - val_loss: 0.4988 - val_accuracy: 0.8610\n",
            "Epoch 4685/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8998\n",
            "Epoch 4685: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2933 - accuracy: 0.9000 - val_loss: 0.4945 - val_accuracy: 0.8618\n",
            "Epoch 4686/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9015\n",
            "Epoch 4686: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9016 - val_loss: 0.4926 - val_accuracy: 0.8628\n",
            "Epoch 4687/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9007\n",
            "Epoch 4687: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2900 - accuracy: 0.9007 - val_loss: 0.4989 - val_accuracy: 0.8643\n",
            "Epoch 4688/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9013\n",
            "Epoch 4688: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9012 - val_loss: 0.4945 - val_accuracy: 0.8628\n",
            "Epoch 4689/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9015\n",
            "Epoch 4689: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2878 - accuracy: 0.9015 - val_loss: 0.4977 - val_accuracy: 0.8635\n",
            "Epoch 4690/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8987\n",
            "Epoch 4690: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2941 - accuracy: 0.8987 - val_loss: 0.4978 - val_accuracy: 0.8631\n",
            "Epoch 4691/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9020\n",
            "Epoch 4691: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2869 - accuracy: 0.9019 - val_loss: 0.4950 - val_accuracy: 0.8643\n",
            "Epoch 4692/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8992\n",
            "Epoch 4692: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2927 - accuracy: 0.8992 - val_loss: 0.4937 - val_accuracy: 0.8627\n",
            "Epoch 4693/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.9001\n",
            "Epoch 4693: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2926 - accuracy: 0.9001 - val_loss: 0.4910 - val_accuracy: 0.8644\n",
            "Epoch 4694/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.9008\n",
            "Epoch 4694: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2920 - accuracy: 0.9008 - val_loss: 0.4993 - val_accuracy: 0.8641\n",
            "Epoch 4695/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8986\n",
            "Epoch 4695: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2947 - accuracy: 0.8985 - val_loss: 0.4965 - val_accuracy: 0.8628\n",
            "Epoch 4696/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9001\n",
            "Epoch 4696: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2948 - accuracy: 0.8998 - val_loss: 0.4941 - val_accuracy: 0.8601\n",
            "Epoch 4697/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.9023\n",
            "Epoch 4697: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9022 - val_loss: 0.4971 - val_accuracy: 0.8610\n",
            "Epoch 4698/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8982\n",
            "Epoch 4698: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2944 - accuracy: 0.8983 - val_loss: 0.4949 - val_accuracy: 0.8618\n",
            "Epoch 4699/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9004\n",
            "Epoch 4699: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2911 - accuracy: 0.9004 - val_loss: 0.4943 - val_accuracy: 0.8600\n",
            "Epoch 4700/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9019\n",
            "Epoch 4700: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2882 - accuracy: 0.9019 - val_loss: 0.4892 - val_accuracy: 0.8601\n",
            "Epoch 4701/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8978\n",
            "Epoch 4701: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2949 - accuracy: 0.8978 - val_loss: 0.4890 - val_accuracy: 0.8616\n",
            "Epoch 4702/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8990\n",
            "Epoch 4702: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2922 - accuracy: 0.8990 - val_loss: 0.4960 - val_accuracy: 0.8612\n",
            "Epoch 4703/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9015\n",
            "Epoch 4703: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2868 - accuracy: 0.9015 - val_loss: 0.4936 - val_accuracy: 0.8624\n",
            "Epoch 4704/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9019\n",
            "Epoch 4704: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2887 - accuracy: 0.9021 - val_loss: 0.4930 - val_accuracy: 0.8638\n",
            "Epoch 4705/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8993\n",
            "Epoch 4705: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2917 - accuracy: 0.8996 - val_loss: 0.4957 - val_accuracy: 0.8638\n",
            "Epoch 4706/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9018\n",
            "Epoch 4706: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2871 - accuracy: 0.9017 - val_loss: 0.5003 - val_accuracy: 0.8627\n",
            "Epoch 4707/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9001\n",
            "Epoch 4707: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2934 - accuracy: 0.9001 - val_loss: 0.4989 - val_accuracy: 0.8601\n",
            "Epoch 4708/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8993\n",
            "Epoch 4708: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2949 - accuracy: 0.8993 - val_loss: 0.4961 - val_accuracy: 0.8631\n",
            "Epoch 4709/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9009\n",
            "Epoch 4709: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2916 - accuracy: 0.9005 - val_loss: 0.4934 - val_accuracy: 0.8625\n",
            "Epoch 4710/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9020\n",
            "Epoch 4710: val_accuracy did not improve from 0.86465\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9019 - val_loss: 0.4973 - val_accuracy: 0.8624\n",
            "Epoch 4711/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9001\n",
            "Epoch 4711: val_accuracy improved from 0.86465 to 0.86499, saving model to best_weights.h5\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2911 - accuracy: 0.9001 - val_loss: 0.4939 - val_accuracy: 0.8650\n",
            "Epoch 4712/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9002\n",
            "Epoch 4712: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2924 - accuracy: 0.9002 - val_loss: 0.4888 - val_accuracy: 0.8635\n",
            "Epoch 4713/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9006\n",
            "Epoch 4713: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2920 - accuracy: 0.9007 - val_loss: 0.4946 - val_accuracy: 0.8626\n",
            "Epoch 4714/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9001\n",
            "Epoch 4714: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2903 - accuracy: 0.8999 - val_loss: 0.4912 - val_accuracy: 0.8629\n",
            "Epoch 4715/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9007\n",
            "Epoch 4715: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2886 - accuracy: 0.9006 - val_loss: 0.4948 - val_accuracy: 0.8618\n",
            "Epoch 4716/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9017\n",
            "Epoch 4716: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9017 - val_loss: 0.4925 - val_accuracy: 0.8635\n",
            "Epoch 4717/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8998\n",
            "Epoch 4717: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2932 - accuracy: 0.8998 - val_loss: 0.4942 - val_accuracy: 0.8618\n",
            "Epoch 4718/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8983\n",
            "Epoch 4718: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2923 - accuracy: 0.8986 - val_loss: 0.4980 - val_accuracy: 0.8628\n",
            "Epoch 4719/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9003\n",
            "Epoch 4719: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2925 - accuracy: 0.9003 - val_loss: 0.4937 - val_accuracy: 0.8601\n",
            "Epoch 4720/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9009\n",
            "Epoch 4720: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.9012 - val_loss: 0.5007 - val_accuracy: 0.8618\n",
            "Epoch 4721/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8993\n",
            "Epoch 4721: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2938 - accuracy: 0.8990 - val_loss: 0.4884 - val_accuracy: 0.8634\n",
            "Epoch 4722/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9006\n",
            "Epoch 4722: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2876 - accuracy: 0.9007 - val_loss: 0.4916 - val_accuracy: 0.8649\n",
            "Epoch 4723/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8985\n",
            "Epoch 4723: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2908 - accuracy: 0.8987 - val_loss: 0.4979 - val_accuracy: 0.8628\n",
            "Epoch 4724/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9010\n",
            "Epoch 4724: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9009 - val_loss: 0.4938 - val_accuracy: 0.8620\n",
            "Epoch 4725/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9012\n",
            "Epoch 4725: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2872 - accuracy: 0.9013 - val_loss: 0.4992 - val_accuracy: 0.8642\n",
            "Epoch 4726/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9011\n",
            "Epoch 4726: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2898 - accuracy: 0.9011 - val_loss: 0.4932 - val_accuracy: 0.8645\n",
            "Epoch 4727/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8999\n",
            "Epoch 4727: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2926 - accuracy: 0.8999 - val_loss: 0.4935 - val_accuracy: 0.8628\n",
            "Epoch 4728/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.8998\n",
            "Epoch 4728: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2905 - accuracy: 0.8998 - val_loss: 0.5060 - val_accuracy: 0.8613\n",
            "Epoch 4729/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9017\n",
            "Epoch 4729: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2886 - accuracy: 0.9017 - val_loss: 0.4995 - val_accuracy: 0.8628\n",
            "Epoch 4730/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9006\n",
            "Epoch 4730: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2919 - accuracy: 0.9006 - val_loss: 0.4933 - val_accuracy: 0.8623\n",
            "Epoch 4731/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9003\n",
            "Epoch 4731: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.9003 - val_loss: 0.4968 - val_accuracy: 0.8624\n",
            "Epoch 4732/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9013\n",
            "Epoch 4732: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9011 - val_loss: 0.5006 - val_accuracy: 0.8628\n",
            "Epoch 4733/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8986\n",
            "Epoch 4733: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2931 - accuracy: 0.8985 - val_loss: 0.4928 - val_accuracy: 0.8612\n",
            "Epoch 4734/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9006\n",
            "Epoch 4734: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2943 - accuracy: 0.9006 - val_loss: 0.4984 - val_accuracy: 0.8616\n",
            "Epoch 4735/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.8999\n",
            "Epoch 4735: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2895 - accuracy: 0.8999 - val_loss: 0.4957 - val_accuracy: 0.8620\n",
            "Epoch 4736/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9004\n",
            "Epoch 4736: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2924 - accuracy: 0.9003 - val_loss: 0.4953 - val_accuracy: 0.8612\n",
            "Epoch 4737/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8999\n",
            "Epoch 4737: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2950 - accuracy: 0.9001 - val_loss: 0.4924 - val_accuracy: 0.8611\n",
            "Epoch 4738/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8994\n",
            "Epoch 4738: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2906 - accuracy: 0.8993 - val_loss: 0.4958 - val_accuracy: 0.8629\n",
            "Epoch 4739/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8998\n",
            "Epoch 4739: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2897 - accuracy: 0.9000 - val_loss: 0.4984 - val_accuracy: 0.8625\n",
            "Epoch 4740/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9013\n",
            "Epoch 4740: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2904 - accuracy: 0.9013 - val_loss: 0.4973 - val_accuracy: 0.8624\n",
            "Epoch 4741/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.8996\n",
            "Epoch 4741: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2870 - accuracy: 0.9007 - val_loss: 0.4952 - val_accuracy: 0.8610\n",
            "Epoch 4742/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9003\n",
            "Epoch 4742: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.9002 - val_loss: 0.4937 - val_accuracy: 0.8638\n",
            "Epoch 4743/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9019\n",
            "Epoch 4743: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9019 - val_loss: 0.4957 - val_accuracy: 0.8624\n",
            "Epoch 4744/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9008\n",
            "Epoch 4744: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9008 - val_loss: 0.4950 - val_accuracy: 0.8611\n",
            "Epoch 4745/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9001\n",
            "Epoch 4745: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.8999 - val_loss: 0.4942 - val_accuracy: 0.8633\n",
            "Epoch 4746/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9024\n",
            "Epoch 4746: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2883 - accuracy: 0.9021 - val_loss: 0.4958 - val_accuracy: 0.8604\n",
            "Epoch 4747/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9009\n",
            "Epoch 4747: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2924 - accuracy: 0.9009 - val_loss: 0.4898 - val_accuracy: 0.8605\n",
            "Epoch 4748/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9033\n",
            "Epoch 4748: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2872 - accuracy: 0.9032 - val_loss: 0.4952 - val_accuracy: 0.8629\n",
            "Epoch 4749/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8984\n",
            "Epoch 4749: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2942 - accuracy: 0.8984 - val_loss: 0.4978 - val_accuracy: 0.8626\n",
            "Epoch 4750/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9010\n",
            "Epoch 4750: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.9009 - val_loss: 0.4917 - val_accuracy: 0.8620\n",
            "Epoch 4751/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.8991\n",
            "Epoch 4751: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2935 - accuracy: 0.8991 - val_loss: 0.4909 - val_accuracy: 0.8615\n",
            "Epoch 4752/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9027\n",
            "Epoch 4752: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2865 - accuracy: 0.9027 - val_loss: 0.4997 - val_accuracy: 0.8618\n",
            "Epoch 4753/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9021\n",
            "Epoch 4753: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2893 - accuracy: 0.9022 - val_loss: 0.4929 - val_accuracy: 0.8619\n",
            "Epoch 4754/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9002\n",
            "Epoch 4754: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2891 - accuracy: 0.8999 - val_loss: 0.4935 - val_accuracy: 0.8622\n",
            "Epoch 4755/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8998\n",
            "Epoch 4755: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2902 - accuracy: 0.9000 - val_loss: 0.4972 - val_accuracy: 0.8626\n",
            "Epoch 4756/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9013\n",
            "Epoch 4756: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2902 - accuracy: 0.9013 - val_loss: 0.4911 - val_accuracy: 0.8626\n",
            "Epoch 4757/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8989\n",
            "Epoch 4757: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2953 - accuracy: 0.8990 - val_loss: 0.4950 - val_accuracy: 0.8615\n",
            "Epoch 4758/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8990\n",
            "Epoch 4758: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2981 - accuracy: 0.8988 - val_loss: 0.4941 - val_accuracy: 0.8618\n",
            "Epoch 4759/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9011\n",
            "Epoch 4759: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9014 - val_loss: 0.4981 - val_accuracy: 0.8621\n",
            "Epoch 4760/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9009\n",
            "Epoch 4760: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.9009 - val_loss: 0.4970 - val_accuracy: 0.8618\n",
            "Epoch 4761/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8992\n",
            "Epoch 4761: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2911 - accuracy: 0.8993 - val_loss: 0.4945 - val_accuracy: 0.8610\n",
            "Epoch 4762/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9007\n",
            "Epoch 4762: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9005 - val_loss: 0.4965 - val_accuracy: 0.8624\n",
            "Epoch 4763/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9014\n",
            "Epoch 4763: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2879 - accuracy: 0.9013 - val_loss: 0.5070 - val_accuracy: 0.8604\n",
            "Epoch 4764/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9030\n",
            "Epoch 4764: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2887 - accuracy: 0.9028 - val_loss: 0.4978 - val_accuracy: 0.8618\n",
            "Epoch 4765/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8992\n",
            "Epoch 4765: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2918 - accuracy: 0.8996 - val_loss: 0.5044 - val_accuracy: 0.8616\n",
            "Epoch 4766/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9016\n",
            "Epoch 4766: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2888 - accuracy: 0.9014 - val_loss: 0.5018 - val_accuracy: 0.8606\n",
            "Epoch 4767/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9010\n",
            "Epoch 4767: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2908 - accuracy: 0.9010 - val_loss: 0.4972 - val_accuracy: 0.8626\n",
            "Epoch 4768/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9000\n",
            "Epoch 4768: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2922 - accuracy: 0.9001 - val_loss: 0.4936 - val_accuracy: 0.8618\n",
            "Epoch 4769/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8996\n",
            "Epoch 4769: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2928 - accuracy: 0.8998 - val_loss: 0.4962 - val_accuracy: 0.8607\n",
            "Epoch 4770/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9023\n",
            "Epoch 4770: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2864 - accuracy: 0.9024 - val_loss: 0.4988 - val_accuracy: 0.8627\n",
            "Epoch 4771/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9007\n",
            "Epoch 4771: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2901 - accuracy: 0.9008 - val_loss: 0.4995 - val_accuracy: 0.8634\n",
            "Epoch 4772/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8986\n",
            "Epoch 4772: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2945 - accuracy: 0.8988 - val_loss: 0.4981 - val_accuracy: 0.8601\n",
            "Epoch 4773/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9003\n",
            "Epoch 4773: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2902 - accuracy: 0.9003 - val_loss: 0.4980 - val_accuracy: 0.8618\n",
            "Epoch 4774/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9022\n",
            "Epoch 4774: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.9021 - val_loss: 0.4985 - val_accuracy: 0.8624\n",
            "Epoch 4775/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9019\n",
            "Epoch 4775: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2841 - accuracy: 0.9019 - val_loss: 0.4975 - val_accuracy: 0.8615\n",
            "Epoch 4776/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.9007\n",
            "Epoch 4776: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2933 - accuracy: 0.9007 - val_loss: 0.4915 - val_accuracy: 0.8612\n",
            "Epoch 4777/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9019\n",
            "Epoch 4777: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2913 - accuracy: 0.9019 - val_loss: 0.4965 - val_accuracy: 0.8616\n",
            "Epoch 4778/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.9011\n",
            "Epoch 4778: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2947 - accuracy: 0.9011 - val_loss: 0.4933 - val_accuracy: 0.8621\n",
            "Epoch 4779/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9017\n",
            "Epoch 4779: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2873 - accuracy: 0.9018 - val_loss: 0.4928 - val_accuracy: 0.8617\n",
            "Epoch 4780/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9032\n",
            "Epoch 4780: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2853 - accuracy: 0.9032 - val_loss: 0.4966 - val_accuracy: 0.8616\n",
            "Epoch 4781/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.8994\n",
            "Epoch 4781: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2904 - accuracy: 0.8994 - val_loss: 0.4984 - val_accuracy: 0.8612\n",
            "Epoch 4782/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.8995\n",
            "Epoch 4782: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2899 - accuracy: 0.8995 - val_loss: 0.4967 - val_accuracy: 0.8597\n",
            "Epoch 4783/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9009\n",
            "Epoch 4783: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2901 - accuracy: 0.9009 - val_loss: 0.4995 - val_accuracy: 0.8601\n",
            "Epoch 4784/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9005\n",
            "Epoch 4784: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.8998 - val_loss: 0.4997 - val_accuracy: 0.8611\n",
            "Epoch 4785/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8995\n",
            "Epoch 4785: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2938 - accuracy: 0.8995 - val_loss: 0.4938 - val_accuracy: 0.8612\n",
            "Epoch 4786/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8995\n",
            "Epoch 4786: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2938 - accuracy: 0.8992 - val_loss: 0.4919 - val_accuracy: 0.8619\n",
            "Epoch 4787/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9005\n",
            "Epoch 4787: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2898 - accuracy: 0.9005 - val_loss: 0.4980 - val_accuracy: 0.8619\n",
            "Epoch 4788/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8996\n",
            "Epoch 4788: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2927 - accuracy: 0.8995 - val_loss: 0.4990 - val_accuracy: 0.8608\n",
            "Epoch 4789/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9022\n",
            "Epoch 4789: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2895 - accuracy: 0.9017 - val_loss: 0.4929 - val_accuracy: 0.8617\n",
            "Epoch 4790/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8998\n",
            "Epoch 4790: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2909 - accuracy: 0.8998 - val_loss: 0.4919 - val_accuracy: 0.8621\n",
            "Epoch 4791/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.8995\n",
            "Epoch 4791: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.8995 - val_loss: 0.5042 - val_accuracy: 0.8598\n",
            "Epoch 4792/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9003\n",
            "Epoch 4792: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9003 - val_loss: 0.4940 - val_accuracy: 0.8625\n",
            "Epoch 4793/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.9005\n",
            "Epoch 4793: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2907 - accuracy: 0.9005 - val_loss: 0.4974 - val_accuracy: 0.8601\n",
            "Epoch 4794/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.8979\n",
            "Epoch 4794: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2976 - accuracy: 0.8979 - val_loss: 0.4932 - val_accuracy: 0.8603\n",
            "Epoch 4795/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9008\n",
            "Epoch 4795: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2896 - accuracy: 0.9009 - val_loss: 0.4932 - val_accuracy: 0.8609\n",
            "Epoch 4796/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.8996\n",
            "Epoch 4796: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2945 - accuracy: 0.8996 - val_loss: 0.4954 - val_accuracy: 0.8608\n",
            "Epoch 4797/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8997\n",
            "Epoch 4797: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2925 - accuracy: 0.8999 - val_loss: 0.4989 - val_accuracy: 0.8626\n",
            "Epoch 4798/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9012\n",
            "Epoch 4798: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9011 - val_loss: 0.4927 - val_accuracy: 0.8624\n",
            "Epoch 4799/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9005\n",
            "Epoch 4799: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2941 - accuracy: 0.9005 - val_loss: 0.4959 - val_accuracy: 0.8621\n",
            "Epoch 4800/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9001\n",
            "Epoch 4800: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2923 - accuracy: 0.9000 - val_loss: 0.5002 - val_accuracy: 0.8621\n",
            "Epoch 4801/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8986\n",
            "Epoch 4801: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2964 - accuracy: 0.8987 - val_loss: 0.4979 - val_accuracy: 0.8625\n",
            "Epoch 4802/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.9014\n",
            "Epoch 4802: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2929 - accuracy: 0.9012 - val_loss: 0.4972 - val_accuracy: 0.8634\n",
            "Epoch 4803/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8997\n",
            "Epoch 4803: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2927 - accuracy: 0.8997 - val_loss: 0.4959 - val_accuracy: 0.8630\n",
            "Epoch 4804/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8987\n",
            "Epoch 4804: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2922 - accuracy: 0.8989 - val_loss: 0.4969 - val_accuracy: 0.8618\n",
            "Epoch 4805/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9013\n",
            "Epoch 4805: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2879 - accuracy: 0.9014 - val_loss: 0.4952 - val_accuracy: 0.8638\n",
            "Epoch 4806/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9021\n",
            "Epoch 4806: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2876 - accuracy: 0.9019 - val_loss: 0.5003 - val_accuracy: 0.8630\n",
            "Epoch 4807/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9008\n",
            "Epoch 4807: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2885 - accuracy: 0.9008 - val_loss: 0.4995 - val_accuracy: 0.8628\n",
            "Epoch 4808/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8993\n",
            "Epoch 4808: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2942 - accuracy: 0.8993 - val_loss: 0.4991 - val_accuracy: 0.8630\n",
            "Epoch 4809/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8995\n",
            "Epoch 4809: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2941 - accuracy: 0.8994 - val_loss: 0.4990 - val_accuracy: 0.8618\n",
            "Epoch 4810/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.9009\n",
            "Epoch 4810: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2944 - accuracy: 0.9009 - val_loss: 0.4979 - val_accuracy: 0.8631\n",
            "Epoch 4811/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9009\n",
            "Epoch 4811: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2910 - accuracy: 0.9009 - val_loss: 0.4937 - val_accuracy: 0.8633\n",
            "Epoch 4812/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9011\n",
            "Epoch 4812: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.9014 - val_loss: 0.4912 - val_accuracy: 0.8639\n",
            "Epoch 4813/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9009\n",
            "Epoch 4813: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9008 - val_loss: 0.4979 - val_accuracy: 0.8631\n",
            "Epoch 4814/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9005\n",
            "Epoch 4814: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2918 - accuracy: 0.9005 - val_loss: 0.4987 - val_accuracy: 0.8635\n",
            "Epoch 4815/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9012\n",
            "Epoch 4815: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2842 - accuracy: 0.9012 - val_loss: 0.4963 - val_accuracy: 0.8626\n",
            "Epoch 4816/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8993\n",
            "Epoch 4816: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2923 - accuracy: 0.8995 - val_loss: 0.5020 - val_accuracy: 0.8610\n",
            "Epoch 4817/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9011\n",
            "Epoch 4817: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.9010 - val_loss: 0.4931 - val_accuracy: 0.8620\n",
            "Epoch 4818/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8988\n",
            "Epoch 4818: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2905 - accuracy: 0.8986 - val_loss: 0.4991 - val_accuracy: 0.8611\n",
            "Epoch 4819/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9002\n",
            "Epoch 4819: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2906 - accuracy: 0.9005 - val_loss: 0.4997 - val_accuracy: 0.8628\n",
            "Epoch 4820/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8998\n",
            "Epoch 4820: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2943 - accuracy: 0.8996 - val_loss: 0.4969 - val_accuracy: 0.8614\n",
            "Epoch 4821/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.9012\n",
            "Epoch 4821: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2907 - accuracy: 0.9012 - val_loss: 0.5008 - val_accuracy: 0.8611\n",
            "Epoch 4822/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9020\n",
            "Epoch 4822: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2890 - accuracy: 0.9019 - val_loss: 0.4993 - val_accuracy: 0.8620\n",
            "Epoch 4823/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9006\n",
            "Epoch 4823: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2918 - accuracy: 0.9006 - val_loss: 0.4992 - val_accuracy: 0.8612\n",
            "Epoch 4824/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.8991\n",
            "Epoch 4824: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2886 - accuracy: 0.8992 - val_loss: 0.4968 - val_accuracy: 0.8618\n",
            "Epoch 4825/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8995\n",
            "Epoch 4825: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2951 - accuracy: 0.8995 - val_loss: 0.4952 - val_accuracy: 0.8624\n",
            "Epoch 4826/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9015\n",
            "Epoch 4826: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9014 - val_loss: 0.5024 - val_accuracy: 0.8636\n",
            "Epoch 4827/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.9042\n",
            "Epoch 4827: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2815 - accuracy: 0.9037 - val_loss: 0.5054 - val_accuracy: 0.8620\n",
            "Epoch 4828/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9014\n",
            "Epoch 4828: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2900 - accuracy: 0.9015 - val_loss: 0.5025 - val_accuracy: 0.8616\n",
            "Epoch 4829/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8993\n",
            "Epoch 4829: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2958 - accuracy: 0.8994 - val_loss: 0.4957 - val_accuracy: 0.8635\n",
            "Epoch 4830/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.9006\n",
            "Epoch 4830: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2916 - accuracy: 0.9007 - val_loss: 0.4978 - val_accuracy: 0.8629\n",
            "Epoch 4831/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8999\n",
            "Epoch 4831: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2922 - accuracy: 0.8997 - val_loss: 0.4974 - val_accuracy: 0.8627\n",
            "Epoch 4832/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8983\n",
            "Epoch 4832: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2942 - accuracy: 0.8983 - val_loss: 0.4952 - val_accuracy: 0.8630\n",
            "Epoch 4833/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9018\n",
            "Epoch 4833: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2907 - accuracy: 0.9016 - val_loss: 0.4940 - val_accuracy: 0.8637\n",
            "Epoch 4834/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.8999\n",
            "Epoch 4834: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2894 - accuracy: 0.8999 - val_loss: 0.4975 - val_accuracy: 0.8640\n",
            "Epoch 4835/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9006\n",
            "Epoch 4835: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2910 - accuracy: 0.9006 - val_loss: 0.4975 - val_accuracy: 0.8628\n",
            "Epoch 4836/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8998\n",
            "Epoch 4836: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2910 - accuracy: 0.8996 - val_loss: 0.4951 - val_accuracy: 0.8614\n",
            "Epoch 4837/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9012\n",
            "Epoch 4837: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2899 - accuracy: 0.9012 - val_loss: 0.5017 - val_accuracy: 0.8600\n",
            "Epoch 4838/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9012\n",
            "Epoch 4838: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2897 - accuracy: 0.9012 - val_loss: 0.5041 - val_accuracy: 0.8621\n",
            "Epoch 4839/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.8998\n",
            "Epoch 4839: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2921 - accuracy: 0.8998 - val_loss: 0.5005 - val_accuracy: 0.8624\n",
            "Epoch 4840/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9001\n",
            "Epoch 4840: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2912 - accuracy: 0.8998 - val_loss: 0.5003 - val_accuracy: 0.8593\n",
            "Epoch 4841/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9035\n",
            "Epoch 4841: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2861 - accuracy: 0.9038 - val_loss: 0.5035 - val_accuracy: 0.8612\n",
            "Epoch 4842/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9011\n",
            "Epoch 4842: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2870 - accuracy: 0.9013 - val_loss: 0.5013 - val_accuracy: 0.8617\n",
            "Epoch 4843/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9010\n",
            "Epoch 4843: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2878 - accuracy: 0.9009 - val_loss: 0.5107 - val_accuracy: 0.8596\n",
            "Epoch 4844/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8994\n",
            "Epoch 4844: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2916 - accuracy: 0.8992 - val_loss: 0.4964 - val_accuracy: 0.8612\n",
            "Epoch 4845/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.8999\n",
            "Epoch 4845: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2918 - accuracy: 0.8998 - val_loss: 0.4932 - val_accuracy: 0.8619\n",
            "Epoch 4846/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9011\n",
            "Epoch 4846: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2898 - accuracy: 0.9011 - val_loss: 0.4985 - val_accuracy: 0.8612\n",
            "Epoch 4847/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.9002\n",
            "Epoch 4847: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2930 - accuracy: 0.9005 - val_loss: 0.4983 - val_accuracy: 0.8594\n",
            "Epoch 4848/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9018\n",
            "Epoch 4848: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2916 - accuracy: 0.9013 - val_loss: 0.4991 - val_accuracy: 0.8607\n",
            "Epoch 4849/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9010\n",
            "Epoch 4849: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2890 - accuracy: 0.9011 - val_loss: 0.4956 - val_accuracy: 0.8626\n",
            "Epoch 4850/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9012\n",
            "Epoch 4850: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9012 - val_loss: 0.4988 - val_accuracy: 0.8614\n",
            "Epoch 4851/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8994\n",
            "Epoch 4851: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2928 - accuracy: 0.8994 - val_loss: 0.4991 - val_accuracy: 0.8617\n",
            "Epoch 4852/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9005\n",
            "Epoch 4852: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2900 - accuracy: 0.9006 - val_loss: 0.4964 - val_accuracy: 0.8598\n",
            "Epoch 4853/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9008\n",
            "Epoch 4853: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2918 - accuracy: 0.9009 - val_loss: 0.4923 - val_accuracy: 0.8622\n",
            "Epoch 4854/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8996\n",
            "Epoch 4854: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2879 - accuracy: 0.8996 - val_loss: 0.4978 - val_accuracy: 0.8618\n",
            "Epoch 4855/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9022\n",
            "Epoch 4855: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2897 - accuracy: 0.9021 - val_loss: 0.4924 - val_accuracy: 0.8623\n",
            "Epoch 4856/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9030\n",
            "Epoch 4856: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2898 - accuracy: 0.9029 - val_loss: 0.4916 - val_accuracy: 0.8612\n",
            "Epoch 4857/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8995\n",
            "Epoch 4857: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2926 - accuracy: 0.8997 - val_loss: 0.4914 - val_accuracy: 0.8600\n",
            "Epoch 4858/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9008\n",
            "Epoch 4858: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2923 - accuracy: 0.9008 - val_loss: 0.4989 - val_accuracy: 0.8598\n",
            "Epoch 4859/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9013\n",
            "Epoch 4859: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2880 - accuracy: 0.9011 - val_loss: 0.5026 - val_accuracy: 0.8608\n",
            "Epoch 4860/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8998\n",
            "Epoch 4860: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2946 - accuracy: 0.8998 - val_loss: 0.4961 - val_accuracy: 0.8602\n",
            "Epoch 4861/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9012\n",
            "Epoch 4861: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9012 - val_loss: 0.4986 - val_accuracy: 0.8628\n",
            "Epoch 4862/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8981\n",
            "Epoch 4862: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2940 - accuracy: 0.8981 - val_loss: 0.4963 - val_accuracy: 0.8622\n",
            "Epoch 4863/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9013\n",
            "Epoch 4863: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2896 - accuracy: 0.9013 - val_loss: 0.5031 - val_accuracy: 0.8613\n",
            "Epoch 4864/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9026\n",
            "Epoch 4864: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2876 - accuracy: 0.9028 - val_loss: 0.4972 - val_accuracy: 0.8610\n",
            "Epoch 4865/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9008\n",
            "Epoch 4865: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2916 - accuracy: 0.9009 - val_loss: 0.4925 - val_accuracy: 0.8618\n",
            "Epoch 4866/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9018\n",
            "Epoch 4866: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.9018 - val_loss: 0.4923 - val_accuracy: 0.8631\n",
            "Epoch 4867/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8996\n",
            "Epoch 4867: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2925 - accuracy: 0.8996 - val_loss: 0.4981 - val_accuracy: 0.8610\n",
            "Epoch 4868/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.9003\n",
            "Epoch 4868: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2927 - accuracy: 0.9003 - val_loss: 0.4971 - val_accuracy: 0.8620\n",
            "Epoch 4869/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9012\n",
            "Epoch 4869: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9011 - val_loss: 0.5027 - val_accuracy: 0.8634\n",
            "Epoch 4870/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9008\n",
            "Epoch 4870: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9007 - val_loss: 0.4996 - val_accuracy: 0.8620\n",
            "Epoch 4871/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9002\n",
            "Epoch 4871: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2922 - accuracy: 0.9002 - val_loss: 0.5031 - val_accuracy: 0.8607\n",
            "Epoch 4872/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9016\n",
            "Epoch 4872: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2887 - accuracy: 0.9016 - val_loss: 0.4989 - val_accuracy: 0.8619\n",
            "Epoch 4873/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9010\n",
            "Epoch 4873: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2932 - accuracy: 0.9010 - val_loss: 0.4931 - val_accuracy: 0.8609\n",
            "Epoch 4874/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9011\n",
            "Epoch 4874: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9005 - val_loss: 0.4917 - val_accuracy: 0.8624\n",
            "Epoch 4875/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9004\n",
            "Epoch 4875: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2891 - accuracy: 0.9004 - val_loss: 0.4967 - val_accuracy: 0.8608\n",
            "Epoch 4876/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.9002\n",
            "Epoch 4876: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2934 - accuracy: 0.9001 - val_loss: 0.5017 - val_accuracy: 0.8612\n",
            "Epoch 4877/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8988\n",
            "Epoch 4877: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2932 - accuracy: 0.8986 - val_loss: 0.4987 - val_accuracy: 0.8620\n",
            "Epoch 4878/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8999\n",
            "Epoch 4878: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2924 - accuracy: 0.8999 - val_loss: 0.4985 - val_accuracy: 0.8612\n",
            "Epoch 4879/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9005\n",
            "Epoch 4879: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2903 - accuracy: 0.9006 - val_loss: 0.5012 - val_accuracy: 0.8627\n",
            "Epoch 4880/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.8995\n",
            "Epoch 4880: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.8992 - val_loss: 0.4976 - val_accuracy: 0.8608\n",
            "Epoch 4881/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9003\n",
            "Epoch 4881: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2910 - accuracy: 0.9004 - val_loss: 0.5010 - val_accuracy: 0.8624\n",
            "Epoch 4882/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8972\n",
            "Epoch 4882: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2934 - accuracy: 0.8975 - val_loss: 0.4957 - val_accuracy: 0.8622\n",
            "Epoch 4883/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9019\n",
            "Epoch 4883: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2890 - accuracy: 0.9020 - val_loss: 0.4983 - val_accuracy: 0.8616\n",
            "Epoch 4884/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9010\n",
            "Epoch 4884: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9011 - val_loss: 0.5001 - val_accuracy: 0.8607\n",
            "Epoch 4885/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9000\n",
            "Epoch 4885: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.8999 - val_loss: 0.5006 - val_accuracy: 0.8617\n",
            "Epoch 4886/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8992\n",
            "Epoch 4886: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.8990 - val_loss: 0.5035 - val_accuracy: 0.8618\n",
            "Epoch 4887/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9001\n",
            "Epoch 4887: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9001 - val_loss: 0.4997 - val_accuracy: 0.8618\n",
            "Epoch 4888/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.9002\n",
            "Epoch 4888: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2896 - accuracy: 0.9006 - val_loss: 0.4965 - val_accuracy: 0.8633\n",
            "Epoch 4889/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.8997\n",
            "Epoch 4889: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.8997 - val_loss: 0.4939 - val_accuracy: 0.8616\n",
            "Epoch 4890/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.9015\n",
            "Epoch 4890: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2920 - accuracy: 0.9015 - val_loss: 0.5012 - val_accuracy: 0.8624\n",
            "Epoch 4891/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9023\n",
            "Epoch 4891: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2900 - accuracy: 0.9022 - val_loss: 0.5014 - val_accuracy: 0.8629\n",
            "Epoch 4892/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9014\n",
            "Epoch 4892: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2890 - accuracy: 0.9013 - val_loss: 0.4975 - val_accuracy: 0.8629\n",
            "Epoch 4893/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9005\n",
            "Epoch 4893: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2892 - accuracy: 0.9005 - val_loss: 0.4957 - val_accuracy: 0.8624\n",
            "Epoch 4894/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.9001\n",
            "Epoch 4894: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2936 - accuracy: 0.9002 - val_loss: 0.5028 - val_accuracy: 0.8612\n",
            "Epoch 4895/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8977\n",
            "Epoch 4895: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2959 - accuracy: 0.8977 - val_loss: 0.4960 - val_accuracy: 0.8625\n",
            "Epoch 4896/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9013\n",
            "Epoch 4896: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2896 - accuracy: 0.9013 - val_loss: 0.4985 - val_accuracy: 0.8607\n",
            "Epoch 4897/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9023\n",
            "Epoch 4897: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9022 - val_loss: 0.5024 - val_accuracy: 0.8601\n",
            "Epoch 4898/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.9006\n",
            "Epoch 4898: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2907 - accuracy: 0.9006 - val_loss: 0.4994 - val_accuracy: 0.8624\n",
            "Epoch 4899/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8992\n",
            "Epoch 4899: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2943 - accuracy: 0.8992 - val_loss: 0.4995 - val_accuracy: 0.8620\n",
            "Epoch 4900/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9013\n",
            "Epoch 4900: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2881 - accuracy: 0.9011 - val_loss: 0.5024 - val_accuracy: 0.8607\n",
            "Epoch 4901/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8998\n",
            "Epoch 4901: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2944 - accuracy: 0.8994 - val_loss: 0.4985 - val_accuracy: 0.8612\n",
            "Epoch 4902/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9024\n",
            "Epoch 4902: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2863 - accuracy: 0.9024 - val_loss: 0.5065 - val_accuracy: 0.8623\n",
            "Epoch 4903/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.9000\n",
            "Epoch 4903: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2934 - accuracy: 0.8998 - val_loss: 0.5041 - val_accuracy: 0.8600\n",
            "Epoch 4904/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9012\n",
            "Epoch 4904: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2925 - accuracy: 0.9011 - val_loss: 0.4947 - val_accuracy: 0.8635\n",
            "Epoch 4905/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9003\n",
            "Epoch 4905: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.9003 - val_loss: 0.4940 - val_accuracy: 0.8621\n",
            "Epoch 4906/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9021\n",
            "Epoch 4906: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.9019 - val_loss: 0.5046 - val_accuracy: 0.8614\n",
            "Epoch 4907/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9014\n",
            "Epoch 4907: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2871 - accuracy: 0.9012 - val_loss: 0.4981 - val_accuracy: 0.8616\n",
            "Epoch 4908/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.8991\n",
            "Epoch 4908: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2972 - accuracy: 0.8991 - val_loss: 0.4926 - val_accuracy: 0.8618\n",
            "Epoch 4909/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9031\n",
            "Epoch 4909: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.9028 - val_loss: 0.5080 - val_accuracy: 0.8623\n",
            "Epoch 4910/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9006\n",
            "Epoch 4910: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2874 - accuracy: 0.9006 - val_loss: 0.4957 - val_accuracy: 0.8626\n",
            "Epoch 4911/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9019\n",
            "Epoch 4911: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2882 - accuracy: 0.9019 - val_loss: 0.5012 - val_accuracy: 0.8605\n",
            "Epoch 4912/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8998\n",
            "Epoch 4912: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2924 - accuracy: 0.8997 - val_loss: 0.4925 - val_accuracy: 0.8620\n",
            "Epoch 4913/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9008\n",
            "Epoch 4913: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2902 - accuracy: 0.9006 - val_loss: 0.4954 - val_accuracy: 0.8614\n",
            "Epoch 4914/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9016\n",
            "Epoch 4914: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2853 - accuracy: 0.9018 - val_loss: 0.4967 - val_accuracy: 0.8624\n",
            "Epoch 4915/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9005\n",
            "Epoch 4915: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.9002 - val_loss: 0.5023 - val_accuracy: 0.8622\n",
            "Epoch 4916/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9014\n",
            "Epoch 4916: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2884 - accuracy: 0.9012 - val_loss: 0.4973 - val_accuracy: 0.8615\n",
            "Epoch 4917/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9028\n",
            "Epoch 4917: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2839 - accuracy: 0.9029 - val_loss: 0.5001 - val_accuracy: 0.8617\n",
            "Epoch 4918/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9019\n",
            "Epoch 4918: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.9018 - val_loss: 0.4999 - val_accuracy: 0.8633\n",
            "Epoch 4919/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9003\n",
            "Epoch 4919: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.9002 - val_loss: 0.4980 - val_accuracy: 0.8623\n",
            "Epoch 4920/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.8999\n",
            "Epoch 4920: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2916 - accuracy: 0.8999 - val_loss: 0.4951 - val_accuracy: 0.8612\n",
            "Epoch 4921/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9022\n",
            "Epoch 4921: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2894 - accuracy: 0.9022 - val_loss: 0.4991 - val_accuracy: 0.8607\n",
            "Epoch 4922/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8992\n",
            "Epoch 4922: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2951 - accuracy: 0.8992 - val_loss: 0.5007 - val_accuracy: 0.8608\n",
            "Epoch 4923/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8994\n",
            "Epoch 4923: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2923 - accuracy: 0.8994 - val_loss: 0.4929 - val_accuracy: 0.8619\n",
            "Epoch 4924/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9002\n",
            "Epoch 4924: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2930 - accuracy: 0.8999 - val_loss: 0.4919 - val_accuracy: 0.8608\n",
            "Epoch 4925/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9002\n",
            "Epoch 4925: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.9000 - val_loss: 0.4926 - val_accuracy: 0.8619\n",
            "Epoch 4926/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9005\n",
            "Epoch 4926: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2907 - accuracy: 0.9006 - val_loss: 0.4951 - val_accuracy: 0.8627\n",
            "Epoch 4927/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.8999\n",
            "Epoch 4927: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2890 - accuracy: 0.8999 - val_loss: 0.5020 - val_accuracy: 0.8609\n",
            "Epoch 4928/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8995\n",
            "Epoch 4928: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2926 - accuracy: 0.8996 - val_loss: 0.4980 - val_accuracy: 0.8631\n",
            "Epoch 4929/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8998\n",
            "Epoch 4929: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2914 - accuracy: 0.8998 - val_loss: 0.4919 - val_accuracy: 0.8614\n",
            "Epoch 4930/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9008\n",
            "Epoch 4930: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2893 - accuracy: 0.9008 - val_loss: 0.5024 - val_accuracy: 0.8599\n",
            "Epoch 4931/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9010\n",
            "Epoch 4931: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2940 - accuracy: 0.9010 - val_loss: 0.4981 - val_accuracy: 0.8599\n",
            "Epoch 4932/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.9015\n",
            "Epoch 4932: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2918 - accuracy: 0.9015 - val_loss: 0.4970 - val_accuracy: 0.8625\n",
            "Epoch 4933/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9018\n",
            "Epoch 4933: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2908 - accuracy: 0.9018 - val_loss: 0.4975 - val_accuracy: 0.8627\n",
            "Epoch 4934/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9022\n",
            "Epoch 4934: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2891 - accuracy: 0.9025 - val_loss: 0.4971 - val_accuracy: 0.8619\n",
            "Epoch 4935/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9025\n",
            "Epoch 4935: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2847 - accuracy: 0.9029 - val_loss: 0.4961 - val_accuracy: 0.8629\n",
            "Epoch 4936/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9026\n",
            "Epoch 4936: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2887 - accuracy: 0.9027 - val_loss: 0.4919 - val_accuracy: 0.8619\n",
            "Epoch 4937/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9023\n",
            "Epoch 4937: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2845 - accuracy: 0.9023 - val_loss: 0.4971 - val_accuracy: 0.8613\n",
            "Epoch 4938/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9008\n",
            "Epoch 4938: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2871 - accuracy: 0.9007 - val_loss: 0.5013 - val_accuracy: 0.8623\n",
            "Epoch 4939/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8983\n",
            "Epoch 4939: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2887 - accuracy: 0.8983 - val_loss: 0.5015 - val_accuracy: 0.8617\n",
            "Epoch 4940/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9009\n",
            "Epoch 4940: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2891 - accuracy: 0.9009 - val_loss: 0.4945 - val_accuracy: 0.8620\n",
            "Epoch 4941/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9017\n",
            "Epoch 4941: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2887 - accuracy: 0.9017 - val_loss: 0.4971 - val_accuracy: 0.8618\n",
            "Epoch 4942/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.8992\n",
            "Epoch 4942: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2907 - accuracy: 0.8994 - val_loss: 0.4959 - val_accuracy: 0.8616\n",
            "Epoch 4943/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8996\n",
            "Epoch 4943: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2913 - accuracy: 0.8996 - val_loss: 0.4949 - val_accuracy: 0.8617\n",
            "Epoch 4944/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9021\n",
            "Epoch 4944: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2870 - accuracy: 0.9020 - val_loss: 0.4977 - val_accuracy: 0.8622\n",
            "Epoch 4945/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9001\n",
            "Epoch 4945: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2911 - accuracy: 0.9001 - val_loss: 0.4992 - val_accuracy: 0.8624\n",
            "Epoch 4946/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8998\n",
            "Epoch 4946: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2917 - accuracy: 0.8999 - val_loss: 0.4962 - val_accuracy: 0.8624\n",
            "Epoch 4947/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9003\n",
            "Epoch 4947: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2911 - accuracy: 0.9003 - val_loss: 0.5028 - val_accuracy: 0.8593\n",
            "Epoch 4948/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8998\n",
            "Epoch 4948: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2921 - accuracy: 0.8999 - val_loss: 0.4959 - val_accuracy: 0.8610\n",
            "Epoch 4949/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9003\n",
            "Epoch 4949: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2904 - accuracy: 0.9000 - val_loss: 0.5040 - val_accuracy: 0.8605\n",
            "Epoch 4950/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.8995\n",
            "Epoch 4950: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2875 - accuracy: 0.8995 - val_loss: 0.5002 - val_accuracy: 0.8625\n",
            "Epoch 4951/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9029\n",
            "Epoch 4951: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2880 - accuracy: 0.9029 - val_loss: 0.5005 - val_accuracy: 0.8614\n",
            "Epoch 4952/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8987\n",
            "Epoch 4952: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2927 - accuracy: 0.8987 - val_loss: 0.5001 - val_accuracy: 0.8598\n",
            "Epoch 4953/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9020\n",
            "Epoch 4953: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2895 - accuracy: 0.9020 - val_loss: 0.4954 - val_accuracy: 0.8619\n",
            "Epoch 4954/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9015\n",
            "Epoch 4954: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2860 - accuracy: 0.9015 - val_loss: 0.4982 - val_accuracy: 0.8600\n",
            "Epoch 4955/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8988\n",
            "Epoch 4955: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2930 - accuracy: 0.8991 - val_loss: 0.4931 - val_accuracy: 0.8620\n",
            "Epoch 4956/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9015\n",
            "Epoch 4956: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2901 - accuracy: 0.9015 - val_loss: 0.4988 - val_accuracy: 0.8607\n",
            "Epoch 4957/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9031\n",
            "Epoch 4957: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2878 - accuracy: 0.9029 - val_loss: 0.5012 - val_accuracy: 0.8612\n",
            "Epoch 4958/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9020\n",
            "Epoch 4958: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.9014 - val_loss: 0.4947 - val_accuracy: 0.8629\n",
            "Epoch 4959/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9017\n",
            "Epoch 4959: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2906 - accuracy: 0.9017 - val_loss: 0.4941 - val_accuracy: 0.8628\n",
            "Epoch 4960/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.8992\n",
            "Epoch 4960: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2894 - accuracy: 0.8994 - val_loss: 0.4978 - val_accuracy: 0.8612\n",
            "Epoch 4961/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9033\n",
            "Epoch 4961: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2876 - accuracy: 0.9033 - val_loss: 0.4977 - val_accuracy: 0.8621\n",
            "Epoch 4962/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9013\n",
            "Epoch 4962: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2927 - accuracy: 0.9010 - val_loss: 0.4968 - val_accuracy: 0.8608\n",
            "Epoch 4963/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9008\n",
            "Epoch 4963: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2897 - accuracy: 0.9009 - val_loss: 0.4913 - val_accuracy: 0.8618\n",
            "Epoch 4964/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8997\n",
            "Epoch 4964: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2928 - accuracy: 0.8996 - val_loss: 0.4984 - val_accuracy: 0.8625\n",
            "Epoch 4965/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9026\n",
            "Epoch 4965: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2883 - accuracy: 0.9026 - val_loss: 0.4975 - val_accuracy: 0.8611\n",
            "Epoch 4966/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9015\n",
            "Epoch 4966: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2886 - accuracy: 0.9017 - val_loss: 0.4973 - val_accuracy: 0.8635\n",
            "Epoch 4967/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8994\n",
            "Epoch 4967: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.8996 - val_loss: 0.4996 - val_accuracy: 0.8614\n",
            "Epoch 4968/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9017\n",
            "Epoch 4968: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9018 - val_loss: 0.4993 - val_accuracy: 0.8601\n",
            "Epoch 4969/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9005\n",
            "Epoch 4969: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 23ms/step - loss: 0.2924 - accuracy: 0.9005 - val_loss: 0.4948 - val_accuracy: 0.8622\n",
            "Epoch 4970/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9018\n",
            "Epoch 4970: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2895 - accuracy: 0.9018 - val_loss: 0.4980 - val_accuracy: 0.8601\n",
            "Epoch 4971/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.9008\n",
            "Epoch 4971: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2927 - accuracy: 0.9008 - val_loss: 0.4949 - val_accuracy: 0.8601\n",
            "Epoch 4972/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9009\n",
            "Epoch 4972: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2901 - accuracy: 0.9010 - val_loss: 0.4963 - val_accuracy: 0.8615\n",
            "Epoch 4973/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9007\n",
            "Epoch 4973: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2886 - accuracy: 0.9006 - val_loss: 0.4972 - val_accuracy: 0.8595\n",
            "Epoch 4974/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9025\n",
            "Epoch 4974: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2868 - accuracy: 0.9026 - val_loss: 0.4982 - val_accuracy: 0.8624\n",
            "Epoch 4975/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9001\n",
            "Epoch 4975: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2933 - accuracy: 0.8997 - val_loss: 0.4884 - val_accuracy: 0.8627\n",
            "Epoch 4976/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9007\n",
            "Epoch 4976: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2908 - accuracy: 0.9007 - val_loss: 0.4972 - val_accuracy: 0.8621\n",
            "Epoch 4977/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9000\n",
            "Epoch 4977: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9003 - val_loss: 0.4986 - val_accuracy: 0.8578\n",
            "Epoch 4978/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9019\n",
            "Epoch 4978: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2900 - accuracy: 0.9015 - val_loss: 0.5007 - val_accuracy: 0.8609\n",
            "Epoch 4979/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9010\n",
            "Epoch 4979: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2917 - accuracy: 0.9012 - val_loss: 0.5008 - val_accuracy: 0.8601\n",
            "Epoch 4980/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9005\n",
            "Epoch 4980: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2906 - accuracy: 0.9005 - val_loss: 0.5001 - val_accuracy: 0.8615\n",
            "Epoch 4981/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9014\n",
            "Epoch 4981: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2926 - accuracy: 0.9010 - val_loss: 0.4974 - val_accuracy: 0.8615\n",
            "Epoch 4982/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9013\n",
            "Epoch 4982: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.9013 - val_loss: 0.4976 - val_accuracy: 0.8626\n",
            "Epoch 4983/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9006\n",
            "Epoch 4983: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2900 - accuracy: 0.9006 - val_loss: 0.5054 - val_accuracy: 0.8612\n",
            "Epoch 4984/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9017\n",
            "Epoch 4984: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2886 - accuracy: 0.9018 - val_loss: 0.4933 - val_accuracy: 0.8624\n",
            "Epoch 4985/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9018\n",
            "Epoch 4985: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9016 - val_loss: 0.5028 - val_accuracy: 0.8601\n",
            "Epoch 4986/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9028\n",
            "Epoch 4986: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2851 - accuracy: 0.9030 - val_loss: 0.4983 - val_accuracy: 0.8608\n",
            "Epoch 4987/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9016\n",
            "Epoch 4987: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2873 - accuracy: 0.9016 - val_loss: 0.5027 - val_accuracy: 0.8619\n",
            "Epoch 4988/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9001\n",
            "Epoch 4988: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2896 - accuracy: 0.9002 - val_loss: 0.5011 - val_accuracy: 0.8619\n",
            "Epoch 4989/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9005\n",
            "Epoch 4989: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2913 - accuracy: 0.9005 - val_loss: 0.4979 - val_accuracy: 0.8622\n",
            "Epoch 4990/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9009\n",
            "Epoch 4990: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2896 - accuracy: 0.9007 - val_loss: 0.4989 - val_accuracy: 0.8624\n",
            "Epoch 4991/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9005\n",
            "Epoch 4991: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9005 - val_loss: 0.4967 - val_accuracy: 0.8634\n",
            "Epoch 4992/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9022\n",
            "Epoch 4992: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9022 - val_loss: 0.5019 - val_accuracy: 0.8595\n",
            "Epoch 4993/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9019\n",
            "Epoch 4993: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2857 - accuracy: 0.9019 - val_loss: 0.4972 - val_accuracy: 0.8617\n",
            "Epoch 4994/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9003\n",
            "Epoch 4994: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9004 - val_loss: 0.5002 - val_accuracy: 0.8617\n",
            "Epoch 4995/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9010\n",
            "Epoch 4995: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2924 - accuracy: 0.9010 - val_loss: 0.5006 - val_accuracy: 0.8618\n",
            "Epoch 4996/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8988\n",
            "Epoch 4996: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2916 - accuracy: 0.8990 - val_loss: 0.4973 - val_accuracy: 0.8641\n",
            "Epoch 4997/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9018\n",
            "Epoch 4997: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.9019 - val_loss: 0.4961 - val_accuracy: 0.8630\n",
            "Epoch 4998/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9008\n",
            "Epoch 4998: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2936 - accuracy: 0.9007 - val_loss: 0.5034 - val_accuracy: 0.8592\n",
            "Epoch 4999/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8999\n",
            "Epoch 4999: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2924 - accuracy: 0.8996 - val_loss: 0.4960 - val_accuracy: 0.8618\n",
            "Epoch 5000/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9022\n",
            "Epoch 5000: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9027 - val_loss: 0.4934 - val_accuracy: 0.8613\n",
            "Epoch 5001/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9017\n",
            "Epoch 5001: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.9016 - val_loss: 0.4991 - val_accuracy: 0.8624\n",
            "Epoch 5002/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8997\n",
            "Epoch 5002: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2920 - accuracy: 0.8995 - val_loss: 0.4993 - val_accuracy: 0.8605\n",
            "Epoch 5003/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8977\n",
            "Epoch 5003: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2945 - accuracy: 0.8980 - val_loss: 0.4923 - val_accuracy: 0.8623\n",
            "Epoch 5004/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9026\n",
            "Epoch 5004: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9026 - val_loss: 0.5042 - val_accuracy: 0.8594\n",
            "Epoch 5005/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9006\n",
            "Epoch 5005: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9002 - val_loss: 0.4966 - val_accuracy: 0.8607\n",
            "Epoch 5006/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9019\n",
            "Epoch 5006: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2886 - accuracy: 0.9019 - val_loss: 0.5001 - val_accuracy: 0.8603\n",
            "Epoch 5007/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9019\n",
            "Epoch 5007: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2876 - accuracy: 0.9019 - val_loss: 0.5017 - val_accuracy: 0.8628\n",
            "Epoch 5008/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.9005\n",
            "Epoch 5008: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2938 - accuracy: 0.9005 - val_loss: 0.5009 - val_accuracy: 0.8631\n",
            "Epoch 5009/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9003\n",
            "Epoch 5009: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2897 - accuracy: 0.9003 - val_loss: 0.4964 - val_accuracy: 0.8623\n",
            "Epoch 5010/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9004\n",
            "Epoch 5010: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2899 - accuracy: 0.9004 - val_loss: 0.4951 - val_accuracy: 0.8624\n",
            "Epoch 5011/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8999\n",
            "Epoch 5011: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2926 - accuracy: 0.8998 - val_loss: 0.4871 - val_accuracy: 0.8627\n",
            "Epoch 5012/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9003\n",
            "Epoch 5012: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2892 - accuracy: 0.9003 - val_loss: 0.5006 - val_accuracy: 0.8625\n",
            "Epoch 5013/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2936 - accuracy: 0.8994\n",
            "Epoch 5013: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2936 - accuracy: 0.8994 - val_loss: 0.4908 - val_accuracy: 0.8615\n",
            "Epoch 5014/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9012\n",
            "Epoch 5014: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9012 - val_loss: 0.5008 - val_accuracy: 0.8620\n",
            "Epoch 5015/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9004\n",
            "Epoch 5015: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2893 - accuracy: 0.9004 - val_loss: 0.5011 - val_accuracy: 0.8613\n",
            "Epoch 5016/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9033\n",
            "Epoch 5016: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.9032 - val_loss: 0.4988 - val_accuracy: 0.8612\n",
            "Epoch 5017/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9011\n",
            "Epoch 5017: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2916 - accuracy: 0.9009 - val_loss: 0.4962 - val_accuracy: 0.8614\n",
            "Epoch 5018/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9005\n",
            "Epoch 5018: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2905 - accuracy: 0.9002 - val_loss: 0.4966 - val_accuracy: 0.8614\n",
            "Epoch 5019/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9009\n",
            "Epoch 5019: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2892 - accuracy: 0.9010 - val_loss: 0.4971 - val_accuracy: 0.8615\n",
            "Epoch 5020/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9011\n",
            "Epoch 5020: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2861 - accuracy: 0.9009 - val_loss: 0.5014 - val_accuracy: 0.8616\n",
            "Epoch 5021/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9000\n",
            "Epoch 5021: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2915 - accuracy: 0.9000 - val_loss: 0.4976 - val_accuracy: 0.8612\n",
            "Epoch 5022/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9003\n",
            "Epoch 5022: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9003 - val_loss: 0.4977 - val_accuracy: 0.8602\n",
            "Epoch 5023/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8991\n",
            "Epoch 5023: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2923 - accuracy: 0.8991 - val_loss: 0.4997 - val_accuracy: 0.8618\n",
            "Epoch 5024/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8997\n",
            "Epoch 5024: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.8997 - val_loss: 0.5021 - val_accuracy: 0.8584\n",
            "Epoch 5025/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9008\n",
            "Epoch 5025: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2916 - accuracy: 0.9010 - val_loss: 0.4977 - val_accuracy: 0.8611\n",
            "Epoch 5026/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9006\n",
            "Epoch 5026: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2899 - accuracy: 0.9006 - val_loss: 0.5034 - val_accuracy: 0.8618\n",
            "Epoch 5027/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9009\n",
            "Epoch 5027: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2909 - accuracy: 0.9005 - val_loss: 0.4939 - val_accuracy: 0.8619\n",
            "Epoch 5028/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.8999\n",
            "Epoch 5028: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2888 - accuracy: 0.8999 - val_loss: 0.4994 - val_accuracy: 0.8614\n",
            "Epoch 5029/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9008\n",
            "Epoch 5029: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2903 - accuracy: 0.9006 - val_loss: 0.4970 - val_accuracy: 0.8629\n",
            "Epoch 5030/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9023\n",
            "Epoch 5030: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2901 - accuracy: 0.9019 - val_loss: 0.4993 - val_accuracy: 0.8616\n",
            "Epoch 5031/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8992\n",
            "Epoch 5031: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2961 - accuracy: 0.8991 - val_loss: 0.4895 - val_accuracy: 0.8627\n",
            "Epoch 5032/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.9003\n",
            "Epoch 5032: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2869 - accuracy: 0.9003 - val_loss: 0.5020 - val_accuracy: 0.8614\n",
            "Epoch 5033/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9010\n",
            "Epoch 5033: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2916 - accuracy: 0.9009 - val_loss: 0.5041 - val_accuracy: 0.8607\n",
            "Epoch 5034/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9024\n",
            "Epoch 5034: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9024 - val_loss: 0.5011 - val_accuracy: 0.8599\n",
            "Epoch 5035/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9005\n",
            "Epoch 5035: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2932 - accuracy: 0.9005 - val_loss: 0.4978 - val_accuracy: 0.8625\n",
            "Epoch 5036/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9029\n",
            "Epoch 5036: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2855 - accuracy: 0.9029 - val_loss: 0.4997 - val_accuracy: 0.8616\n",
            "Epoch 5037/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9009\n",
            "Epoch 5037: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2890 - accuracy: 0.9006 - val_loss: 0.5041 - val_accuracy: 0.8615\n",
            "Epoch 5038/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9006\n",
            "Epoch 5038: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2894 - accuracy: 0.9006 - val_loss: 0.5076 - val_accuracy: 0.8613\n",
            "Epoch 5039/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8996\n",
            "Epoch 5039: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2899 - accuracy: 0.8997 - val_loss: 0.4995 - val_accuracy: 0.8619\n",
            "Epoch 5040/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9012\n",
            "Epoch 5040: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2928 - accuracy: 0.9010 - val_loss: 0.4956 - val_accuracy: 0.8604\n",
            "Epoch 5041/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8998\n",
            "Epoch 5041: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2929 - accuracy: 0.8998 - val_loss: 0.4987 - val_accuracy: 0.8618\n",
            "Epoch 5042/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9017\n",
            "Epoch 5042: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9018 - val_loss: 0.4969 - val_accuracy: 0.8618\n",
            "Epoch 5043/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9005\n",
            "Epoch 5043: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2902 - accuracy: 0.9001 - val_loss: 0.5000 - val_accuracy: 0.8623\n",
            "Epoch 5044/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9032\n",
            "Epoch 5044: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2846 - accuracy: 0.9032 - val_loss: 0.4955 - val_accuracy: 0.8628\n",
            "Epoch 5045/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9017\n",
            "Epoch 5045: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2893 - accuracy: 0.9020 - val_loss: 0.4958 - val_accuracy: 0.8626\n",
            "Epoch 5046/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9004\n",
            "Epoch 5046: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2909 - accuracy: 0.9004 - val_loss: 0.4969 - val_accuracy: 0.8622\n",
            "Epoch 5047/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9016\n",
            "Epoch 5047: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2858 - accuracy: 0.9016 - val_loss: 0.4994 - val_accuracy: 0.8624\n",
            "Epoch 5048/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9020\n",
            "Epoch 5048: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2859 - accuracy: 0.9021 - val_loss: 0.4969 - val_accuracy: 0.8612\n",
            "Epoch 5049/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9024\n",
            "Epoch 5049: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2850 - accuracy: 0.9025 - val_loss: 0.4967 - val_accuracy: 0.8615\n",
            "Epoch 5050/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.9008\n",
            "Epoch 5050: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2870 - accuracy: 0.9008 - val_loss: 0.4959 - val_accuracy: 0.8629\n",
            "Epoch 5051/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9020\n",
            "Epoch 5051: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2860 - accuracy: 0.9022 - val_loss: 0.4984 - val_accuracy: 0.8629\n",
            "Epoch 5052/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9009\n",
            "Epoch 5052: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2915 - accuracy: 0.9009 - val_loss: 0.5000 - val_accuracy: 0.8614\n",
            "Epoch 5053/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9024\n",
            "Epoch 5053: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9024 - val_loss: 0.5029 - val_accuracy: 0.8613\n",
            "Epoch 5054/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9007\n",
            "Epoch 5054: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2917 - accuracy: 0.9006 - val_loss: 0.4999 - val_accuracy: 0.8626\n",
            "Epoch 5055/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9000\n",
            "Epoch 5055: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2917 - accuracy: 0.9001 - val_loss: 0.5030 - val_accuracy: 0.8623\n",
            "Epoch 5056/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9003\n",
            "Epoch 5056: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2891 - accuracy: 0.9003 - val_loss: 0.4979 - val_accuracy: 0.8613\n",
            "Epoch 5057/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9016\n",
            "Epoch 5057: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2895 - accuracy: 0.9016 - val_loss: 0.5069 - val_accuracy: 0.8606\n",
            "Epoch 5058/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9012\n",
            "Epoch 5058: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.9009 - val_loss: 0.5047 - val_accuracy: 0.8621\n",
            "Epoch 5059/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9012\n",
            "Epoch 5059: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2862 - accuracy: 0.9012 - val_loss: 0.4970 - val_accuracy: 0.8624\n",
            "Epoch 5060/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9029\n",
            "Epoch 5060: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9029 - val_loss: 0.4986 - val_accuracy: 0.8616\n",
            "Epoch 5061/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9001\n",
            "Epoch 5061: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2865 - accuracy: 0.9002 - val_loss: 0.5029 - val_accuracy: 0.8607\n",
            "Epoch 5062/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9031\n",
            "Epoch 5062: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9032 - val_loss: 0.5031 - val_accuracy: 0.8623\n",
            "Epoch 5063/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.9020\n",
            "Epoch 5063: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2934 - accuracy: 0.9019 - val_loss: 0.5010 - val_accuracy: 0.8650\n",
            "Epoch 5064/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9016\n",
            "Epoch 5064: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2889 - accuracy: 0.9014 - val_loss: 0.4974 - val_accuracy: 0.8626\n",
            "Epoch 5065/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9030\n",
            "Epoch 5065: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9030 - val_loss: 0.5003 - val_accuracy: 0.8627\n",
            "Epoch 5066/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9002\n",
            "Epoch 5066: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2928 - accuracy: 0.8998 - val_loss: 0.4988 - val_accuracy: 0.8616\n",
            "Epoch 5067/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9006\n",
            "Epoch 5067: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2899 - accuracy: 0.9004 - val_loss: 0.4979 - val_accuracy: 0.8612\n",
            "Epoch 5068/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9012\n",
            "Epoch 5068: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9012 - val_loss: 0.4979 - val_accuracy: 0.8641\n",
            "Epoch 5069/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.9021\n",
            "Epoch 5069: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2844 - accuracy: 0.9022 - val_loss: 0.4999 - val_accuracy: 0.8636\n",
            "Epoch 5070/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9009\n",
            "Epoch 5070: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9009 - val_loss: 0.4973 - val_accuracy: 0.8625\n",
            "Epoch 5071/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.9005\n",
            "Epoch 5071: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2913 - accuracy: 0.9004 - val_loss: 0.5009 - val_accuracy: 0.8629\n",
            "Epoch 5072/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9029\n",
            "Epoch 5072: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2856 - accuracy: 0.9029 - val_loss: 0.5017 - val_accuracy: 0.8631\n",
            "Epoch 5073/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9015\n",
            "Epoch 5073: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2881 - accuracy: 0.9016 - val_loss: 0.5026 - val_accuracy: 0.8614\n",
            "Epoch 5074/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9011\n",
            "Epoch 5074: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2855 - accuracy: 0.9009 - val_loss: 0.4997 - val_accuracy: 0.8641\n",
            "Epoch 5075/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9024\n",
            "Epoch 5075: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2869 - accuracy: 0.9023 - val_loss: 0.4964 - val_accuracy: 0.8624\n",
            "Epoch 5076/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9020\n",
            "Epoch 5076: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2846 - accuracy: 0.9020 - val_loss: 0.5000 - val_accuracy: 0.8630\n",
            "Epoch 5077/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9010\n",
            "Epoch 5077: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2894 - accuracy: 0.9010 - val_loss: 0.4962 - val_accuracy: 0.8621\n",
            "Epoch 5078/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9004\n",
            "Epoch 5078: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2896 - accuracy: 0.9005 - val_loss: 0.4968 - val_accuracy: 0.8625\n",
            "Epoch 5079/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9000\n",
            "Epoch 5079: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2917 - accuracy: 0.8999 - val_loss: 0.4962 - val_accuracy: 0.8607\n",
            "Epoch 5080/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8998\n",
            "Epoch 5080: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2914 - accuracy: 0.8998 - val_loss: 0.5001 - val_accuracy: 0.8611\n",
            "Epoch 5081/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9007\n",
            "Epoch 5081: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2885 - accuracy: 0.9008 - val_loss: 0.4969 - val_accuracy: 0.8618\n",
            "Epoch 5082/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.9000\n",
            "Epoch 5082: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2915 - accuracy: 0.9001 - val_loss: 0.4896 - val_accuracy: 0.8629\n",
            "Epoch 5083/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9028\n",
            "Epoch 5083: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2903 - accuracy: 0.9028 - val_loss: 0.4970 - val_accuracy: 0.8624\n",
            "Epoch 5084/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.8995\n",
            "Epoch 5084: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2898 - accuracy: 0.8995 - val_loss: 0.4998 - val_accuracy: 0.8618\n",
            "Epoch 5085/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9028\n",
            "Epoch 5085: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9026 - val_loss: 0.5033 - val_accuracy: 0.8612\n",
            "Epoch 5086/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9005\n",
            "Epoch 5086: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2918 - accuracy: 0.9006 - val_loss: 0.4972 - val_accuracy: 0.8625\n",
            "Epoch 5087/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9009\n",
            "Epoch 5087: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2899 - accuracy: 0.9009 - val_loss: 0.4963 - val_accuracy: 0.8621\n",
            "Epoch 5088/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9034\n",
            "Epoch 5088: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2842 - accuracy: 0.9035 - val_loss: 0.4978 - val_accuracy: 0.8610\n",
            "Epoch 5089/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9026\n",
            "Epoch 5089: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2886 - accuracy: 0.9027 - val_loss: 0.4994 - val_accuracy: 0.8614\n",
            "Epoch 5090/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9022\n",
            "Epoch 5090: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2888 - accuracy: 0.9019 - val_loss: 0.5003 - val_accuracy: 0.8615\n",
            "Epoch 5091/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8999\n",
            "Epoch 5091: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2877 - accuracy: 0.8998 - val_loss: 0.4982 - val_accuracy: 0.8633\n",
            "Epoch 5092/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9030\n",
            "Epoch 5092: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9028 - val_loss: 0.4934 - val_accuracy: 0.8639\n",
            "Epoch 5093/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9009\n",
            "Epoch 5093: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9010 - val_loss: 0.4916 - val_accuracy: 0.8630\n",
            "Epoch 5094/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9014\n",
            "Epoch 5094: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2860 - accuracy: 0.9013 - val_loss: 0.4935 - val_accuracy: 0.8620\n",
            "Epoch 5095/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9022\n",
            "Epoch 5095: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2886 - accuracy: 0.9021 - val_loss: 0.4961 - val_accuracy: 0.8607\n",
            "Epoch 5096/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9010\n",
            "Epoch 5096: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2883 - accuracy: 0.9013 - val_loss: 0.4986 - val_accuracy: 0.8627\n",
            "Epoch 5097/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9006\n",
            "Epoch 5097: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2898 - accuracy: 0.9008 - val_loss: 0.4973 - val_accuracy: 0.8615\n",
            "Epoch 5098/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8999\n",
            "Epoch 5098: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2922 - accuracy: 0.9000 - val_loss: 0.4937 - val_accuracy: 0.8622\n",
            "Epoch 5099/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8994\n",
            "Epoch 5099: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2897 - accuracy: 0.8993 - val_loss: 0.4950 - val_accuracy: 0.8628\n",
            "Epoch 5100/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9011\n",
            "Epoch 5100: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2873 - accuracy: 0.9006 - val_loss: 0.4996 - val_accuracy: 0.8633\n",
            "Epoch 5101/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9017\n",
            "Epoch 5101: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9012 - val_loss: 0.4994 - val_accuracy: 0.8632\n",
            "Epoch 5102/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9030\n",
            "Epoch 5102: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2901 - accuracy: 0.9030 - val_loss: 0.4938 - val_accuracy: 0.8630\n",
            "Epoch 5103/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9015\n",
            "Epoch 5103: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2892 - accuracy: 0.9015 - val_loss: 0.4907 - val_accuracy: 0.8619\n",
            "Epoch 5104/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9032\n",
            "Epoch 5104: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2874 - accuracy: 0.9028 - val_loss: 0.4956 - val_accuracy: 0.8612\n",
            "Epoch 5105/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9016\n",
            "Epoch 5105: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2877 - accuracy: 0.9016 - val_loss: 0.4973 - val_accuracy: 0.8618\n",
            "Epoch 5106/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9007\n",
            "Epoch 5106: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2892 - accuracy: 0.9007 - val_loss: 0.4939 - val_accuracy: 0.8624\n",
            "Epoch 5107/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9011\n",
            "Epoch 5107: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2888 - accuracy: 0.9009 - val_loss: 0.4993 - val_accuracy: 0.8624\n",
            "Epoch 5108/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9032\n",
            "Epoch 5108: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2850 - accuracy: 0.9031 - val_loss: 0.5044 - val_accuracy: 0.8624\n",
            "Epoch 5109/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9030\n",
            "Epoch 5109: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2852 - accuracy: 0.9030 - val_loss: 0.5013 - val_accuracy: 0.8637\n",
            "Epoch 5110/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9009\n",
            "Epoch 5110: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2903 - accuracy: 0.9009 - val_loss: 0.4959 - val_accuracy: 0.8635\n",
            "Epoch 5111/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9015\n",
            "Epoch 5111: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2875 - accuracy: 0.9012 - val_loss: 0.5033 - val_accuracy: 0.8645\n",
            "Epoch 5112/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9011\n",
            "Epoch 5112: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2898 - accuracy: 0.9011 - val_loss: 0.4981 - val_accuracy: 0.8629\n",
            "Epoch 5113/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.9013\n",
            "Epoch 5113: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2917 - accuracy: 0.9013 - val_loss: 0.4983 - val_accuracy: 0.8635\n",
            "Epoch 5114/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9022\n",
            "Epoch 5114: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9021 - val_loss: 0.4969 - val_accuracy: 0.8624\n",
            "Epoch 5115/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9014\n",
            "Epoch 5115: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2876 - accuracy: 0.9016 - val_loss: 0.4998 - val_accuracy: 0.8610\n",
            "Epoch 5116/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.9030\n",
            "Epoch 5116: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2825 - accuracy: 0.9029 - val_loss: 0.5000 - val_accuracy: 0.8612\n",
            "Epoch 5117/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9005\n",
            "Epoch 5117: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.9005 - val_loss: 0.4949 - val_accuracy: 0.8605\n",
            "Epoch 5118/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9008\n",
            "Epoch 5118: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9008 - val_loss: 0.4971 - val_accuracy: 0.8621\n",
            "Epoch 5119/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9033\n",
            "Epoch 5119: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2844 - accuracy: 0.9033 - val_loss: 0.4997 - val_accuracy: 0.8601\n",
            "Epoch 5120/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9027\n",
            "Epoch 5120: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2862 - accuracy: 0.9026 - val_loss: 0.4976 - val_accuracy: 0.8625\n",
            "Epoch 5121/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9031\n",
            "Epoch 5121: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2875 - accuracy: 0.9031 - val_loss: 0.4958 - val_accuracy: 0.8607\n",
            "Epoch 5122/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9008\n",
            "Epoch 5122: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9008 - val_loss: 0.5007 - val_accuracy: 0.8607\n",
            "Epoch 5123/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9022\n",
            "Epoch 5123: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2860 - accuracy: 0.9019 - val_loss: 0.4973 - val_accuracy: 0.8609\n",
            "Epoch 5124/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9011\n",
            "Epoch 5124: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2904 - accuracy: 0.9011 - val_loss: 0.5013 - val_accuracy: 0.8619\n",
            "Epoch 5125/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8999\n",
            "Epoch 5125: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2915 - accuracy: 0.8998 - val_loss: 0.5017 - val_accuracy: 0.8622\n",
            "Epoch 5126/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9016\n",
            "Epoch 5126: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2897 - accuracy: 0.9016 - val_loss: 0.4973 - val_accuracy: 0.8606\n",
            "Epoch 5127/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9017\n",
            "Epoch 5127: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2884 - accuracy: 0.9018 - val_loss: 0.4985 - val_accuracy: 0.8624\n",
            "Epoch 5128/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8991\n",
            "Epoch 5128: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2924 - accuracy: 0.8992 - val_loss: 0.4981 - val_accuracy: 0.8611\n",
            "Epoch 5129/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9015\n",
            "Epoch 5129: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9015 - val_loss: 0.4945 - val_accuracy: 0.8635\n",
            "Epoch 5130/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9011\n",
            "Epoch 5130: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2889 - accuracy: 0.9009 - val_loss: 0.4983 - val_accuracy: 0.8614\n",
            "Epoch 5131/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9003\n",
            "Epoch 5131: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2888 - accuracy: 0.9002 - val_loss: 0.4957 - val_accuracy: 0.8628\n",
            "Epoch 5132/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2951 - accuracy: 0.8979\n",
            "Epoch 5132: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2951 - accuracy: 0.8979 - val_loss: 0.4947 - val_accuracy: 0.8626\n",
            "Epoch 5133/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9015\n",
            "Epoch 5133: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9013 - val_loss: 0.5012 - val_accuracy: 0.8621\n",
            "Epoch 5134/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9017\n",
            "Epoch 5134: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2863 - accuracy: 0.9017 - val_loss: 0.5000 - val_accuracy: 0.8618\n",
            "Epoch 5135/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9018\n",
            "Epoch 5135: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9019 - val_loss: 0.4994 - val_accuracy: 0.8620\n",
            "Epoch 5136/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9018\n",
            "Epoch 5136: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2875 - accuracy: 0.9018 - val_loss: 0.4940 - val_accuracy: 0.8627\n",
            "Epoch 5137/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.9002\n",
            "Epoch 5137: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2909 - accuracy: 0.9002 - val_loss: 0.4910 - val_accuracy: 0.8610\n",
            "Epoch 5138/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9015\n",
            "Epoch 5138: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2846 - accuracy: 0.9015 - val_loss: 0.5000 - val_accuracy: 0.8618\n",
            "Epoch 5139/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9017\n",
            "Epoch 5139: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2856 - accuracy: 0.9018 - val_loss: 0.5027 - val_accuracy: 0.8620\n",
            "Epoch 5140/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.9015\n",
            "Epoch 5140: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2916 - accuracy: 0.9015 - val_loss: 0.5006 - val_accuracy: 0.8595\n",
            "Epoch 5141/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9011\n",
            "Epoch 5141: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2884 - accuracy: 0.9011 - val_loss: 0.4927 - val_accuracy: 0.8632\n",
            "Epoch 5142/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9003\n",
            "Epoch 5142: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2894 - accuracy: 0.9004 - val_loss: 0.4992 - val_accuracy: 0.8616\n",
            "Epoch 5143/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9010\n",
            "Epoch 5143: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2920 - accuracy: 0.9009 - val_loss: 0.4938 - val_accuracy: 0.8625\n",
            "Epoch 5144/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8999\n",
            "Epoch 5144: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2907 - accuracy: 0.8999 - val_loss: 0.5015 - val_accuracy: 0.8608\n",
            "Epoch 5145/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9013\n",
            "Epoch 5145: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9013 - val_loss: 0.4897 - val_accuracy: 0.8626\n",
            "Epoch 5146/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9004\n",
            "Epoch 5146: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2883 - accuracy: 0.9008 - val_loss: 0.4937 - val_accuracy: 0.8629\n",
            "Epoch 5147/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9012\n",
            "Epoch 5147: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.9012 - val_loss: 0.4936 - val_accuracy: 0.8623\n",
            "Epoch 5148/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9004\n",
            "Epoch 5148: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2918 - accuracy: 0.9002 - val_loss: 0.4982 - val_accuracy: 0.8622\n",
            "Epoch 5149/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.9007\n",
            "Epoch 5149: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2931 - accuracy: 0.9007 - val_loss: 0.4955 - val_accuracy: 0.8627\n",
            "Epoch 5150/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9013\n",
            "Epoch 5150: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2866 - accuracy: 0.9013 - val_loss: 0.4970 - val_accuracy: 0.8618\n",
            "Epoch 5151/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9029\n",
            "Epoch 5151: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2883 - accuracy: 0.9028 - val_loss: 0.4961 - val_accuracy: 0.8628\n",
            "Epoch 5152/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.9009\n",
            "Epoch 5152: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9010 - val_loss: 0.4906 - val_accuracy: 0.8627\n",
            "Epoch 5153/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9001\n",
            "Epoch 5153: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2882 - accuracy: 0.9004 - val_loss: 0.5017 - val_accuracy: 0.8613\n",
            "Epoch 5154/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9007\n",
            "Epoch 5154: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9007 - val_loss: 0.5022 - val_accuracy: 0.8620\n",
            "Epoch 5155/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9005\n",
            "Epoch 5155: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2901 - accuracy: 0.9005 - val_loss: 0.4973 - val_accuracy: 0.8624\n",
            "Epoch 5156/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9001\n",
            "Epoch 5156: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9002 - val_loss: 0.5020 - val_accuracy: 0.8620\n",
            "Epoch 5157/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8998\n",
            "Epoch 5157: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2903 - accuracy: 0.8998 - val_loss: 0.5010 - val_accuracy: 0.8608\n",
            "Epoch 5158/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9024\n",
            "Epoch 5158: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9026 - val_loss: 0.4959 - val_accuracy: 0.8610\n",
            "Epoch 5159/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9014\n",
            "Epoch 5159: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2859 - accuracy: 0.9014 - val_loss: 0.4994 - val_accuracy: 0.8641\n",
            "Epoch 5160/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9019\n",
            "Epoch 5160: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2872 - accuracy: 0.9019 - val_loss: 0.4956 - val_accuracy: 0.8613\n",
            "Epoch 5161/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9016\n",
            "Epoch 5161: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2845 - accuracy: 0.9016 - val_loss: 0.4975 - val_accuracy: 0.8635\n",
            "Epoch 5162/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9021\n",
            "Epoch 5162: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2880 - accuracy: 0.9020 - val_loss: 0.4989 - val_accuracy: 0.8620\n",
            "Epoch 5163/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9010\n",
            "Epoch 5163: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2903 - accuracy: 0.9012 - val_loss: 0.4936 - val_accuracy: 0.8624\n",
            "Epoch 5164/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9017\n",
            "Epoch 5164: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2883 - accuracy: 0.9016 - val_loss: 0.4976 - val_accuracy: 0.8624\n",
            "Epoch 5165/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8995\n",
            "Epoch 5165: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2904 - accuracy: 0.8998 - val_loss: 0.4974 - val_accuracy: 0.8623\n",
            "Epoch 5166/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9007\n",
            "Epoch 5166: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2890 - accuracy: 0.9009 - val_loss: 0.5017 - val_accuracy: 0.8617\n",
            "Epoch 5167/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9022\n",
            "Epoch 5167: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2845 - accuracy: 0.9020 - val_loss: 0.4992 - val_accuracy: 0.8607\n",
            "Epoch 5168/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9028\n",
            "Epoch 5168: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2880 - accuracy: 0.9027 - val_loss: 0.5012 - val_accuracy: 0.8605\n",
            "Epoch 5169/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.9005\n",
            "Epoch 5169: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2930 - accuracy: 0.9006 - val_loss: 0.4936 - val_accuracy: 0.8604\n",
            "Epoch 5170/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9016\n",
            "Epoch 5170: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9012 - val_loss: 0.4987 - val_accuracy: 0.8616\n",
            "Epoch 5171/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9012\n",
            "Epoch 5171: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2876 - accuracy: 0.9012 - val_loss: 0.4974 - val_accuracy: 0.8632\n",
            "Epoch 5172/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9032\n",
            "Epoch 5172: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9028 - val_loss: 0.5004 - val_accuracy: 0.8617\n",
            "Epoch 5173/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9031\n",
            "Epoch 5173: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2886 - accuracy: 0.9031 - val_loss: 0.4945 - val_accuracy: 0.8631\n",
            "Epoch 5174/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9038\n",
            "Epoch 5174: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2824 - accuracy: 0.9039 - val_loss: 0.5008 - val_accuracy: 0.8648\n",
            "Epoch 5175/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9023\n",
            "Epoch 5175: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9023 - val_loss: 0.4989 - val_accuracy: 0.8601\n",
            "Epoch 5176/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9027\n",
            "Epoch 5176: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2863 - accuracy: 0.9027 - val_loss: 0.5008 - val_accuracy: 0.8639\n",
            "Epoch 5177/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9009\n",
            "Epoch 5177: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2922 - accuracy: 0.9009 - val_loss: 0.5001 - val_accuracy: 0.8631\n",
            "Epoch 5178/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9016\n",
            "Epoch 5178: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2912 - accuracy: 0.9016 - val_loss: 0.4924 - val_accuracy: 0.8624\n",
            "Epoch 5179/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.9000\n",
            "Epoch 5179: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2933 - accuracy: 0.9000 - val_loss: 0.4871 - val_accuracy: 0.8642\n",
            "Epoch 5180/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9018\n",
            "Epoch 5180: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2895 - accuracy: 0.9018 - val_loss: 0.4893 - val_accuracy: 0.8645\n",
            "Epoch 5181/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9028\n",
            "Epoch 5181: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9028 - val_loss: 0.4938 - val_accuracy: 0.8639\n",
            "Epoch 5182/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9010\n",
            "Epoch 5182: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2882 - accuracy: 0.9012 - val_loss: 0.5013 - val_accuracy: 0.8621\n",
            "Epoch 5183/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9013\n",
            "Epoch 5183: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2891 - accuracy: 0.9013 - val_loss: 0.4977 - val_accuracy: 0.8635\n",
            "Epoch 5184/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9017\n",
            "Epoch 5184: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9012 - val_loss: 0.4974 - val_accuracy: 0.8625\n",
            "Epoch 5185/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9019\n",
            "Epoch 5185: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2857 - accuracy: 0.9020 - val_loss: 0.4975 - val_accuracy: 0.8624\n",
            "Epoch 5186/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9029\n",
            "Epoch 5186: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2880 - accuracy: 0.9029 - val_loss: 0.4983 - val_accuracy: 0.8598\n",
            "Epoch 5187/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.9016\n",
            "Epoch 5187: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2917 - accuracy: 0.9016 - val_loss: 0.4957 - val_accuracy: 0.8614\n",
            "Epoch 5188/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.8993\n",
            "Epoch 5188: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2895 - accuracy: 0.8993 - val_loss: 0.4958 - val_accuracy: 0.8625\n",
            "Epoch 5189/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9028\n",
            "Epoch 5189: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2897 - accuracy: 0.9028 - val_loss: 0.5008 - val_accuracy: 0.8624\n",
            "Epoch 5190/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9003\n",
            "Epoch 5190: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2905 - accuracy: 0.9005 - val_loss: 0.4984 - val_accuracy: 0.8600\n",
            "Epoch 5191/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.8991\n",
            "Epoch 5191: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2891 - accuracy: 0.8989 - val_loss: 0.5002 - val_accuracy: 0.8618\n",
            "Epoch 5192/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9002\n",
            "Epoch 5192: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2907 - accuracy: 0.9000 - val_loss: 0.4879 - val_accuracy: 0.8629\n",
            "Epoch 5193/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9018\n",
            "Epoch 5193: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2849 - accuracy: 0.9018 - val_loss: 0.4989 - val_accuracy: 0.8618\n",
            "Epoch 5194/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9012\n",
            "Epoch 5194: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2899 - accuracy: 0.9012 - val_loss: 0.4916 - val_accuracy: 0.8607\n",
            "Epoch 5195/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9035\n",
            "Epoch 5195: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2816 - accuracy: 0.9031 - val_loss: 0.4981 - val_accuracy: 0.8606\n",
            "Epoch 5196/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9020\n",
            "Epoch 5196: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9021 - val_loss: 0.4938 - val_accuracy: 0.8629\n",
            "Epoch 5197/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9007\n",
            "Epoch 5197: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2926 - accuracy: 0.9008 - val_loss: 0.4967 - val_accuracy: 0.8619\n",
            "Epoch 5198/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9014\n",
            "Epoch 5198: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2875 - accuracy: 0.9013 - val_loss: 0.4957 - val_accuracy: 0.8623\n",
            "Epoch 5199/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9018\n",
            "Epoch 5199: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2912 - accuracy: 0.9019 - val_loss: 0.4949 - val_accuracy: 0.8628\n",
            "Epoch 5200/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9022\n",
            "Epoch 5200: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2873 - accuracy: 0.9022 - val_loss: 0.5029 - val_accuracy: 0.8596\n",
            "Epoch 5201/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9020\n",
            "Epoch 5201: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2899 - accuracy: 0.9018 - val_loss: 0.4972 - val_accuracy: 0.8608\n",
            "Epoch 5202/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9032\n",
            "Epoch 5202: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2847 - accuracy: 0.9032 - val_loss: 0.4991 - val_accuracy: 0.8607\n",
            "Epoch 5203/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9012\n",
            "Epoch 5203: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2906 - accuracy: 0.9013 - val_loss: 0.4992 - val_accuracy: 0.8628\n",
            "Epoch 5204/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9012\n",
            "Epoch 5204: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2899 - accuracy: 0.9016 - val_loss: 0.5008 - val_accuracy: 0.8612\n",
            "Epoch 5205/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9021\n",
            "Epoch 5205: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9018 - val_loss: 0.4987 - val_accuracy: 0.8602\n",
            "Epoch 5206/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9007\n",
            "Epoch 5206: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2879 - accuracy: 0.9007 - val_loss: 0.4959 - val_accuracy: 0.8629\n",
            "Epoch 5207/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9014\n",
            "Epoch 5207: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2902 - accuracy: 0.9014 - val_loss: 0.4989 - val_accuracy: 0.8616\n",
            "Epoch 5208/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8996\n",
            "Epoch 5208: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2923 - accuracy: 0.8996 - val_loss: 0.4947 - val_accuracy: 0.8634\n",
            "Epoch 5209/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9028\n",
            "Epoch 5209: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2900 - accuracy: 0.9028 - val_loss: 0.4940 - val_accuracy: 0.8647\n",
            "Epoch 5210/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9025\n",
            "Epoch 5210: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2864 - accuracy: 0.9023 - val_loss: 0.5012 - val_accuracy: 0.8629\n",
            "Epoch 5211/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9016\n",
            "Epoch 5211: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2910 - accuracy: 0.9016 - val_loss: 0.4936 - val_accuracy: 0.8631\n",
            "Epoch 5212/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.8985\n",
            "Epoch 5212: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2931 - accuracy: 0.8985 - val_loss: 0.4961 - val_accuracy: 0.8632\n",
            "Epoch 5213/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9007\n",
            "Epoch 5213: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2875 - accuracy: 0.9006 - val_loss: 0.4997 - val_accuracy: 0.8624\n",
            "Epoch 5214/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9006\n",
            "Epoch 5214: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2898 - accuracy: 0.9005 - val_loss: 0.4937 - val_accuracy: 0.8617\n",
            "Epoch 5215/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9029\n",
            "Epoch 5215: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2875 - accuracy: 0.9026 - val_loss: 0.4996 - val_accuracy: 0.8611\n",
            "Epoch 5216/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9032\n",
            "Epoch 5216: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2850 - accuracy: 0.9031 - val_loss: 0.4929 - val_accuracy: 0.8601\n",
            "Epoch 5217/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9022\n",
            "Epoch 5217: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9022 - val_loss: 0.4956 - val_accuracy: 0.8624\n",
            "Epoch 5218/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9014\n",
            "Epoch 5218: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2869 - accuracy: 0.9012 - val_loss: 0.4991 - val_accuracy: 0.8635\n",
            "Epoch 5219/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9018\n",
            "Epoch 5219: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2886 - accuracy: 0.9021 - val_loss: 0.4942 - val_accuracy: 0.8613\n",
            "Epoch 5220/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9032\n",
            "Epoch 5220: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2872 - accuracy: 0.9033 - val_loss: 0.4939 - val_accuracy: 0.8634\n",
            "Epoch 5221/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9012\n",
            "Epoch 5221: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2863 - accuracy: 0.9012 - val_loss: 0.5004 - val_accuracy: 0.8614\n",
            "Epoch 5222/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9001\n",
            "Epoch 5222: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2916 - accuracy: 0.9006 - val_loss: 0.4964 - val_accuracy: 0.8608\n",
            "Epoch 5223/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9019\n",
            "Epoch 5223: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9016 - val_loss: 0.5044 - val_accuracy: 0.8609\n",
            "Epoch 5224/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9026\n",
            "Epoch 5224: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2850 - accuracy: 0.9026 - val_loss: 0.5014 - val_accuracy: 0.8629\n",
            "Epoch 5225/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9039\n",
            "Epoch 5225: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9039 - val_loss: 0.4965 - val_accuracy: 0.8624\n",
            "Epoch 5226/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9023\n",
            "Epoch 5226: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2882 - accuracy: 0.9023 - val_loss: 0.4943 - val_accuracy: 0.8635\n",
            "Epoch 5227/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9015\n",
            "Epoch 5227: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2878 - accuracy: 0.9015 - val_loss: 0.4998 - val_accuracy: 0.8627\n",
            "Epoch 5228/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9012\n",
            "Epoch 5228: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2891 - accuracy: 0.9012 - val_loss: 0.4952 - val_accuracy: 0.8640\n",
            "Epoch 5229/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9017\n",
            "Epoch 5229: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2876 - accuracy: 0.9019 - val_loss: 0.4991 - val_accuracy: 0.8618\n",
            "Epoch 5230/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9000\n",
            "Epoch 5230: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2866 - accuracy: 0.8998 - val_loss: 0.4998 - val_accuracy: 0.8611\n",
            "Epoch 5231/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9014\n",
            "Epoch 5231: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2843 - accuracy: 0.9015 - val_loss: 0.5061 - val_accuracy: 0.8640\n",
            "Epoch 5232/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9001\n",
            "Epoch 5232: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2910 - accuracy: 0.9002 - val_loss: 0.5033 - val_accuracy: 0.8633\n",
            "Epoch 5233/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9013\n",
            "Epoch 5233: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2883 - accuracy: 0.9011 - val_loss: 0.5003 - val_accuracy: 0.8634\n",
            "Epoch 5234/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.8997\n",
            "Epoch 5234: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2877 - accuracy: 0.8998 - val_loss: 0.5055 - val_accuracy: 0.8641\n",
            "Epoch 5235/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9027\n",
            "Epoch 5235: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2893 - accuracy: 0.9026 - val_loss: 0.5004 - val_accuracy: 0.8622\n",
            "Epoch 5236/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.9012\n",
            "Epoch 5236: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9013 - val_loss: 0.5012 - val_accuracy: 0.8612\n",
            "Epoch 5237/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9020\n",
            "Epoch 5237: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2911 - accuracy: 0.9017 - val_loss: 0.4968 - val_accuracy: 0.8644\n",
            "Epoch 5238/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.9014\n",
            "Epoch 5238: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2920 - accuracy: 0.9014 - val_loss: 0.4944 - val_accuracy: 0.8637\n",
            "Epoch 5239/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9020\n",
            "Epoch 5239: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2885 - accuracy: 0.9021 - val_loss: 0.4928 - val_accuracy: 0.8633\n",
            "Epoch 5240/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9024\n",
            "Epoch 5240: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2901 - accuracy: 0.9022 - val_loss: 0.5004 - val_accuracy: 0.8619\n",
            "Epoch 5241/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9012\n",
            "Epoch 5241: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9012 - val_loss: 0.5012 - val_accuracy: 0.8618\n",
            "Epoch 5242/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9015\n",
            "Epoch 5242: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.9016 - val_loss: 0.5007 - val_accuracy: 0.8618\n",
            "Epoch 5243/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9027\n",
            "Epoch 5243: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2844 - accuracy: 0.9030 - val_loss: 0.5038 - val_accuracy: 0.8629\n",
            "Epoch 5244/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.8994\n",
            "Epoch 5244: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2942 - accuracy: 0.8994 - val_loss: 0.5035 - val_accuracy: 0.8612\n",
            "Epoch 5245/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9005\n",
            "Epoch 5245: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9008 - val_loss: 0.4990 - val_accuracy: 0.8604\n",
            "Epoch 5246/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9022\n",
            "Epoch 5246: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2829 - accuracy: 0.9022 - val_loss: 0.4986 - val_accuracy: 0.8629\n",
            "Epoch 5247/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9007\n",
            "Epoch 5247: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2940 - accuracy: 0.9008 - val_loss: 0.4988 - val_accuracy: 0.8633\n",
            "Epoch 5248/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9013\n",
            "Epoch 5248: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2881 - accuracy: 0.9013 - val_loss: 0.4987 - val_accuracy: 0.8635\n",
            "Epoch 5249/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9015\n",
            "Epoch 5249: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2898 - accuracy: 0.9015 - val_loss: 0.4985 - val_accuracy: 0.8629\n",
            "Epoch 5250/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9017\n",
            "Epoch 5250: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2857 - accuracy: 0.9020 - val_loss: 0.4992 - val_accuracy: 0.8621\n",
            "Epoch 5251/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9011\n",
            "Epoch 5251: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2854 - accuracy: 0.9011 - val_loss: 0.4974 - val_accuracy: 0.8608\n",
            "Epoch 5252/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9010\n",
            "Epoch 5252: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2906 - accuracy: 0.9010 - val_loss: 0.4936 - val_accuracy: 0.8631\n",
            "Epoch 5253/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9005\n",
            "Epoch 5253: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2922 - accuracy: 0.9008 - val_loss: 0.4971 - val_accuracy: 0.8635\n",
            "Epoch 5254/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9010\n",
            "Epoch 5254: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2904 - accuracy: 0.9010 - val_loss: 0.4927 - val_accuracy: 0.8629\n",
            "Epoch 5255/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9008\n",
            "Epoch 5255: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2906 - accuracy: 0.9007 - val_loss: 0.5043 - val_accuracy: 0.8619\n",
            "Epoch 5256/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9024\n",
            "Epoch 5256: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2871 - accuracy: 0.9028 - val_loss: 0.4965 - val_accuracy: 0.8621\n",
            "Epoch 5257/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9027\n",
            "Epoch 5257: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9025 - val_loss: 0.4979 - val_accuracy: 0.8641\n",
            "Epoch 5258/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.9022\n",
            "Epoch 5258: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2834 - accuracy: 0.9025 - val_loss: 0.4971 - val_accuracy: 0.8627\n",
            "Epoch 5259/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9014\n",
            "Epoch 5259: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2870 - accuracy: 0.9015 - val_loss: 0.5002 - val_accuracy: 0.8627\n",
            "Epoch 5260/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9018\n",
            "Epoch 5260: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2913 - accuracy: 0.9014 - val_loss: 0.4995 - val_accuracy: 0.8620\n",
            "Epoch 5261/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9023\n",
            "Epoch 5261: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2874 - accuracy: 0.9023 - val_loss: 0.5005 - val_accuracy: 0.8635\n",
            "Epoch 5262/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9021\n",
            "Epoch 5262: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2855 - accuracy: 0.9022 - val_loss: 0.4967 - val_accuracy: 0.8620\n",
            "Epoch 5263/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9020\n",
            "Epoch 5263: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2886 - accuracy: 0.9020 - val_loss: 0.4944 - val_accuracy: 0.8627\n",
            "Epoch 5264/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9031\n",
            "Epoch 5264: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2870 - accuracy: 0.9031 - val_loss: 0.4946 - val_accuracy: 0.8628\n",
            "Epoch 5265/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9012\n",
            "Epoch 5265: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2868 - accuracy: 0.9011 - val_loss: 0.5006 - val_accuracy: 0.8615\n",
            "Epoch 5266/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9001\n",
            "Epoch 5266: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2921 - accuracy: 0.9001 - val_loss: 0.4980 - val_accuracy: 0.8627\n",
            "Epoch 5267/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9000\n",
            "Epoch 5267: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.8998 - val_loss: 0.5020 - val_accuracy: 0.8622\n",
            "Epoch 5268/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9025\n",
            "Epoch 5268: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2845 - accuracy: 0.9023 - val_loss: 0.4993 - val_accuracy: 0.8639\n",
            "Epoch 5269/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9035\n",
            "Epoch 5269: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2861 - accuracy: 0.9034 - val_loss: 0.4980 - val_accuracy: 0.8616\n",
            "Epoch 5270/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9031\n",
            "Epoch 5270: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9030 - val_loss: 0.5001 - val_accuracy: 0.8599\n",
            "Epoch 5271/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9021\n",
            "Epoch 5271: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2881 - accuracy: 0.9021 - val_loss: 0.4983 - val_accuracy: 0.8635\n",
            "Epoch 5272/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9005\n",
            "Epoch 5272: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2886 - accuracy: 0.9004 - val_loss: 0.5014 - val_accuracy: 0.8610\n",
            "Epoch 5273/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9021\n",
            "Epoch 5273: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2930 - accuracy: 0.9019 - val_loss: 0.4938 - val_accuracy: 0.8630\n",
            "Epoch 5274/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9028\n",
            "Epoch 5274: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2829 - accuracy: 0.9028 - val_loss: 0.4973 - val_accuracy: 0.8633\n",
            "Epoch 5275/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9009\n",
            "Epoch 5275: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2885 - accuracy: 0.9007 - val_loss: 0.5031 - val_accuracy: 0.8635\n",
            "Epoch 5276/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9010\n",
            "Epoch 5276: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2855 - accuracy: 0.9010 - val_loss: 0.4983 - val_accuracy: 0.8636\n",
            "Epoch 5277/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9026\n",
            "Epoch 5277: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2834 - accuracy: 0.9025 - val_loss: 0.4999 - val_accuracy: 0.8641\n",
            "Epoch 5278/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9028\n",
            "Epoch 5278: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2868 - accuracy: 0.9028 - val_loss: 0.5012 - val_accuracy: 0.8633\n",
            "Epoch 5279/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9015\n",
            "Epoch 5279: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2878 - accuracy: 0.9015 - val_loss: 0.4938 - val_accuracy: 0.8612\n",
            "Epoch 5280/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9025\n",
            "Epoch 5280: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9023 - val_loss: 0.4974 - val_accuracy: 0.8611\n",
            "Epoch 5281/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9003\n",
            "Epoch 5281: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9003 - val_loss: 0.4976 - val_accuracy: 0.8629\n",
            "Epoch 5282/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9005\n",
            "Epoch 5282: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2905 - accuracy: 0.9005 - val_loss: 0.5003 - val_accuracy: 0.8626\n",
            "Epoch 5283/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.9029\n",
            "Epoch 5283: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2835 - accuracy: 0.9029 - val_loss: 0.4971 - val_accuracy: 0.8642\n",
            "Epoch 5284/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8992\n",
            "Epoch 5284: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2891 - accuracy: 0.8992 - val_loss: 0.5001 - val_accuracy: 0.8620\n",
            "Epoch 5285/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9023\n",
            "Epoch 5285: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2852 - accuracy: 0.9022 - val_loss: 0.4956 - val_accuracy: 0.8626\n",
            "Epoch 5286/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9037\n",
            "Epoch 5286: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2833 - accuracy: 0.9037 - val_loss: 0.5019 - val_accuracy: 0.8646\n",
            "Epoch 5287/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9028\n",
            "Epoch 5287: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9029 - val_loss: 0.4976 - val_accuracy: 0.8633\n",
            "Epoch 5288/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9020\n",
            "Epoch 5288: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2889 - accuracy: 0.9013 - val_loss: 0.4998 - val_accuracy: 0.8608\n",
            "Epoch 5289/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.8998\n",
            "Epoch 5289: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2901 - accuracy: 0.8998 - val_loss: 0.4984 - val_accuracy: 0.8618\n",
            "Epoch 5290/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9016\n",
            "Epoch 5290: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2892 - accuracy: 0.9022 - val_loss: 0.4988 - val_accuracy: 0.8620\n",
            "Epoch 5291/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9017\n",
            "Epoch 5291: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2906 - accuracy: 0.9018 - val_loss: 0.4929 - val_accuracy: 0.8626\n",
            "Epoch 5292/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9009\n",
            "Epoch 5292: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.9009 - val_loss: 0.4955 - val_accuracy: 0.8628\n",
            "Epoch 5293/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9021\n",
            "Epoch 5293: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.9023 - val_loss: 0.4994 - val_accuracy: 0.8616\n",
            "Epoch 5294/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9027\n",
            "Epoch 5294: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2853 - accuracy: 0.9027 - val_loss: 0.5061 - val_accuracy: 0.8616\n",
            "Epoch 5295/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9008\n",
            "Epoch 5295: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2909 - accuracy: 0.9011 - val_loss: 0.5018 - val_accuracy: 0.8622\n",
            "Epoch 5296/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9013\n",
            "Epoch 5296: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2891 - accuracy: 0.9013 - val_loss: 0.4996 - val_accuracy: 0.8608\n",
            "Epoch 5297/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9022\n",
            "Epoch 5297: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9022 - val_loss: 0.5082 - val_accuracy: 0.8612\n",
            "Epoch 5298/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.8992\n",
            "Epoch 5298: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2894 - accuracy: 0.8995 - val_loss: 0.4957 - val_accuracy: 0.8613\n",
            "Epoch 5299/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9021\n",
            "Epoch 5299: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2879 - accuracy: 0.9021 - val_loss: 0.4996 - val_accuracy: 0.8612\n",
            "Epoch 5300/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9007\n",
            "Epoch 5300: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.9007 - val_loss: 0.4988 - val_accuracy: 0.8630\n",
            "Epoch 5301/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9016\n",
            "Epoch 5301: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2870 - accuracy: 0.9015 - val_loss: 0.5054 - val_accuracy: 0.8624\n",
            "Epoch 5302/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9034\n",
            "Epoch 5302: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2832 - accuracy: 0.9036 - val_loss: 0.5017 - val_accuracy: 0.8621\n",
            "Epoch 5303/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9030\n",
            "Epoch 5303: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9030 - val_loss: 0.5023 - val_accuracy: 0.8614\n",
            "Epoch 5304/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9020\n",
            "Epoch 5304: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9017 - val_loss: 0.5066 - val_accuracy: 0.8622\n",
            "Epoch 5305/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9039\n",
            "Epoch 5305: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2871 - accuracy: 0.9037 - val_loss: 0.5021 - val_accuracy: 0.8611\n",
            "Epoch 5306/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9000\n",
            "Epoch 5306: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2895 - accuracy: 0.9000 - val_loss: 0.4942 - val_accuracy: 0.8618\n",
            "Epoch 5307/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9009\n",
            "Epoch 5307: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2880 - accuracy: 0.9009 - val_loss: 0.4936 - val_accuracy: 0.8625\n",
            "Epoch 5308/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9039\n",
            "Epoch 5308: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2822 - accuracy: 0.9039 - val_loss: 0.5018 - val_accuracy: 0.8625\n",
            "Epoch 5309/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9002\n",
            "Epoch 5309: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2894 - accuracy: 0.9001 - val_loss: 0.4980 - val_accuracy: 0.8619\n",
            "Epoch 5310/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9003\n",
            "Epoch 5310: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9007 - val_loss: 0.5006 - val_accuracy: 0.8631\n",
            "Epoch 5311/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9013\n",
            "Epoch 5311: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9013 - val_loss: 0.5005 - val_accuracy: 0.8613\n",
            "Epoch 5312/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9019\n",
            "Epoch 5312: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2847 - accuracy: 0.9024 - val_loss: 0.4998 - val_accuracy: 0.8624\n",
            "Epoch 5313/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9018\n",
            "Epoch 5313: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2877 - accuracy: 0.9017 - val_loss: 0.5018 - val_accuracy: 0.8621\n",
            "Epoch 5314/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9005\n",
            "Epoch 5314: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9005 - val_loss: 0.5005 - val_accuracy: 0.8625\n",
            "Epoch 5315/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9040\n",
            "Epoch 5315: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2850 - accuracy: 0.9040 - val_loss: 0.4977 - val_accuracy: 0.8637\n",
            "Epoch 5316/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9012\n",
            "Epoch 5316: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9010 - val_loss: 0.4934 - val_accuracy: 0.8632\n",
            "Epoch 5317/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9016\n",
            "Epoch 5317: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2903 - accuracy: 0.9016 - val_loss: 0.4967 - val_accuracy: 0.8639\n",
            "Epoch 5318/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9027\n",
            "Epoch 5318: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2879 - accuracy: 0.9027 - val_loss: 0.4918 - val_accuracy: 0.8627\n",
            "Epoch 5319/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9000\n",
            "Epoch 5319: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2887 - accuracy: 0.9002 - val_loss: 0.4919 - val_accuracy: 0.8650\n",
            "Epoch 5320/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9025\n",
            "Epoch 5320: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9025 - val_loss: 0.4964 - val_accuracy: 0.8638\n",
            "Epoch 5321/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9011\n",
            "Epoch 5321: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2876 - accuracy: 0.9010 - val_loss: 0.4991 - val_accuracy: 0.8642\n",
            "Epoch 5322/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9006\n",
            "Epoch 5322: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2888 - accuracy: 0.9006 - val_loss: 0.4998 - val_accuracy: 0.8619\n",
            "Epoch 5323/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9006\n",
            "Epoch 5323: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2890 - accuracy: 0.9006 - val_loss: 0.4974 - val_accuracy: 0.8618\n",
            "Epoch 5324/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9013\n",
            "Epoch 5324: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2886 - accuracy: 0.9013 - val_loss: 0.5032 - val_accuracy: 0.8610\n",
            "Epoch 5325/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9007\n",
            "Epoch 5325: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2906 - accuracy: 0.9008 - val_loss: 0.4979 - val_accuracy: 0.8614\n",
            "Epoch 5326/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.9015\n",
            "Epoch 5326: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2870 - accuracy: 0.9015 - val_loss: 0.5032 - val_accuracy: 0.8628\n",
            "Epoch 5327/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9007\n",
            "Epoch 5327: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2886 - accuracy: 0.9006 - val_loss: 0.4995 - val_accuracy: 0.8629\n",
            "Epoch 5328/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8997\n",
            "Epoch 5328: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2918 - accuracy: 0.8995 - val_loss: 0.5000 - val_accuracy: 0.8622\n",
            "Epoch 5329/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9035\n",
            "Epoch 5329: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2847 - accuracy: 0.9035 - val_loss: 0.5007 - val_accuracy: 0.8605\n",
            "Epoch 5330/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9018\n",
            "Epoch 5330: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9018 - val_loss: 0.5051 - val_accuracy: 0.8618\n",
            "Epoch 5331/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9006\n",
            "Epoch 5331: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9006 - val_loss: 0.5008 - val_accuracy: 0.8623\n",
            "Epoch 5332/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9029\n",
            "Epoch 5332: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9025 - val_loss: 0.5020 - val_accuracy: 0.8614\n",
            "Epoch 5333/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9010\n",
            "Epoch 5333: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2888 - accuracy: 0.9009 - val_loss: 0.5046 - val_accuracy: 0.8629\n",
            "Epoch 5334/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9018\n",
            "Epoch 5334: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2866 - accuracy: 0.9017 - val_loss: 0.5016 - val_accuracy: 0.8619\n",
            "Epoch 5335/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9020\n",
            "Epoch 5335: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2881 - accuracy: 0.9019 - val_loss: 0.4976 - val_accuracy: 0.8622\n",
            "Epoch 5336/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9012\n",
            "Epoch 5336: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2893 - accuracy: 0.9012 - val_loss: 0.5085 - val_accuracy: 0.8603\n",
            "Epoch 5337/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.9044\n",
            "Epoch 5337: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2809 - accuracy: 0.9045 - val_loss: 0.5048 - val_accuracy: 0.8616\n",
            "Epoch 5338/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9010\n",
            "Epoch 5338: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2915 - accuracy: 0.9010 - val_loss: 0.5045 - val_accuracy: 0.8615\n",
            "Epoch 5339/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9009\n",
            "Epoch 5339: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9009 - val_loss: 0.5017 - val_accuracy: 0.8601\n",
            "Epoch 5340/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9023\n",
            "Epoch 5340: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2847 - accuracy: 0.9023 - val_loss: 0.5033 - val_accuracy: 0.8587\n",
            "Epoch 5341/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9019\n",
            "Epoch 5341: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2841 - accuracy: 0.9021 - val_loss: 0.4997 - val_accuracy: 0.8618\n",
            "Epoch 5342/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9016\n",
            "Epoch 5342: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9016 - val_loss: 0.5004 - val_accuracy: 0.8624\n",
            "Epoch 5343/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2807 - accuracy: 0.9038\n",
            "Epoch 5343: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2807 - accuracy: 0.9038 - val_loss: 0.5116 - val_accuracy: 0.8605\n",
            "Epoch 5344/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9010\n",
            "Epoch 5344: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2912 - accuracy: 0.9007 - val_loss: 0.4984 - val_accuracy: 0.8634\n",
            "Epoch 5345/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9016\n",
            "Epoch 5345: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2864 - accuracy: 0.9012 - val_loss: 0.4933 - val_accuracy: 0.8645\n",
            "Epoch 5346/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9011\n",
            "Epoch 5346: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.9011 - val_loss: 0.5011 - val_accuracy: 0.8625\n",
            "Epoch 5347/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9020\n",
            "Epoch 5347: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2855 - accuracy: 0.9019 - val_loss: 0.5028 - val_accuracy: 0.8607\n",
            "Epoch 5348/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9019\n",
            "Epoch 5348: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.9019 - val_loss: 0.4924 - val_accuracy: 0.8636\n",
            "Epoch 5349/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9025\n",
            "Epoch 5349: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2831 - accuracy: 0.9026 - val_loss: 0.4994 - val_accuracy: 0.8627\n",
            "Epoch 5350/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9014\n",
            "Epoch 5350: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9013 - val_loss: 0.4991 - val_accuracy: 0.8600\n",
            "Epoch 5351/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9045\n",
            "Epoch 5351: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2824 - accuracy: 0.9045 - val_loss: 0.4995 - val_accuracy: 0.8621\n",
            "Epoch 5352/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9022\n",
            "Epoch 5352: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2871 - accuracy: 0.9023 - val_loss: 0.4965 - val_accuracy: 0.8620\n",
            "Epoch 5353/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8998\n",
            "Epoch 5353: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2937 - accuracy: 0.8998 - val_loss: 0.4927 - val_accuracy: 0.8642\n",
            "Epoch 5354/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9002\n",
            "Epoch 5354: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2927 - accuracy: 0.9001 - val_loss: 0.4957 - val_accuracy: 0.8624\n",
            "Epoch 5355/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9025\n",
            "Epoch 5355: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2862 - accuracy: 0.9025 - val_loss: 0.5041 - val_accuracy: 0.8618\n",
            "Epoch 5356/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9036\n",
            "Epoch 5356: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2871 - accuracy: 0.9037 - val_loss: 0.4989 - val_accuracy: 0.8628\n",
            "Epoch 5357/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9006\n",
            "Epoch 5357: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2861 - accuracy: 0.9009 - val_loss: 0.4973 - val_accuracy: 0.8629\n",
            "Epoch 5358/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9015\n",
            "Epoch 5358: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2890 - accuracy: 0.9011 - val_loss: 0.5065 - val_accuracy: 0.8618\n",
            "Epoch 5359/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9003\n",
            "Epoch 5359: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9007 - val_loss: 0.4989 - val_accuracy: 0.8621\n",
            "Epoch 5360/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.9044\n",
            "Epoch 5360: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2822 - accuracy: 0.9041 - val_loss: 0.5049 - val_accuracy: 0.8623\n",
            "Epoch 5361/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9025\n",
            "Epoch 5361: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2885 - accuracy: 0.9026 - val_loss: 0.4962 - val_accuracy: 0.8612\n",
            "Epoch 5362/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9026\n",
            "Epoch 5362: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2895 - accuracy: 0.9026 - val_loss: 0.4979 - val_accuracy: 0.8618\n",
            "Epoch 5363/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9008\n",
            "Epoch 5363: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2887 - accuracy: 0.9008 - val_loss: 0.4996 - val_accuracy: 0.8618\n",
            "Epoch 5364/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8995\n",
            "Epoch 5364: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.8997 - val_loss: 0.4921 - val_accuracy: 0.8629\n",
            "Epoch 5365/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.9016\n",
            "Epoch 5365: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2920 - accuracy: 0.9018 - val_loss: 0.4929 - val_accuracy: 0.8618\n",
            "Epoch 5366/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.9028\n",
            "Epoch 5366: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2869 - accuracy: 0.9028 - val_loss: 0.5009 - val_accuracy: 0.8635\n",
            "Epoch 5367/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9022\n",
            "Epoch 5367: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2862 - accuracy: 0.9022 - val_loss: 0.5045 - val_accuracy: 0.8631\n",
            "Epoch 5368/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9029\n",
            "Epoch 5368: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9029 - val_loss: 0.4966 - val_accuracy: 0.8635\n",
            "Epoch 5369/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9016\n",
            "Epoch 5369: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2845 - accuracy: 0.9016 - val_loss: 0.4951 - val_accuracy: 0.8623\n",
            "Epoch 5370/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9018\n",
            "Epoch 5370: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2864 - accuracy: 0.9018 - val_loss: 0.4941 - val_accuracy: 0.8638\n",
            "Epoch 5371/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9020\n",
            "Epoch 5371: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9020 - val_loss: 0.5010 - val_accuracy: 0.8629\n",
            "Epoch 5372/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9021\n",
            "Epoch 5372: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.9020 - val_loss: 0.4979 - val_accuracy: 0.8624\n",
            "Epoch 5373/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9023\n",
            "Epoch 5373: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2836 - accuracy: 0.9023 - val_loss: 0.5004 - val_accuracy: 0.8627\n",
            "Epoch 5374/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9012\n",
            "Epoch 5374: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9011 - val_loss: 0.4956 - val_accuracy: 0.8613\n",
            "Epoch 5375/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9002\n",
            "Epoch 5375: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2929 - accuracy: 0.9003 - val_loss: 0.4969 - val_accuracy: 0.8644\n",
            "Epoch 5376/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9012\n",
            "Epoch 5376: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2926 - accuracy: 0.9012 - val_loss: 0.4951 - val_accuracy: 0.8643\n",
            "Epoch 5377/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9021\n",
            "Epoch 5377: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2866 - accuracy: 0.9023 - val_loss: 0.4963 - val_accuracy: 0.8618\n",
            "Epoch 5378/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9028\n",
            "Epoch 5378: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2876 - accuracy: 0.9027 - val_loss: 0.5006 - val_accuracy: 0.8616\n",
            "Epoch 5379/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9033\n",
            "Epoch 5379: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2839 - accuracy: 0.9033 - val_loss: 0.5014 - val_accuracy: 0.8621\n",
            "Epoch 5380/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9021\n",
            "Epoch 5380: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2899 - accuracy: 0.9022 - val_loss: 0.4970 - val_accuracy: 0.8625\n",
            "Epoch 5381/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9003\n",
            "Epoch 5381: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2900 - accuracy: 0.9004 - val_loss: 0.4984 - val_accuracy: 0.8627\n",
            "Epoch 5382/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8997\n",
            "Epoch 5382: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2920 - accuracy: 0.8997 - val_loss: 0.4935 - val_accuracy: 0.8635\n",
            "Epoch 5383/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.9003\n",
            "Epoch 5383: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2940 - accuracy: 0.9004 - val_loss: 0.4963 - val_accuracy: 0.8621\n",
            "Epoch 5384/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9030\n",
            "Epoch 5384: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2862 - accuracy: 0.9031 - val_loss: 0.4978 - val_accuracy: 0.8635\n",
            "Epoch 5385/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9011\n",
            "Epoch 5385: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2863 - accuracy: 0.9010 - val_loss: 0.4958 - val_accuracy: 0.8624\n",
            "Epoch 5386/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9017\n",
            "Epoch 5386: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2891 - accuracy: 0.9019 - val_loss: 0.4998 - val_accuracy: 0.8619\n",
            "Epoch 5387/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9034\n",
            "Epoch 5387: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2834 - accuracy: 0.9034 - val_loss: 0.5028 - val_accuracy: 0.8624\n",
            "Epoch 5388/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9004\n",
            "Epoch 5388: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2884 - accuracy: 0.9004 - val_loss: 0.4966 - val_accuracy: 0.8618\n",
            "Epoch 5389/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9003\n",
            "Epoch 5389: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2880 - accuracy: 0.9000 - val_loss: 0.4930 - val_accuracy: 0.8635\n",
            "Epoch 5390/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9027\n",
            "Epoch 5390: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2856 - accuracy: 0.9024 - val_loss: 0.5008 - val_accuracy: 0.8631\n",
            "Epoch 5391/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9042\n",
            "Epoch 5391: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2814 - accuracy: 0.9040 - val_loss: 0.4948 - val_accuracy: 0.8610\n",
            "Epoch 5392/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9027\n",
            "Epoch 5392: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2858 - accuracy: 0.9029 - val_loss: 0.4943 - val_accuracy: 0.8622\n",
            "Epoch 5393/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9027\n",
            "Epoch 5393: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2852 - accuracy: 0.9026 - val_loss: 0.4995 - val_accuracy: 0.8631\n",
            "Epoch 5394/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9018\n",
            "Epoch 5394: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9017 - val_loss: 0.5051 - val_accuracy: 0.8617\n",
            "Epoch 5395/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9007\n",
            "Epoch 5395: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2895 - accuracy: 0.9006 - val_loss: 0.4920 - val_accuracy: 0.8646\n",
            "Epoch 5396/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.9019\n",
            "Epoch 5396: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2824 - accuracy: 0.9019 - val_loss: 0.5006 - val_accuracy: 0.8622\n",
            "Epoch 5397/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9016\n",
            "Epoch 5397: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2853 - accuracy: 0.9014 - val_loss: 0.5002 - val_accuracy: 0.8621\n",
            "Epoch 5398/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9024\n",
            "Epoch 5398: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2877 - accuracy: 0.9024 - val_loss: 0.4926 - val_accuracy: 0.8626\n",
            "Epoch 5399/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9021\n",
            "Epoch 5399: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2868 - accuracy: 0.9023 - val_loss: 0.4964 - val_accuracy: 0.8631\n",
            "Epoch 5400/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9021\n",
            "Epoch 5400: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2862 - accuracy: 0.9026 - val_loss: 0.4942 - val_accuracy: 0.8624\n",
            "Epoch 5401/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9011\n",
            "Epoch 5401: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2869 - accuracy: 0.9011 - val_loss: 0.4983 - val_accuracy: 0.8616\n",
            "Epoch 5402/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9022\n",
            "Epoch 5402: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2869 - accuracy: 0.9019 - val_loss: 0.4948 - val_accuracy: 0.8619\n",
            "Epoch 5403/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9034\n",
            "Epoch 5403: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2841 - accuracy: 0.9035 - val_loss: 0.5006 - val_accuracy: 0.8602\n",
            "Epoch 5404/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9023\n",
            "Epoch 5404: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2871 - accuracy: 0.9023 - val_loss: 0.4973 - val_accuracy: 0.8635\n",
            "Epoch 5405/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9032\n",
            "Epoch 5405: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2852 - accuracy: 0.9034 - val_loss: 0.4916 - val_accuracy: 0.8619\n",
            "Epoch 5406/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9025\n",
            "Epoch 5406: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2869 - accuracy: 0.9020 - val_loss: 0.4933 - val_accuracy: 0.8624\n",
            "Epoch 5407/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9036\n",
            "Epoch 5407: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2844 - accuracy: 0.9037 - val_loss: 0.4956 - val_accuracy: 0.8612\n",
            "Epoch 5408/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9035\n",
            "Epoch 5408: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2865 - accuracy: 0.9035 - val_loss: 0.4935 - val_accuracy: 0.8616\n",
            "Epoch 5409/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9024\n",
            "Epoch 5409: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2888 - accuracy: 0.9021 - val_loss: 0.4919 - val_accuracy: 0.8627\n",
            "Epoch 5410/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9001\n",
            "Epoch 5410: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2908 - accuracy: 0.9001 - val_loss: 0.4918 - val_accuracy: 0.8628\n",
            "Epoch 5411/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9021\n",
            "Epoch 5411: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9019 - val_loss: 0.4961 - val_accuracy: 0.8629\n",
            "Epoch 5412/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9006\n",
            "Epoch 5412: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2849 - accuracy: 0.9006 - val_loss: 0.5000 - val_accuracy: 0.8624\n",
            "Epoch 5413/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9042\n",
            "Epoch 5413: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2856 - accuracy: 0.9042 - val_loss: 0.4944 - val_accuracy: 0.8623\n",
            "Epoch 5414/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9021\n",
            "Epoch 5414: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2887 - accuracy: 0.9021 - val_loss: 0.4927 - val_accuracy: 0.8639\n",
            "Epoch 5415/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9022\n",
            "Epoch 5415: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2876 - accuracy: 0.9020 - val_loss: 0.5012 - val_accuracy: 0.8618\n",
            "Epoch 5416/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9009\n",
            "Epoch 5416: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2872 - accuracy: 0.9009 - val_loss: 0.5042 - val_accuracy: 0.8631\n",
            "Epoch 5417/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9033\n",
            "Epoch 5417: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2852 - accuracy: 0.9033 - val_loss: 0.5038 - val_accuracy: 0.8615\n",
            "Epoch 5418/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9016\n",
            "Epoch 5418: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2873 - accuracy: 0.9013 - val_loss: 0.4934 - val_accuracy: 0.8612\n",
            "Epoch 5419/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9008\n",
            "Epoch 5419: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2875 - accuracy: 0.9008 - val_loss: 0.4981 - val_accuracy: 0.8619\n",
            "Epoch 5420/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9020\n",
            "Epoch 5420: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2879 - accuracy: 0.9020 - val_loss: 0.4975 - val_accuracy: 0.8606\n",
            "Epoch 5421/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9004\n",
            "Epoch 5421: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2911 - accuracy: 0.9005 - val_loss: 0.4954 - val_accuracy: 0.8627\n",
            "Epoch 5422/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9021\n",
            "Epoch 5422: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2853 - accuracy: 0.9021 - val_loss: 0.5006 - val_accuracy: 0.8626\n",
            "Epoch 5423/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9014\n",
            "Epoch 5423: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2916 - accuracy: 0.9012 - val_loss: 0.4935 - val_accuracy: 0.8619\n",
            "Epoch 5424/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9000\n",
            "Epoch 5424: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2947 - accuracy: 0.9000 - val_loss: 0.4934 - val_accuracy: 0.8626\n",
            "Epoch 5425/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9023\n",
            "Epoch 5425: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2873 - accuracy: 0.9021 - val_loss: 0.4934 - val_accuracy: 0.8629\n",
            "Epoch 5426/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9005\n",
            "Epoch 5426: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2900 - accuracy: 0.9006 - val_loss: 0.4928 - val_accuracy: 0.8612\n",
            "Epoch 5427/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9019\n",
            "Epoch 5427: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2862 - accuracy: 0.9021 - val_loss: 0.4970 - val_accuracy: 0.8612\n",
            "Epoch 5428/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9018\n",
            "Epoch 5428: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2898 - accuracy: 0.9020 - val_loss: 0.4961 - val_accuracy: 0.8624\n",
            "Epoch 5429/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9007\n",
            "Epoch 5429: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2911 - accuracy: 0.9007 - val_loss: 0.4940 - val_accuracy: 0.8607\n",
            "Epoch 5430/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9013\n",
            "Epoch 5430: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2906 - accuracy: 0.9011 - val_loss: 0.4979 - val_accuracy: 0.8619\n",
            "Epoch 5431/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9003\n",
            "Epoch 5431: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2908 - accuracy: 0.9000 - val_loss: 0.4961 - val_accuracy: 0.8624\n",
            "Epoch 5432/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9012\n",
            "Epoch 5432: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2884 - accuracy: 0.9013 - val_loss: 0.4948 - val_accuracy: 0.8625\n",
            "Epoch 5433/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9005\n",
            "Epoch 5433: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9008 - val_loss: 0.4978 - val_accuracy: 0.8622\n",
            "Epoch 5434/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9029\n",
            "Epoch 5434: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9029 - val_loss: 0.4982 - val_accuracy: 0.8627\n",
            "Epoch 5435/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9016\n",
            "Epoch 5435: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2876 - accuracy: 0.9016 - val_loss: 0.4920 - val_accuracy: 0.8621\n",
            "Epoch 5436/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9018\n",
            "Epoch 5436: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2892 - accuracy: 0.9018 - val_loss: 0.4956 - val_accuracy: 0.8635\n",
            "Epoch 5437/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9026\n",
            "Epoch 5437: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.9023 - val_loss: 0.4961 - val_accuracy: 0.8627\n",
            "Epoch 5438/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9015\n",
            "Epoch 5438: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9015 - val_loss: 0.4968 - val_accuracy: 0.8638\n",
            "Epoch 5439/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9018\n",
            "Epoch 5439: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2904 - accuracy: 0.9016 - val_loss: 0.4980 - val_accuracy: 0.8609\n",
            "Epoch 5440/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9011\n",
            "Epoch 5440: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2881 - accuracy: 0.9012 - val_loss: 0.5013 - val_accuracy: 0.8623\n",
            "Epoch 5441/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9021\n",
            "Epoch 5441: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2847 - accuracy: 0.9023 - val_loss: 0.4974 - val_accuracy: 0.8630\n",
            "Epoch 5442/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9025\n",
            "Epoch 5442: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2857 - accuracy: 0.9030 - val_loss: 0.5032 - val_accuracy: 0.8624\n",
            "Epoch 5443/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9010\n",
            "Epoch 5443: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2891 - accuracy: 0.9012 - val_loss: 0.4931 - val_accuracy: 0.8626\n",
            "Epoch 5444/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9024\n",
            "Epoch 5444: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9023 - val_loss: 0.4971 - val_accuracy: 0.8622\n",
            "Epoch 5445/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9028\n",
            "Epoch 5445: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2875 - accuracy: 0.9030 - val_loss: 0.4956 - val_accuracy: 0.8613\n",
            "Epoch 5446/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9043\n",
            "Epoch 5446: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2838 - accuracy: 0.9044 - val_loss: 0.4987 - val_accuracy: 0.8626\n",
            "Epoch 5447/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9024\n",
            "Epoch 5447: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2820 - accuracy: 0.9026 - val_loss: 0.5035 - val_accuracy: 0.8628\n",
            "Epoch 5448/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9011\n",
            "Epoch 5448: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2930 - accuracy: 0.9008 - val_loss: 0.4941 - val_accuracy: 0.8618\n",
            "Epoch 5449/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9014\n",
            "Epoch 5449: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2855 - accuracy: 0.9012 - val_loss: 0.5007 - val_accuracy: 0.8616\n",
            "Epoch 5450/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.9022\n",
            "Epoch 5450: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2845 - accuracy: 0.9024 - val_loss: 0.4968 - val_accuracy: 0.8622\n",
            "Epoch 5451/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9000\n",
            "Epoch 5451: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2877 - accuracy: 0.9000 - val_loss: 0.4970 - val_accuracy: 0.8624\n",
            "Epoch 5452/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8992\n",
            "Epoch 5452: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.8993 - val_loss: 0.4995 - val_accuracy: 0.8610\n",
            "Epoch 5453/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9016\n",
            "Epoch 5453: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9018 - val_loss: 0.5029 - val_accuracy: 0.8604\n",
            "Epoch 5454/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9011\n",
            "Epoch 5454: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2873 - accuracy: 0.9011 - val_loss: 0.4988 - val_accuracy: 0.8636\n",
            "Epoch 5455/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9009\n",
            "Epoch 5455: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2875 - accuracy: 0.9011 - val_loss: 0.5009 - val_accuracy: 0.8619\n",
            "Epoch 5456/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9009\n",
            "Epoch 5456: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2864 - accuracy: 0.9009 - val_loss: 0.4967 - val_accuracy: 0.8622\n",
            "Epoch 5457/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9016\n",
            "Epoch 5457: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2843 - accuracy: 0.9016 - val_loss: 0.5013 - val_accuracy: 0.8626\n",
            "Epoch 5458/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9029\n",
            "Epoch 5458: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2892 - accuracy: 0.9029 - val_loss: 0.4914 - val_accuracy: 0.8631\n",
            "Epoch 5459/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9030\n",
            "Epoch 5459: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2876 - accuracy: 0.9030 - val_loss: 0.4945 - val_accuracy: 0.8615\n",
            "Epoch 5460/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8994\n",
            "Epoch 5460: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2932 - accuracy: 0.8992 - val_loss: 0.4948 - val_accuracy: 0.8606\n",
            "Epoch 5461/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.9009\n",
            "Epoch 5461: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2908 - accuracy: 0.9012 - val_loss: 0.4995 - val_accuracy: 0.8585\n",
            "Epoch 5462/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9011\n",
            "Epoch 5462: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2887 - accuracy: 0.9012 - val_loss: 0.4965 - val_accuracy: 0.8629\n",
            "Epoch 5463/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9025\n",
            "Epoch 5463: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2849 - accuracy: 0.9022 - val_loss: 0.5027 - val_accuracy: 0.8614\n",
            "Epoch 5464/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9007\n",
            "Epoch 5464: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.9003 - val_loss: 0.4975 - val_accuracy: 0.8613\n",
            "Epoch 5465/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9017\n",
            "Epoch 5465: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2869 - accuracy: 0.9019 - val_loss: 0.4981 - val_accuracy: 0.8629\n",
            "Epoch 5466/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9018\n",
            "Epoch 5466: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2897 - accuracy: 0.9018 - val_loss: 0.4905 - val_accuracy: 0.8636\n",
            "Epoch 5467/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9021\n",
            "Epoch 5467: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2863 - accuracy: 0.9021 - val_loss: 0.4959 - val_accuracy: 0.8611\n",
            "Epoch 5468/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9017\n",
            "Epoch 5468: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2877 - accuracy: 0.9015 - val_loss: 0.4989 - val_accuracy: 0.8625\n",
            "Epoch 5469/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9006\n",
            "Epoch 5469: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.9006 - val_loss: 0.4915 - val_accuracy: 0.8630\n",
            "Epoch 5470/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9005\n",
            "Epoch 5470: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2895 - accuracy: 0.9008 - val_loss: 0.4942 - val_accuracy: 0.8626\n",
            "Epoch 5471/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9011\n",
            "Epoch 5471: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9011 - val_loss: 0.4916 - val_accuracy: 0.8646\n",
            "Epoch 5472/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9012\n",
            "Epoch 5472: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2878 - accuracy: 0.9013 - val_loss: 0.4930 - val_accuracy: 0.8635\n",
            "Epoch 5473/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9012\n",
            "Epoch 5473: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.9013 - val_loss: 0.4994 - val_accuracy: 0.8614\n",
            "Epoch 5474/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9018\n",
            "Epoch 5474: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2877 - accuracy: 0.9019 - val_loss: 0.4955 - val_accuracy: 0.8621\n",
            "Epoch 5475/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9020\n",
            "Epoch 5475: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2882 - accuracy: 0.9020 - val_loss: 0.4923 - val_accuracy: 0.8612\n",
            "Epoch 5476/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9020\n",
            "Epoch 5476: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2861 - accuracy: 0.9020 - val_loss: 0.4937 - val_accuracy: 0.8606\n",
            "Epoch 5477/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9029\n",
            "Epoch 5477: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9030 - val_loss: 0.4993 - val_accuracy: 0.8615\n",
            "Epoch 5478/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9006\n",
            "Epoch 5478: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9001 - val_loss: 0.4958 - val_accuracy: 0.8614\n",
            "Epoch 5479/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9037\n",
            "Epoch 5479: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2863 - accuracy: 0.9033 - val_loss: 0.4965 - val_accuracy: 0.8601\n",
            "Epoch 5480/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9014\n",
            "Epoch 5480: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2869 - accuracy: 0.9011 - val_loss: 0.4982 - val_accuracy: 0.8607\n",
            "Epoch 5481/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.9018\n",
            "Epoch 5481: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2832 - accuracy: 0.9018 - val_loss: 0.5050 - val_accuracy: 0.8624\n",
            "Epoch 5482/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9021\n",
            "Epoch 5482: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2845 - accuracy: 0.9020 - val_loss: 0.4975 - val_accuracy: 0.8618\n",
            "Epoch 5483/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9026\n",
            "Epoch 5483: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2841 - accuracy: 0.9026 - val_loss: 0.5023 - val_accuracy: 0.8608\n",
            "Epoch 5484/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9005\n",
            "Epoch 5484: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2903 - accuracy: 0.9009 - val_loss: 0.4963 - val_accuracy: 0.8604\n",
            "Epoch 5485/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9031\n",
            "Epoch 5485: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2863 - accuracy: 0.9031 - val_loss: 0.4951 - val_accuracy: 0.8612\n",
            "Epoch 5486/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.9031\n",
            "Epoch 5486: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2850 - accuracy: 0.9030 - val_loss: 0.4969 - val_accuracy: 0.8607\n",
            "Epoch 5487/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9011\n",
            "Epoch 5487: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2882 - accuracy: 0.9011 - val_loss: 0.5044 - val_accuracy: 0.8613\n",
            "Epoch 5488/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9022\n",
            "Epoch 5488: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2888 - accuracy: 0.9023 - val_loss: 0.4942 - val_accuracy: 0.8591\n",
            "Epoch 5489/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9023\n",
            "Epoch 5489: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.9023 - val_loss: 0.4941 - val_accuracy: 0.8599\n",
            "Epoch 5490/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9004\n",
            "Epoch 5490: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2898 - accuracy: 0.9006 - val_loss: 0.4965 - val_accuracy: 0.8614\n",
            "Epoch 5491/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9008\n",
            "Epoch 5491: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2896 - accuracy: 0.9008 - val_loss: 0.4933 - val_accuracy: 0.8635\n",
            "Epoch 5492/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9017\n",
            "Epoch 5492: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2864 - accuracy: 0.9015 - val_loss: 0.4998 - val_accuracy: 0.8605\n",
            "Epoch 5493/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9016\n",
            "Epoch 5493: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2891 - accuracy: 0.9016 - val_loss: 0.4973 - val_accuracy: 0.8626\n",
            "Epoch 5494/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9013\n",
            "Epoch 5494: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2907 - accuracy: 0.9011 - val_loss: 0.4975 - val_accuracy: 0.8618\n",
            "Epoch 5495/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9012\n",
            "Epoch 5495: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2896 - accuracy: 0.9011 - val_loss: 0.5052 - val_accuracy: 0.8630\n",
            "Epoch 5496/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9014\n",
            "Epoch 5496: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2897 - accuracy: 0.9013 - val_loss: 0.4969 - val_accuracy: 0.8634\n",
            "Epoch 5497/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9023\n",
            "Epoch 5497: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2850 - accuracy: 0.9023 - val_loss: 0.4973 - val_accuracy: 0.8625\n",
            "Epoch 5498/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9037\n",
            "Epoch 5498: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2813 - accuracy: 0.9037 - val_loss: 0.4996 - val_accuracy: 0.8624\n",
            "Epoch 5499/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9020\n",
            "Epoch 5499: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9017 - val_loss: 0.4995 - val_accuracy: 0.8615\n",
            "Epoch 5500/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8995\n",
            "Epoch 5500: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2923 - accuracy: 0.8996 - val_loss: 0.4918 - val_accuracy: 0.8606\n",
            "Epoch 5501/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9002\n",
            "Epoch 5501: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2898 - accuracy: 0.9004 - val_loss: 0.4923 - val_accuracy: 0.8606\n",
            "Epoch 5502/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9015\n",
            "Epoch 5502: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2895 - accuracy: 0.9015 - val_loss: 0.4955 - val_accuracy: 0.8607\n",
            "Epoch 5503/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9043\n",
            "Epoch 5503: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2846 - accuracy: 0.9041 - val_loss: 0.4971 - val_accuracy: 0.8618\n",
            "Epoch 5504/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.9018\n",
            "Epoch 5504: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2859 - accuracy: 0.9018 - val_loss: 0.4954 - val_accuracy: 0.8612\n",
            "Epoch 5505/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9027\n",
            "Epoch 5505: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2818 - accuracy: 0.9027 - val_loss: 0.5038 - val_accuracy: 0.8605\n",
            "Epoch 5506/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9011\n",
            "Epoch 5506: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9010 - val_loss: 0.4949 - val_accuracy: 0.8621\n",
            "Epoch 5507/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9022\n",
            "Epoch 5507: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2880 - accuracy: 0.9022 - val_loss: 0.5009 - val_accuracy: 0.8631\n",
            "Epoch 5508/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9011\n",
            "Epoch 5508: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2913 - accuracy: 0.9006 - val_loss: 0.4936 - val_accuracy: 0.8617\n",
            "Epoch 5509/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9024\n",
            "Epoch 5509: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2891 - accuracy: 0.9024 - val_loss: 0.4985 - val_accuracy: 0.8601\n",
            "Epoch 5510/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9026\n",
            "Epoch 5510: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2872 - accuracy: 0.9026 - val_loss: 0.4941 - val_accuracy: 0.8617\n",
            "Epoch 5511/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9032\n",
            "Epoch 5511: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2865 - accuracy: 0.9032 - val_loss: 0.4961 - val_accuracy: 0.8591\n",
            "Epoch 5512/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9017\n",
            "Epoch 5512: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2868 - accuracy: 0.9017 - val_loss: 0.4974 - val_accuracy: 0.8595\n",
            "Epoch 5513/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.9020\n",
            "Epoch 5513: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2874 - accuracy: 0.9020 - val_loss: 0.4990 - val_accuracy: 0.8612\n",
            "Epoch 5514/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9029\n",
            "Epoch 5514: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2855 - accuracy: 0.9028 - val_loss: 0.5003 - val_accuracy: 0.8611\n",
            "Epoch 5515/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9022\n",
            "Epoch 5515: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2853 - accuracy: 0.9022 - val_loss: 0.5034 - val_accuracy: 0.8601\n",
            "Epoch 5516/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9055\n",
            "Epoch 5516: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2823 - accuracy: 0.9054 - val_loss: 0.5020 - val_accuracy: 0.8616\n",
            "Epoch 5517/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9011\n",
            "Epoch 5517: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.9010 - val_loss: 0.5034 - val_accuracy: 0.8604\n",
            "Epoch 5518/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.9006\n",
            "Epoch 5518: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2869 - accuracy: 0.9006 - val_loss: 0.4993 - val_accuracy: 0.8607\n",
            "Epoch 5519/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9009\n",
            "Epoch 5519: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2877 - accuracy: 0.9009 - val_loss: 0.4917 - val_accuracy: 0.8605\n",
            "Epoch 5520/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9017\n",
            "Epoch 5520: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2887 - accuracy: 0.9021 - val_loss: 0.4965 - val_accuracy: 0.8608\n",
            "Epoch 5521/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9015\n",
            "Epoch 5521: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2879 - accuracy: 0.9014 - val_loss: 0.4934 - val_accuracy: 0.8620\n",
            "Epoch 5522/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9018\n",
            "Epoch 5522: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2865 - accuracy: 0.9018 - val_loss: 0.5002 - val_accuracy: 0.8613\n",
            "Epoch 5523/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9044\n",
            "Epoch 5523: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2850 - accuracy: 0.9043 - val_loss: 0.4969 - val_accuracy: 0.8609\n",
            "Epoch 5524/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9038\n",
            "Epoch 5524: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2858 - accuracy: 0.9039 - val_loss: 0.4970 - val_accuracy: 0.8601\n",
            "Epoch 5525/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9011\n",
            "Epoch 5525: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.9011 - val_loss: 0.5008 - val_accuracy: 0.8609\n",
            "Epoch 5526/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9020\n",
            "Epoch 5526: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2868 - accuracy: 0.9018 - val_loss: 0.4996 - val_accuracy: 0.8614\n",
            "Epoch 5527/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9033\n",
            "Epoch 5527: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2879 - accuracy: 0.9032 - val_loss: 0.5013 - val_accuracy: 0.8609\n",
            "Epoch 5528/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9009\n",
            "Epoch 5528: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2897 - accuracy: 0.9009 - val_loss: 0.4932 - val_accuracy: 0.8620\n",
            "Epoch 5529/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9025\n",
            "Epoch 5529: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9025 - val_loss: 0.4949 - val_accuracy: 0.8621\n",
            "Epoch 5530/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9022\n",
            "Epoch 5530: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2873 - accuracy: 0.9022 - val_loss: 0.5000 - val_accuracy: 0.8606\n",
            "Epoch 5531/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9022\n",
            "Epoch 5531: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2909 - accuracy: 0.9021 - val_loss: 0.4896 - val_accuracy: 0.8605\n",
            "Epoch 5532/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9018\n",
            "Epoch 5532: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.9016 - val_loss: 0.4939 - val_accuracy: 0.8601\n",
            "Epoch 5533/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9011\n",
            "Epoch 5533: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2896 - accuracy: 0.9013 - val_loss: 0.4893 - val_accuracy: 0.8612\n",
            "Epoch 5534/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.8989\n",
            "Epoch 5534: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2893 - accuracy: 0.8991 - val_loss: 0.4959 - val_accuracy: 0.8618\n",
            "Epoch 5535/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.9032\n",
            "Epoch 5535: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2830 - accuracy: 0.9032 - val_loss: 0.4950 - val_accuracy: 0.8618\n",
            "Epoch 5536/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9014\n",
            "Epoch 5536: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2899 - accuracy: 0.9014 - val_loss: 0.4960 - val_accuracy: 0.8632\n",
            "Epoch 5537/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9010\n",
            "Epoch 5537: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2898 - accuracy: 0.9010 - val_loss: 0.5031 - val_accuracy: 0.8615\n",
            "Epoch 5538/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.8998\n",
            "Epoch 5538: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2854 - accuracy: 0.8998 - val_loss: 0.5034 - val_accuracy: 0.8627\n",
            "Epoch 5539/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.8991\n",
            "Epoch 5539: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2935 - accuracy: 0.8991 - val_loss: 0.4970 - val_accuracy: 0.8621\n",
            "Epoch 5540/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9015\n",
            "Epoch 5540: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2924 - accuracy: 0.9015 - val_loss: 0.4983 - val_accuracy: 0.8621\n",
            "Epoch 5541/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9010\n",
            "Epoch 5541: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2914 - accuracy: 0.9010 - val_loss: 0.4920 - val_accuracy: 0.8634\n",
            "Epoch 5542/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9016\n",
            "Epoch 5542: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2880 - accuracy: 0.9013 - val_loss: 0.4929 - val_accuracy: 0.8624\n",
            "Epoch 5543/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9014\n",
            "Epoch 5543: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9014 - val_loss: 0.4929 - val_accuracy: 0.8621\n",
            "Epoch 5544/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9002\n",
            "Epoch 5544: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2912 - accuracy: 0.9002 - val_loss: 0.4902 - val_accuracy: 0.8643\n",
            "Epoch 5545/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.9019\n",
            "Epoch 5545: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2859 - accuracy: 0.9019 - val_loss: 0.4991 - val_accuracy: 0.8640\n",
            "Epoch 5546/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.9002\n",
            "Epoch 5546: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2910 - accuracy: 0.9002 - val_loss: 0.4983 - val_accuracy: 0.8620\n",
            "Epoch 5547/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9023\n",
            "Epoch 5547: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2875 - accuracy: 0.9027 - val_loss: 0.4948 - val_accuracy: 0.8629\n",
            "Epoch 5548/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.9014\n",
            "Epoch 5548: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.9014 - val_loss: 0.4959 - val_accuracy: 0.8632\n",
            "Epoch 5549/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.9049\n",
            "Epoch 5549: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2843 - accuracy: 0.9049 - val_loss: 0.4944 - val_accuracy: 0.8630\n",
            "Epoch 5550/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9021\n",
            "Epoch 5550: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2864 - accuracy: 0.9022 - val_loss: 0.5010 - val_accuracy: 0.8626\n",
            "Epoch 5551/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9023\n",
            "Epoch 5551: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2880 - accuracy: 0.9020 - val_loss: 0.4955 - val_accuracy: 0.8623\n",
            "Epoch 5552/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9028\n",
            "Epoch 5552: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2856 - accuracy: 0.9026 - val_loss: 0.4995 - val_accuracy: 0.8611\n",
            "Epoch 5553/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9018\n",
            "Epoch 5553: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2882 - accuracy: 0.9021 - val_loss: 0.4980 - val_accuracy: 0.8622\n",
            "Epoch 5554/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9034\n",
            "Epoch 5554: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2855 - accuracy: 0.9034 - val_loss: 0.4935 - val_accuracy: 0.8628\n",
            "Epoch 5555/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9014\n",
            "Epoch 5555: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2876 - accuracy: 0.9014 - val_loss: 0.5005 - val_accuracy: 0.8607\n",
            "Epoch 5556/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9033\n",
            "Epoch 5556: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2837 - accuracy: 0.9035 - val_loss: 0.4960 - val_accuracy: 0.8612\n",
            "Epoch 5557/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9007\n",
            "Epoch 5557: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.9007 - val_loss: 0.4921 - val_accuracy: 0.8623\n",
            "Epoch 5558/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9018\n",
            "Epoch 5558: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.9018 - val_loss: 0.5010 - val_accuracy: 0.8604\n",
            "Epoch 5559/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9021\n",
            "Epoch 5559: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2852 - accuracy: 0.9021 - val_loss: 0.4925 - val_accuracy: 0.8624\n",
            "Epoch 5560/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9020\n",
            "Epoch 5560: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2891 - accuracy: 0.9021 - val_loss: 0.4972 - val_accuracy: 0.8620\n",
            "Epoch 5561/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9018\n",
            "Epoch 5561: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2868 - accuracy: 0.9018 - val_loss: 0.4994 - val_accuracy: 0.8612\n",
            "Epoch 5562/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9023\n",
            "Epoch 5562: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2845 - accuracy: 0.9023 - val_loss: 0.4986 - val_accuracy: 0.8621\n",
            "Epoch 5563/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9022\n",
            "Epoch 5563: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2875 - accuracy: 0.9023 - val_loss: 0.5005 - val_accuracy: 0.8610\n",
            "Epoch 5564/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.9031\n",
            "Epoch 5564: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2836 - accuracy: 0.9028 - val_loss: 0.5019 - val_accuracy: 0.8618\n",
            "Epoch 5565/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9024\n",
            "Epoch 5565: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9021 - val_loss: 0.4974 - val_accuracy: 0.8624\n",
            "Epoch 5566/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9020\n",
            "Epoch 5566: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2864 - accuracy: 0.9021 - val_loss: 0.4954 - val_accuracy: 0.8638\n",
            "Epoch 5567/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9023\n",
            "Epoch 5567: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2894 - accuracy: 0.9019 - val_loss: 0.4951 - val_accuracy: 0.8636\n",
            "Epoch 5568/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9031\n",
            "Epoch 5568: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2848 - accuracy: 0.9031 - val_loss: 0.5013 - val_accuracy: 0.8631\n",
            "Epoch 5569/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.9023\n",
            "Epoch 5569: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2821 - accuracy: 0.9027 - val_loss: 0.4988 - val_accuracy: 0.8629\n",
            "Epoch 5570/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9024\n",
            "Epoch 5570: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2859 - accuracy: 0.9022 - val_loss: 0.5034 - val_accuracy: 0.8596\n",
            "Epoch 5571/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9035\n",
            "Epoch 5571: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2882 - accuracy: 0.9037 - val_loss: 0.4964 - val_accuracy: 0.8620\n",
            "Epoch 5572/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9023\n",
            "Epoch 5572: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2858 - accuracy: 0.9023 - val_loss: 0.4997 - val_accuracy: 0.8629\n",
            "Epoch 5573/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9007\n",
            "Epoch 5573: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2873 - accuracy: 0.9003 - val_loss: 0.4977 - val_accuracy: 0.8609\n",
            "Epoch 5574/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9018\n",
            "Epoch 5574: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2884 - accuracy: 0.9016 - val_loss: 0.4965 - val_accuracy: 0.8630\n",
            "Epoch 5575/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9034\n",
            "Epoch 5575: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2841 - accuracy: 0.9037 - val_loss: 0.4951 - val_accuracy: 0.8627\n",
            "Epoch 5576/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9027\n",
            "Epoch 5576: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2844 - accuracy: 0.9027 - val_loss: 0.4992 - val_accuracy: 0.8635\n",
            "Epoch 5577/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.9024\n",
            "Epoch 5577: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2837 - accuracy: 0.9024 - val_loss: 0.5030 - val_accuracy: 0.8622\n",
            "Epoch 5578/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9013\n",
            "Epoch 5578: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2891 - accuracy: 0.9011 - val_loss: 0.5012 - val_accuracy: 0.8608\n",
            "Epoch 5579/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9011\n",
            "Epoch 5579: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2894 - accuracy: 0.9011 - val_loss: 0.4978 - val_accuracy: 0.8612\n",
            "Epoch 5580/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9015\n",
            "Epoch 5580: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2885 - accuracy: 0.9015 - val_loss: 0.5028 - val_accuracy: 0.8609\n",
            "Epoch 5581/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9016\n",
            "Epoch 5581: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2861 - accuracy: 0.9017 - val_loss: 0.4978 - val_accuracy: 0.8617\n",
            "Epoch 5582/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9016\n",
            "Epoch 5582: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2848 - accuracy: 0.9016 - val_loss: 0.4979 - val_accuracy: 0.8639\n",
            "Epoch 5583/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9023\n",
            "Epoch 5583: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9023 - val_loss: 0.5000 - val_accuracy: 0.8616\n",
            "Epoch 5584/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.9030\n",
            "Epoch 5584: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2884 - accuracy: 0.9030 - val_loss: 0.4989 - val_accuracy: 0.8626\n",
            "Epoch 5585/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9011\n",
            "Epoch 5585: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2839 - accuracy: 0.9011 - val_loss: 0.5018 - val_accuracy: 0.8629\n",
            "Epoch 5586/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9003\n",
            "Epoch 5586: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2898 - accuracy: 0.9003 - val_loss: 0.4982 - val_accuracy: 0.8615\n",
            "Epoch 5587/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9027\n",
            "Epoch 5587: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2849 - accuracy: 0.9027 - val_loss: 0.4949 - val_accuracy: 0.8616\n",
            "Epoch 5588/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9003\n",
            "Epoch 5588: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2890 - accuracy: 0.9003 - val_loss: 0.4949 - val_accuracy: 0.8612\n",
            "Epoch 5589/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9032\n",
            "Epoch 5589: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2859 - accuracy: 0.9030 - val_loss: 0.4968 - val_accuracy: 0.8627\n",
            "Epoch 5590/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9026\n",
            "Epoch 5590: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9028 - val_loss: 0.4992 - val_accuracy: 0.8599\n",
            "Epoch 5591/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9028\n",
            "Epoch 5591: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2851 - accuracy: 0.9028 - val_loss: 0.4984 - val_accuracy: 0.8608\n",
            "Epoch 5592/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9033\n",
            "Epoch 5592: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2829 - accuracy: 0.9033 - val_loss: 0.4976 - val_accuracy: 0.8619\n",
            "Epoch 5593/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.9031\n",
            "Epoch 5593: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2840 - accuracy: 0.9031 - val_loss: 0.4983 - val_accuracy: 0.8629\n",
            "Epoch 5594/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8994\n",
            "Epoch 5594: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2912 - accuracy: 0.8994 - val_loss: 0.4985 - val_accuracy: 0.8604\n",
            "Epoch 5595/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9020\n",
            "Epoch 5595: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2850 - accuracy: 0.9021 - val_loss: 0.4997 - val_accuracy: 0.8627\n",
            "Epoch 5596/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.9006\n",
            "Epoch 5596: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2889 - accuracy: 0.9006 - val_loss: 0.4928 - val_accuracy: 0.8628\n",
            "Epoch 5597/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9021\n",
            "Epoch 5597: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2861 - accuracy: 0.9018 - val_loss: 0.4963 - val_accuracy: 0.8619\n",
            "Epoch 5598/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9038\n",
            "Epoch 5598: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2849 - accuracy: 0.9038 - val_loss: 0.4996 - val_accuracy: 0.8615\n",
            "Epoch 5599/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9027\n",
            "Epoch 5599: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2850 - accuracy: 0.9026 - val_loss: 0.4965 - val_accuracy: 0.8622\n",
            "Epoch 5600/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.9032\n",
            "Epoch 5600: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9029 - val_loss: 0.5017 - val_accuracy: 0.8623\n",
            "Epoch 5601/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9011\n",
            "Epoch 5601: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2875 - accuracy: 0.9008 - val_loss: 0.4955 - val_accuracy: 0.8630\n",
            "Epoch 5602/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9032\n",
            "Epoch 5602: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2857 - accuracy: 0.9032 - val_loss: 0.4962 - val_accuracy: 0.8636\n",
            "Epoch 5603/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9019\n",
            "Epoch 5603: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2864 - accuracy: 0.9019 - val_loss: 0.4961 - val_accuracy: 0.8643\n",
            "Epoch 5604/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9031\n",
            "Epoch 5604: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2879 - accuracy: 0.9031 - val_loss: 0.4887 - val_accuracy: 0.8641\n",
            "Epoch 5605/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9004\n",
            "Epoch 5605: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2886 - accuracy: 0.9004 - val_loss: 0.4933 - val_accuracy: 0.8624\n",
            "Epoch 5606/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9025\n",
            "Epoch 5606: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2861 - accuracy: 0.9025 - val_loss: 0.4989 - val_accuracy: 0.8614\n",
            "Epoch 5607/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9000\n",
            "Epoch 5607: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2890 - accuracy: 0.8999 - val_loss: 0.4931 - val_accuracy: 0.8632\n",
            "Epoch 5608/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9028\n",
            "Epoch 5608: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2855 - accuracy: 0.9029 - val_loss: 0.5004 - val_accuracy: 0.8631\n",
            "Epoch 5609/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9018\n",
            "Epoch 5609: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9017 - val_loss: 0.4931 - val_accuracy: 0.8637\n",
            "Epoch 5610/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9016\n",
            "Epoch 5610: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2892 - accuracy: 0.9016 - val_loss: 0.4971 - val_accuracy: 0.8618\n",
            "Epoch 5611/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9002\n",
            "Epoch 5611: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2876 - accuracy: 0.9004 - val_loss: 0.4997 - val_accuracy: 0.8617\n",
            "Epoch 5612/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9012\n",
            "Epoch 5612: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2917 - accuracy: 0.9012 - val_loss: 0.4950 - val_accuracy: 0.8625\n",
            "Epoch 5613/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.9035\n",
            "Epoch 5613: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2841 - accuracy: 0.9035 - val_loss: 0.4945 - val_accuracy: 0.8624\n",
            "Epoch 5614/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8999\n",
            "Epoch 5614: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2921 - accuracy: 0.9001 - val_loss: 0.4959 - val_accuracy: 0.8609\n",
            "Epoch 5615/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9020\n",
            "Epoch 5615: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2888 - accuracy: 0.9020 - val_loss: 0.4969 - val_accuracy: 0.8630\n",
            "Epoch 5616/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9022\n",
            "Epoch 5616: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2857 - accuracy: 0.9022 - val_loss: 0.4994 - val_accuracy: 0.8624\n",
            "Epoch 5617/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9017\n",
            "Epoch 5617: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9022 - val_loss: 0.4935 - val_accuracy: 0.8632\n",
            "Epoch 5618/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9019\n",
            "Epoch 5618: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2880 - accuracy: 0.9018 - val_loss: 0.4920 - val_accuracy: 0.8632\n",
            "Epoch 5619/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9030\n",
            "Epoch 5619: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2840 - accuracy: 0.9029 - val_loss: 0.4981 - val_accuracy: 0.8627\n",
            "Epoch 5620/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9005\n",
            "Epoch 5620: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2892 - accuracy: 0.9009 - val_loss: 0.4976 - val_accuracy: 0.8610\n",
            "Epoch 5621/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9020\n",
            "Epoch 5621: val_accuracy did not improve from 0.86499\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2833 - accuracy: 0.9022 - val_loss: 0.4975 - val_accuracy: 0.8624\n",
            "Epoch 5622/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9023\n",
            "Epoch 5622: val_accuracy improved from 0.86499 to 0.86516, saving model to best_weights.h5\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2860 - accuracy: 0.9020 - val_loss: 0.4901 - val_accuracy: 0.8652\n",
            "Epoch 5623/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9008\n",
            "Epoch 5623: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2859 - accuracy: 0.9008 - val_loss: 0.5041 - val_accuracy: 0.8627\n",
            "Epoch 5624/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9005\n",
            "Epoch 5624: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2883 - accuracy: 0.9006 - val_loss: 0.5000 - val_accuracy: 0.8618\n",
            "Epoch 5625/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9008\n",
            "Epoch 5625: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2852 - accuracy: 0.9008 - val_loss: 0.4994 - val_accuracy: 0.8628\n",
            "Epoch 5626/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9014\n",
            "Epoch 5626: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2856 - accuracy: 0.9016 - val_loss: 0.5034 - val_accuracy: 0.8622\n",
            "Epoch 5627/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9043\n",
            "Epoch 5627: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2832 - accuracy: 0.9046 - val_loss: 0.5002 - val_accuracy: 0.8620\n",
            "Epoch 5628/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9028\n",
            "Epoch 5628: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2840 - accuracy: 0.9029 - val_loss: 0.4976 - val_accuracy: 0.8628\n",
            "Epoch 5629/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.9023\n",
            "Epoch 5629: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2859 - accuracy: 0.9023 - val_loss: 0.5027 - val_accuracy: 0.8627\n",
            "Epoch 5630/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9029\n",
            "Epoch 5630: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2866 - accuracy: 0.9028 - val_loss: 0.5011 - val_accuracy: 0.8607\n",
            "Epoch 5631/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8994\n",
            "Epoch 5631: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2897 - accuracy: 0.8994 - val_loss: 0.5025 - val_accuracy: 0.8611\n",
            "Epoch 5632/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.9019\n",
            "Epoch 5632: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2879 - accuracy: 0.9021 - val_loss: 0.5003 - val_accuracy: 0.8625\n",
            "Epoch 5633/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9036\n",
            "Epoch 5633: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2808 - accuracy: 0.9036 - val_loss: 0.5004 - val_accuracy: 0.8621\n",
            "Epoch 5634/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.9053\n",
            "Epoch 5634: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2789 - accuracy: 0.9053 - val_loss: 0.4983 - val_accuracy: 0.8631\n",
            "Epoch 5635/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9022\n",
            "Epoch 5635: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2861 - accuracy: 0.9022 - val_loss: 0.4993 - val_accuracy: 0.8610\n",
            "Epoch 5636/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9030\n",
            "Epoch 5636: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9029 - val_loss: 0.4966 - val_accuracy: 0.8619\n",
            "Epoch 5637/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9015\n",
            "Epoch 5637: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2855 - accuracy: 0.9015 - val_loss: 0.4979 - val_accuracy: 0.8623\n",
            "Epoch 5638/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8998\n",
            "Epoch 5638: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2886 - accuracy: 0.9004 - val_loss: 0.4964 - val_accuracy: 0.8609\n",
            "Epoch 5639/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.9023\n",
            "Epoch 5639: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9024 - val_loss: 0.4933 - val_accuracy: 0.8631\n",
            "Epoch 5640/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2898 - accuracy: 0.9013\n",
            "Epoch 5640: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2898 - accuracy: 0.9013 - val_loss: 0.4946 - val_accuracy: 0.8613\n",
            "Epoch 5641/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.9035\n",
            "Epoch 5641: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2833 - accuracy: 0.9035 - val_loss: 0.4970 - val_accuracy: 0.8622\n",
            "Epoch 5642/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9024\n",
            "Epoch 5642: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2837 - accuracy: 0.9025 - val_loss: 0.5012 - val_accuracy: 0.8607\n",
            "Epoch 5643/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9032\n",
            "Epoch 5643: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2847 - accuracy: 0.9032 - val_loss: 0.4986 - val_accuracy: 0.8630\n",
            "Epoch 5644/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9037\n",
            "Epoch 5644: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2864 - accuracy: 0.9036 - val_loss: 0.4968 - val_accuracy: 0.8612\n",
            "Epoch 5645/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9031\n",
            "Epoch 5645: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2819 - accuracy: 0.9031 - val_loss: 0.4997 - val_accuracy: 0.8617\n",
            "Epoch 5646/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9045\n",
            "Epoch 5646: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2843 - accuracy: 0.9045 - val_loss: 0.4972 - val_accuracy: 0.8628\n",
            "Epoch 5647/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9027\n",
            "Epoch 5647: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2839 - accuracy: 0.9027 - val_loss: 0.4977 - val_accuracy: 0.8622\n",
            "Epoch 5648/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9008\n",
            "Epoch 5648: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2897 - accuracy: 0.9007 - val_loss: 0.4958 - val_accuracy: 0.8624\n",
            "Epoch 5649/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9031\n",
            "Epoch 5649: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2884 - accuracy: 0.9031 - val_loss: 0.4994 - val_accuracy: 0.8614\n",
            "Epoch 5650/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9015\n",
            "Epoch 5650: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2869 - accuracy: 0.9016 - val_loss: 0.4995 - val_accuracy: 0.8625\n",
            "Epoch 5651/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9038\n",
            "Epoch 5651: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2854 - accuracy: 0.9037 - val_loss: 0.5007 - val_accuracy: 0.8631\n",
            "Epoch 5652/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9039\n",
            "Epoch 5652: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2856 - accuracy: 0.9038 - val_loss: 0.4962 - val_accuracy: 0.8634\n",
            "Epoch 5653/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9048\n",
            "Epoch 5653: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2828 - accuracy: 0.9051 - val_loss: 0.4926 - val_accuracy: 0.8624\n",
            "Epoch 5654/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9020\n",
            "Epoch 5654: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2859 - accuracy: 0.9020 - val_loss: 0.4979 - val_accuracy: 0.8621\n",
            "Epoch 5655/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9022\n",
            "Epoch 5655: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9018 - val_loss: 0.4958 - val_accuracy: 0.8613\n",
            "Epoch 5656/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9014\n",
            "Epoch 5656: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2849 - accuracy: 0.9012 - val_loss: 0.5025 - val_accuracy: 0.8628\n",
            "Epoch 5657/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9025\n",
            "Epoch 5657: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9028 - val_loss: 0.4999 - val_accuracy: 0.8625\n",
            "Epoch 5658/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9015\n",
            "Epoch 5658: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2899 - accuracy: 0.9015 - val_loss: 0.4984 - val_accuracy: 0.8618\n",
            "Epoch 5659/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9039\n",
            "Epoch 5659: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2886 - accuracy: 0.9036 - val_loss: 0.4981 - val_accuracy: 0.8606\n",
            "Epoch 5660/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9032\n",
            "Epoch 5660: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2839 - accuracy: 0.9029 - val_loss: 0.4962 - val_accuracy: 0.8625\n",
            "Epoch 5661/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9010\n",
            "Epoch 5661: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9008 - val_loss: 0.5008 - val_accuracy: 0.8610\n",
            "Epoch 5662/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9028\n",
            "Epoch 5662: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2846 - accuracy: 0.9030 - val_loss: 0.4997 - val_accuracy: 0.8607\n",
            "Epoch 5663/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9025\n",
            "Epoch 5663: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2832 - accuracy: 0.9025 - val_loss: 0.5044 - val_accuracy: 0.8625\n",
            "Epoch 5664/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9029\n",
            "Epoch 5664: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2835 - accuracy: 0.9031 - val_loss: 0.5016 - val_accuracy: 0.8612\n",
            "Epoch 5665/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9011\n",
            "Epoch 5665: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2892 - accuracy: 0.9011 - val_loss: 0.4966 - val_accuracy: 0.8635\n",
            "Epoch 5666/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9035\n",
            "Epoch 5666: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2820 - accuracy: 0.9035 - val_loss: 0.4986 - val_accuracy: 0.8635\n",
            "Epoch 5667/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.9053\n",
            "Epoch 5667: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2813 - accuracy: 0.9054 - val_loss: 0.4977 - val_accuracy: 0.8624\n",
            "Epoch 5668/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.9007\n",
            "Epoch 5668: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2893 - accuracy: 0.9007 - val_loss: 0.4990 - val_accuracy: 0.8621\n",
            "Epoch 5669/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9028\n",
            "Epoch 5669: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2860 - accuracy: 0.9018 - val_loss: 0.4987 - val_accuracy: 0.8627\n",
            "Epoch 5670/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9028\n",
            "Epoch 5670: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2878 - accuracy: 0.9026 - val_loss: 0.4952 - val_accuracy: 0.8630\n",
            "Epoch 5671/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9046\n",
            "Epoch 5671: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2818 - accuracy: 0.9046 - val_loss: 0.4992 - val_accuracy: 0.8612\n",
            "Epoch 5672/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9038\n",
            "Epoch 5672: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2846 - accuracy: 0.9038 - val_loss: 0.4952 - val_accuracy: 0.8615\n",
            "Epoch 5673/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9022\n",
            "Epoch 5673: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9020 - val_loss: 0.4948 - val_accuracy: 0.8628\n",
            "Epoch 5674/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9043\n",
            "Epoch 5674: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2851 - accuracy: 0.9043 - val_loss: 0.4990 - val_accuracy: 0.8623\n",
            "Epoch 5675/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9032\n",
            "Epoch 5675: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2837 - accuracy: 0.9032 - val_loss: 0.4997 - val_accuracy: 0.8618\n",
            "Epoch 5676/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9029\n",
            "Epoch 5676: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2878 - accuracy: 0.9028 - val_loss: 0.4962 - val_accuracy: 0.8618\n",
            "Epoch 5677/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9042\n",
            "Epoch 5677: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2831 - accuracy: 0.9041 - val_loss: 0.4955 - val_accuracy: 0.8636\n",
            "Epoch 5678/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9012\n",
            "Epoch 5678: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2891 - accuracy: 0.9013 - val_loss: 0.5022 - val_accuracy: 0.8607\n",
            "Epoch 5679/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.9012\n",
            "Epoch 5679: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2910 - accuracy: 0.9013 - val_loss: 0.5017 - val_accuracy: 0.8642\n",
            "Epoch 5680/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9015\n",
            "Epoch 5680: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2906 - accuracy: 0.9015 - val_loss: 0.4939 - val_accuracy: 0.8631\n",
            "Epoch 5681/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9021\n",
            "Epoch 5681: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2841 - accuracy: 0.9025 - val_loss: 0.5008 - val_accuracy: 0.8622\n",
            "Epoch 5682/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9037\n",
            "Epoch 5682: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2851 - accuracy: 0.9041 - val_loss: 0.5005 - val_accuracy: 0.8625\n",
            "Epoch 5683/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9017\n",
            "Epoch 5683: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9018 - val_loss: 0.4921 - val_accuracy: 0.8623\n",
            "Epoch 5684/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9018\n",
            "Epoch 5684: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2844 - accuracy: 0.9018 - val_loss: 0.5015 - val_accuracy: 0.8624\n",
            "Epoch 5685/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9035\n",
            "Epoch 5685: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2860 - accuracy: 0.9035 - val_loss: 0.5029 - val_accuracy: 0.8621\n",
            "Epoch 5686/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9020\n",
            "Epoch 5686: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.9024 - val_loss: 0.4964 - val_accuracy: 0.8632\n",
            "Epoch 5687/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9024\n",
            "Epoch 5687: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2873 - accuracy: 0.9025 - val_loss: 0.4966 - val_accuracy: 0.8624\n",
            "Epoch 5688/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9005\n",
            "Epoch 5688: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2903 - accuracy: 0.9006 - val_loss: 0.4953 - val_accuracy: 0.8640\n",
            "Epoch 5689/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9022\n",
            "Epoch 5689: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2841 - accuracy: 0.9021 - val_loss: 0.5031 - val_accuracy: 0.8616\n",
            "Epoch 5690/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2832 - accuracy: 0.9038\n",
            "Epoch 5690: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2832 - accuracy: 0.9038 - val_loss: 0.4998 - val_accuracy: 0.8628\n",
            "Epoch 5691/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9012\n",
            "Epoch 5691: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9015 - val_loss: 0.4991 - val_accuracy: 0.8630\n",
            "Epoch 5692/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9021\n",
            "Epoch 5692: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2898 - accuracy: 0.9021 - val_loss: 0.4984 - val_accuracy: 0.8640\n",
            "Epoch 5693/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9027\n",
            "Epoch 5693: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2896 - accuracy: 0.9027 - val_loss: 0.4967 - val_accuracy: 0.8626\n",
            "Epoch 5694/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9024\n",
            "Epoch 5694: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2866 - accuracy: 0.9024 - val_loss: 0.4945 - val_accuracy: 0.8638\n",
            "Epoch 5695/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9018\n",
            "Epoch 5695: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2887 - accuracy: 0.9018 - val_loss: 0.4991 - val_accuracy: 0.8621\n",
            "Epoch 5696/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9026\n",
            "Epoch 5696: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2851 - accuracy: 0.9026 - val_loss: 0.5044 - val_accuracy: 0.8629\n",
            "Epoch 5697/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.9009\n",
            "Epoch 5697: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2877 - accuracy: 0.9009 - val_loss: 0.5015 - val_accuracy: 0.8618\n",
            "Epoch 5698/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9019\n",
            "Epoch 5698: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2861 - accuracy: 0.9019 - val_loss: 0.4982 - val_accuracy: 0.8604\n",
            "Epoch 5699/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9040\n",
            "Epoch 5699: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2862 - accuracy: 0.9040 - val_loss: 0.5010 - val_accuracy: 0.8609\n",
            "Epoch 5700/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9018\n",
            "Epoch 5700: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.9018 - val_loss: 0.4988 - val_accuracy: 0.8620\n",
            "Epoch 5701/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9031\n",
            "Epoch 5701: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2857 - accuracy: 0.9031 - val_loss: 0.4974 - val_accuracy: 0.8641\n",
            "Epoch 5702/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9029\n",
            "Epoch 5702: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2868 - accuracy: 0.9028 - val_loss: 0.4986 - val_accuracy: 0.8625\n",
            "Epoch 5703/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9031\n",
            "Epoch 5703: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2854 - accuracy: 0.9031 - val_loss: 0.4992 - val_accuracy: 0.8618\n",
            "Epoch 5704/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9030\n",
            "Epoch 5704: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2840 - accuracy: 0.9028 - val_loss: 0.5007 - val_accuracy: 0.8613\n",
            "Epoch 5705/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9021\n",
            "Epoch 5705: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2848 - accuracy: 0.9021 - val_loss: 0.5018 - val_accuracy: 0.8629\n",
            "Epoch 5706/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9033\n",
            "Epoch 5706: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2830 - accuracy: 0.9031 - val_loss: 0.5062 - val_accuracy: 0.8609\n",
            "Epoch 5707/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9032\n",
            "Epoch 5707: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2860 - accuracy: 0.9030 - val_loss: 0.4957 - val_accuracy: 0.8643\n",
            "Epoch 5708/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9036\n",
            "Epoch 5708: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2827 - accuracy: 0.9036 - val_loss: 0.4964 - val_accuracy: 0.8616\n",
            "Epoch 5709/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9017\n",
            "Epoch 5709: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2886 - accuracy: 0.9015 - val_loss: 0.5011 - val_accuracy: 0.8626\n",
            "Epoch 5710/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9020\n",
            "Epoch 5710: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2853 - accuracy: 0.9020 - val_loss: 0.4934 - val_accuracy: 0.8607\n",
            "Epoch 5711/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2835 - accuracy: 0.9035\n",
            "Epoch 5711: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2835 - accuracy: 0.9035 - val_loss: 0.4974 - val_accuracy: 0.8623\n",
            "Epoch 5712/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9019\n",
            "Epoch 5712: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2871 - accuracy: 0.9020 - val_loss: 0.4986 - val_accuracy: 0.8596\n",
            "Epoch 5713/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9026\n",
            "Epoch 5713: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2853 - accuracy: 0.9026 - val_loss: 0.4993 - val_accuracy: 0.8611\n",
            "Epoch 5714/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9042\n",
            "Epoch 5714: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2822 - accuracy: 0.9043 - val_loss: 0.4978 - val_accuracy: 0.8601\n",
            "Epoch 5715/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.9002\n",
            "Epoch 5715: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2914 - accuracy: 0.9002 - val_loss: 0.4943 - val_accuracy: 0.8620\n",
            "Epoch 5716/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9026\n",
            "Epoch 5716: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2893 - accuracy: 0.9027 - val_loss: 0.4942 - val_accuracy: 0.8610\n",
            "Epoch 5717/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9025\n",
            "Epoch 5717: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2878 - accuracy: 0.9021 - val_loss: 0.4957 - val_accuracy: 0.8622\n",
            "Epoch 5718/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9028\n",
            "Epoch 5718: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9025 - val_loss: 0.5009 - val_accuracy: 0.8607\n",
            "Epoch 5719/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9032\n",
            "Epoch 5719: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2825 - accuracy: 0.9032 - val_loss: 0.4987 - val_accuracy: 0.8618\n",
            "Epoch 5720/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8998\n",
            "Epoch 5720: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2914 - accuracy: 0.8997 - val_loss: 0.4951 - val_accuracy: 0.8625\n",
            "Epoch 5721/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9023\n",
            "Epoch 5721: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2870 - accuracy: 0.9022 - val_loss: 0.4995 - val_accuracy: 0.8607\n",
            "Epoch 5722/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9025\n",
            "Epoch 5722: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2852 - accuracy: 0.9025 - val_loss: 0.4991 - val_accuracy: 0.8612\n",
            "Epoch 5723/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9031\n",
            "Epoch 5723: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9029 - val_loss: 0.4985 - val_accuracy: 0.8626\n",
            "Epoch 5724/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9014\n",
            "Epoch 5724: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2893 - accuracy: 0.9013 - val_loss: 0.4913 - val_accuracy: 0.8618\n",
            "Epoch 5725/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9024\n",
            "Epoch 5725: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2857 - accuracy: 0.9024 - val_loss: 0.4987 - val_accuracy: 0.8618\n",
            "Epoch 5726/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9027\n",
            "Epoch 5726: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2871 - accuracy: 0.9027 - val_loss: 0.4956 - val_accuracy: 0.8614\n",
            "Epoch 5727/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9043\n",
            "Epoch 5727: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9041 - val_loss: 0.5062 - val_accuracy: 0.8600\n",
            "Epoch 5728/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9032\n",
            "Epoch 5728: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2868 - accuracy: 0.9033 - val_loss: 0.5065 - val_accuracy: 0.8592\n",
            "Epoch 5729/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9008\n",
            "Epoch 5729: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2888 - accuracy: 0.9009 - val_loss: 0.4910 - val_accuracy: 0.8618\n",
            "Epoch 5730/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9036\n",
            "Epoch 5730: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2830 - accuracy: 0.9035 - val_loss: 0.5015 - val_accuracy: 0.8613\n",
            "Epoch 5731/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9006\n",
            "Epoch 5731: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2893 - accuracy: 0.9004 - val_loss: 0.4996 - val_accuracy: 0.8622\n",
            "Epoch 5732/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9026\n",
            "Epoch 5732: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9027 - val_loss: 0.5055 - val_accuracy: 0.8598\n",
            "Epoch 5733/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9021\n",
            "Epoch 5733: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2881 - accuracy: 0.9021 - val_loss: 0.4949 - val_accuracy: 0.8615\n",
            "Epoch 5734/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9017\n",
            "Epoch 5734: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2876 - accuracy: 0.9017 - val_loss: 0.4950 - val_accuracy: 0.8605\n",
            "Epoch 5735/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2875 - accuracy: 0.9022\n",
            "Epoch 5735: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2875 - accuracy: 0.9022 - val_loss: 0.4987 - val_accuracy: 0.8620\n",
            "Epoch 5736/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2857 - accuracy: 0.9027\n",
            "Epoch 5736: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2858 - accuracy: 0.9026 - val_loss: 0.4952 - val_accuracy: 0.8612\n",
            "Epoch 5737/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9040\n",
            "Epoch 5737: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2875 - accuracy: 0.9038 - val_loss: 0.5006 - val_accuracy: 0.8629\n",
            "Epoch 5738/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9033\n",
            "Epoch 5738: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2871 - accuracy: 0.9033 - val_loss: 0.4998 - val_accuracy: 0.8610\n",
            "Epoch 5739/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9029\n",
            "Epoch 5739: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2854 - accuracy: 0.9027 - val_loss: 0.4946 - val_accuracy: 0.8631\n",
            "Epoch 5740/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.9017\n",
            "Epoch 5740: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2843 - accuracy: 0.9017 - val_loss: 0.5003 - val_accuracy: 0.8630\n",
            "Epoch 5741/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9023\n",
            "Epoch 5741: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2867 - accuracy: 0.9023 - val_loss: 0.4915 - val_accuracy: 0.8626\n",
            "Epoch 5742/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.9003\n",
            "Epoch 5742: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2900 - accuracy: 0.9004 - val_loss: 0.4964 - val_accuracy: 0.8612\n",
            "Epoch 5743/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9017\n",
            "Epoch 5743: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2852 - accuracy: 0.9011 - val_loss: 0.4943 - val_accuracy: 0.8635\n",
            "Epoch 5744/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9021\n",
            "Epoch 5744: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2856 - accuracy: 0.9022 - val_loss: 0.5024 - val_accuracy: 0.8607\n",
            "Epoch 5745/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9024\n",
            "Epoch 5745: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2857 - accuracy: 0.9026 - val_loss: 0.5012 - val_accuracy: 0.8622\n",
            "Epoch 5746/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9044\n",
            "Epoch 5746: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2848 - accuracy: 0.9044 - val_loss: 0.4964 - val_accuracy: 0.8610\n",
            "Epoch 5747/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9026\n",
            "Epoch 5747: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2857 - accuracy: 0.9025 - val_loss: 0.5018 - val_accuracy: 0.8614\n",
            "Epoch 5748/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.9034\n",
            "Epoch 5748: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2832 - accuracy: 0.9028 - val_loss: 0.5050 - val_accuracy: 0.8615\n",
            "Epoch 5749/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9029\n",
            "Epoch 5749: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2849 - accuracy: 0.9031 - val_loss: 0.4979 - val_accuracy: 0.8631\n",
            "Epoch 5750/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9024\n",
            "Epoch 5750: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2839 - accuracy: 0.9023 - val_loss: 0.4993 - val_accuracy: 0.8629\n",
            "Epoch 5751/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9014\n",
            "Epoch 5751: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.9013 - val_loss: 0.4936 - val_accuracy: 0.8629\n",
            "Epoch 5752/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9043\n",
            "Epoch 5752: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2805 - accuracy: 0.9043 - val_loss: 0.5035 - val_accuracy: 0.8628\n",
            "Epoch 5753/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9021\n",
            "Epoch 5753: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2840 - accuracy: 0.9019 - val_loss: 0.5041 - val_accuracy: 0.8624\n",
            "Epoch 5754/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9042\n",
            "Epoch 5754: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2845 - accuracy: 0.9042 - val_loss: 0.4977 - val_accuracy: 0.8626\n",
            "Epoch 5755/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9028\n",
            "Epoch 5755: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2894 - accuracy: 0.9028 - val_loss: 0.4916 - val_accuracy: 0.8626\n",
            "Epoch 5756/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9026\n",
            "Epoch 5756: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2856 - accuracy: 0.9028 - val_loss: 0.4956 - val_accuracy: 0.8619\n",
            "Epoch 5757/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9015\n",
            "Epoch 5757: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2877 - accuracy: 0.9011 - val_loss: 0.4984 - val_accuracy: 0.8640\n",
            "Epoch 5758/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9025\n",
            "Epoch 5758: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2886 - accuracy: 0.9025 - val_loss: 0.4947 - val_accuracy: 0.8634\n",
            "Epoch 5759/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.9040\n",
            "Epoch 5759: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2809 - accuracy: 0.9040 - val_loss: 0.4973 - val_accuracy: 0.8632\n",
            "Epoch 5760/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.9000\n",
            "Epoch 5760: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2888 - accuracy: 0.9002 - val_loss: 0.4993 - val_accuracy: 0.8607\n",
            "Epoch 5761/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9014\n",
            "Epoch 5761: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2912 - accuracy: 0.9012 - val_loss: 0.5005 - val_accuracy: 0.8623\n",
            "Epoch 5762/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9016\n",
            "Epoch 5762: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2861 - accuracy: 0.9015 - val_loss: 0.4980 - val_accuracy: 0.8623\n",
            "Epoch 5763/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9011\n",
            "Epoch 5763: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2883 - accuracy: 0.9005 - val_loss: 0.4986 - val_accuracy: 0.8607\n",
            "Epoch 5764/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.9038\n",
            "Epoch 5764: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2836 - accuracy: 0.9033 - val_loss: 0.4997 - val_accuracy: 0.8633\n",
            "Epoch 5765/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.9020\n",
            "Epoch 5765: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2893 - accuracy: 0.9023 - val_loss: 0.4969 - val_accuracy: 0.8617\n",
            "Epoch 5766/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.9030\n",
            "Epoch 5766: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2830 - accuracy: 0.9028 - val_loss: 0.4988 - val_accuracy: 0.8629\n",
            "Epoch 5767/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9026\n",
            "Epoch 5767: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2846 - accuracy: 0.9026 - val_loss: 0.4984 - val_accuracy: 0.8627\n",
            "Epoch 5768/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9045\n",
            "Epoch 5768: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2836 - accuracy: 0.9045 - val_loss: 0.5044 - val_accuracy: 0.8612\n",
            "Epoch 5769/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9021\n",
            "Epoch 5769: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2849 - accuracy: 0.9019 - val_loss: 0.4998 - val_accuracy: 0.8631\n",
            "Epoch 5770/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9037\n",
            "Epoch 5770: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2853 - accuracy: 0.9034 - val_loss: 0.5050 - val_accuracy: 0.8634\n",
            "Epoch 5771/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.9047\n",
            "Epoch 5771: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2833 - accuracy: 0.9041 - val_loss: 0.5068 - val_accuracy: 0.8605\n",
            "Epoch 5772/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9037\n",
            "Epoch 5772: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2859 - accuracy: 0.9035 - val_loss: 0.5065 - val_accuracy: 0.8621\n",
            "Epoch 5773/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9011\n",
            "Epoch 5773: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2871 - accuracy: 0.9008 - val_loss: 0.4988 - val_accuracy: 0.8625\n",
            "Epoch 5774/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9033\n",
            "Epoch 5774: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2839 - accuracy: 0.9031 - val_loss: 0.4986 - val_accuracy: 0.8633\n",
            "Epoch 5775/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9032\n",
            "Epoch 5775: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2818 - accuracy: 0.9032 - val_loss: 0.5028 - val_accuracy: 0.8602\n",
            "Epoch 5776/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9026\n",
            "Epoch 5776: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2842 - accuracy: 0.9024 - val_loss: 0.5014 - val_accuracy: 0.8612\n",
            "Epoch 5777/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9010\n",
            "Epoch 5777: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.9009 - val_loss: 0.5002 - val_accuracy: 0.8609\n",
            "Epoch 5778/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9019\n",
            "Epoch 5778: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2870 - accuracy: 0.9020 - val_loss: 0.4965 - val_accuracy: 0.8617\n",
            "Epoch 5779/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.9028\n",
            "Epoch 5779: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2820 - accuracy: 0.9026 - val_loss: 0.5043 - val_accuracy: 0.8609\n",
            "Epoch 5780/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9028\n",
            "Epoch 5780: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9024 - val_loss: 0.5034 - val_accuracy: 0.8624\n",
            "Epoch 5781/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9031\n",
            "Epoch 5781: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2849 - accuracy: 0.9031 - val_loss: 0.4985 - val_accuracy: 0.8626\n",
            "Epoch 5782/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.9033\n",
            "Epoch 5782: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2824 - accuracy: 0.9033 - val_loss: 0.5063 - val_accuracy: 0.8627\n",
            "Epoch 5783/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9034\n",
            "Epoch 5783: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2846 - accuracy: 0.9033 - val_loss: 0.5032 - val_accuracy: 0.8638\n",
            "Epoch 5784/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9035\n",
            "Epoch 5784: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2838 - accuracy: 0.9036 - val_loss: 0.5053 - val_accuracy: 0.8628\n",
            "Epoch 5785/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.9042\n",
            "Epoch 5785: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2810 - accuracy: 0.9042 - val_loss: 0.5006 - val_accuracy: 0.8632\n",
            "Epoch 5786/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.9007\n",
            "Epoch 5786: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2931 - accuracy: 0.9007 - val_loss: 0.5013 - val_accuracy: 0.8610\n",
            "Epoch 5787/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9024\n",
            "Epoch 5787: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2877 - accuracy: 0.9024 - val_loss: 0.4957 - val_accuracy: 0.8632\n",
            "Epoch 5788/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9012\n",
            "Epoch 5788: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2923 - accuracy: 0.9012 - val_loss: 0.4980 - val_accuracy: 0.8629\n",
            "Epoch 5789/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9026\n",
            "Epoch 5789: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2868 - accuracy: 0.9025 - val_loss: 0.5035 - val_accuracy: 0.8650\n",
            "Epoch 5790/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9037\n",
            "Epoch 5790: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 22ms/step - loss: 0.2820 - accuracy: 0.9038 - val_loss: 0.5049 - val_accuracy: 0.8624\n",
            "Epoch 5791/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9027\n",
            "Epoch 5791: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2863 - accuracy: 0.9027 - val_loss: 0.5003 - val_accuracy: 0.8616\n",
            "Epoch 5792/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9023\n",
            "Epoch 5792: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2874 - accuracy: 0.9020 - val_loss: 0.5005 - val_accuracy: 0.8612\n",
            "Epoch 5793/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9024\n",
            "Epoch 5793: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2882 - accuracy: 0.9024 - val_loss: 0.4966 - val_accuracy: 0.8630\n",
            "Epoch 5794/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9017\n",
            "Epoch 5794: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2872 - accuracy: 0.9018 - val_loss: 0.5035 - val_accuracy: 0.8613\n",
            "Epoch 5795/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9001\n",
            "Epoch 5795: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9001 - val_loss: 0.4969 - val_accuracy: 0.8619\n",
            "Epoch 5796/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9028\n",
            "Epoch 5796: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2830 - accuracy: 0.9028 - val_loss: 0.5006 - val_accuracy: 0.8603\n",
            "Epoch 5797/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9037\n",
            "Epoch 5797: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2856 - accuracy: 0.9038 - val_loss: 0.4969 - val_accuracy: 0.8635\n",
            "Epoch 5798/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9018\n",
            "Epoch 5798: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2848 - accuracy: 0.9018 - val_loss: 0.4927 - val_accuracy: 0.8611\n",
            "Epoch 5799/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.9025\n",
            "Epoch 5799: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2843 - accuracy: 0.9025 - val_loss: 0.4978 - val_accuracy: 0.8612\n",
            "Epoch 5800/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9020\n",
            "Epoch 5800: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2822 - accuracy: 0.9020 - val_loss: 0.4986 - val_accuracy: 0.8607\n",
            "Epoch 5801/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9005\n",
            "Epoch 5801: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2874 - accuracy: 0.9007 - val_loss: 0.5020 - val_accuracy: 0.8613\n",
            "Epoch 5802/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9010\n",
            "Epoch 5802: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2898 - accuracy: 0.9009 - val_loss: 0.5001 - val_accuracy: 0.8598\n",
            "Epoch 5803/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.9028\n",
            "Epoch 5803: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2835 - accuracy: 0.9028 - val_loss: 0.5028 - val_accuracy: 0.8618\n",
            "Epoch 5804/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9007\n",
            "Epoch 5804: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2906 - accuracy: 0.9007 - val_loss: 0.5053 - val_accuracy: 0.8597\n",
            "Epoch 5805/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9017\n",
            "Epoch 5805: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2881 - accuracy: 0.9017 - val_loss: 0.5003 - val_accuracy: 0.8580\n",
            "Epoch 5806/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9018\n",
            "Epoch 5806: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2861 - accuracy: 0.9018 - val_loss: 0.5002 - val_accuracy: 0.8592\n",
            "Epoch 5807/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9029\n",
            "Epoch 5807: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2828 - accuracy: 0.9033 - val_loss: 0.4989 - val_accuracy: 0.8613\n",
            "Epoch 5808/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9033\n",
            "Epoch 5808: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2850 - accuracy: 0.9033 - val_loss: 0.5010 - val_accuracy: 0.8604\n",
            "Epoch 5809/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9017\n",
            "Epoch 5809: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2911 - accuracy: 0.9019 - val_loss: 0.4958 - val_accuracy: 0.8619\n",
            "Epoch 5810/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.9038\n",
            "Epoch 5810: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2838 - accuracy: 0.9040 - val_loss: 0.4990 - val_accuracy: 0.8621\n",
            "Epoch 5811/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.9004\n",
            "Epoch 5811: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2863 - accuracy: 0.9004 - val_loss: 0.5013 - val_accuracy: 0.8618\n",
            "Epoch 5812/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9027\n",
            "Epoch 5812: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2849 - accuracy: 0.9033 - val_loss: 0.5027 - val_accuracy: 0.8605\n",
            "Epoch 5813/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9009\n",
            "Epoch 5813: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2894 - accuracy: 0.9007 - val_loss: 0.4996 - val_accuracy: 0.8619\n",
            "Epoch 5814/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9026\n",
            "Epoch 5814: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2848 - accuracy: 0.9026 - val_loss: 0.5039 - val_accuracy: 0.8604\n",
            "Epoch 5815/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.9028\n",
            "Epoch 5815: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2858 - accuracy: 0.9026 - val_loss: 0.4994 - val_accuracy: 0.8620\n",
            "Epoch 5816/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9009\n",
            "Epoch 5816: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2865 - accuracy: 0.9009 - val_loss: 0.4989 - val_accuracy: 0.8623\n",
            "Epoch 5817/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9027\n",
            "Epoch 5817: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2862 - accuracy: 0.9027 - val_loss: 0.5002 - val_accuracy: 0.8627\n",
            "Epoch 5818/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9023\n",
            "Epoch 5818: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2862 - accuracy: 0.9022 - val_loss: 0.4982 - val_accuracy: 0.8618\n",
            "Epoch 5819/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9030\n",
            "Epoch 5819: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2783 - accuracy: 0.9030 - val_loss: 0.5046 - val_accuracy: 0.8621\n",
            "Epoch 5820/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.9037\n",
            "Epoch 5820: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2827 - accuracy: 0.9037 - val_loss: 0.5055 - val_accuracy: 0.8615\n",
            "Epoch 5821/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.9036\n",
            "Epoch 5821: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2841 - accuracy: 0.9031 - val_loss: 0.5045 - val_accuracy: 0.8614\n",
            "Epoch 5822/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9019\n",
            "Epoch 5822: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2841 - accuracy: 0.9019 - val_loss: 0.5027 - val_accuracy: 0.8622\n",
            "Epoch 5823/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9014\n",
            "Epoch 5823: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9013 - val_loss: 0.5013 - val_accuracy: 0.8619\n",
            "Epoch 5824/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.9018\n",
            "Epoch 5824: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2862 - accuracy: 0.9019 - val_loss: 0.5003 - val_accuracy: 0.8616\n",
            "Epoch 5825/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.9038\n",
            "Epoch 5825: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2836 - accuracy: 0.9036 - val_loss: 0.5016 - val_accuracy: 0.8613\n",
            "Epoch 5826/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.9028\n",
            "Epoch 5826: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2834 - accuracy: 0.9027 - val_loss: 0.4982 - val_accuracy: 0.8630\n",
            "Epoch 5827/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9007\n",
            "Epoch 5827: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2879 - accuracy: 0.9007 - val_loss: 0.4990 - val_accuracy: 0.8624\n",
            "Epoch 5828/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9030\n",
            "Epoch 5828: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2856 - accuracy: 0.9026 - val_loss: 0.5004 - val_accuracy: 0.8624\n",
            "Epoch 5829/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9023\n",
            "Epoch 5829: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2870 - accuracy: 0.9026 - val_loss: 0.4958 - val_accuracy: 0.8635\n",
            "Epoch 5830/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9025\n",
            "Epoch 5830: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2858 - accuracy: 0.9026 - val_loss: 0.4980 - val_accuracy: 0.8624\n",
            "Epoch 5831/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.9056\n",
            "Epoch 5831: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2802 - accuracy: 0.9054 - val_loss: 0.5048 - val_accuracy: 0.8617\n",
            "Epoch 5832/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.9028\n",
            "Epoch 5832: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2856 - accuracy: 0.9028 - val_loss: 0.5010 - val_accuracy: 0.8628\n",
            "Epoch 5833/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9030\n",
            "Epoch 5833: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2841 - accuracy: 0.9030 - val_loss: 0.4982 - val_accuracy: 0.8629\n",
            "Epoch 5834/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9002\n",
            "Epoch 5834: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2849 - accuracy: 0.9004 - val_loss: 0.5020 - val_accuracy: 0.8617\n",
            "Epoch 5835/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9047\n",
            "Epoch 5835: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2832 - accuracy: 0.9049 - val_loss: 0.5039 - val_accuracy: 0.8613\n",
            "Epoch 5836/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9026\n",
            "Epoch 5836: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2885 - accuracy: 0.9026 - val_loss: 0.5009 - val_accuracy: 0.8616\n",
            "Epoch 5837/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.9031\n",
            "Epoch 5837: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2846 - accuracy: 0.9032 - val_loss: 0.5049 - val_accuracy: 0.8618\n",
            "Epoch 5838/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9011\n",
            "Epoch 5838: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2910 - accuracy: 0.9010 - val_loss: 0.4968 - val_accuracy: 0.8612\n",
            "Epoch 5839/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9012\n",
            "Epoch 5839: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2850 - accuracy: 0.9012 - val_loss: 0.4979 - val_accuracy: 0.8630\n",
            "Epoch 5840/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9007\n",
            "Epoch 5840: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2903 - accuracy: 0.9005 - val_loss: 0.5003 - val_accuracy: 0.8608\n",
            "Epoch 5841/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9002\n",
            "Epoch 5841: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9002 - val_loss: 0.5030 - val_accuracy: 0.8610\n",
            "Epoch 5842/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9038\n",
            "Epoch 5842: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2836 - accuracy: 0.9036 - val_loss: 0.4964 - val_accuracy: 0.8618\n",
            "Epoch 5843/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9015\n",
            "Epoch 5843: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2892 - accuracy: 0.9011 - val_loss: 0.5022 - val_accuracy: 0.8619\n",
            "Epoch 5844/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.9010\n",
            "Epoch 5844: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2916 - accuracy: 0.9008 - val_loss: 0.4937 - val_accuracy: 0.8613\n",
            "Epoch 5845/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9021\n",
            "Epoch 5845: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2830 - accuracy: 0.9025 - val_loss: 0.5086 - val_accuracy: 0.8608\n",
            "Epoch 5846/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9039\n",
            "Epoch 5846: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2830 - accuracy: 0.9039 - val_loss: 0.5002 - val_accuracy: 0.8628\n",
            "Epoch 5847/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9024\n",
            "Epoch 5847: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2840 - accuracy: 0.9023 - val_loss: 0.4975 - val_accuracy: 0.8620\n",
            "Epoch 5848/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9022\n",
            "Epoch 5848: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9018 - val_loss: 0.5090 - val_accuracy: 0.8614\n",
            "Epoch 5849/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.9043\n",
            "Epoch 5849: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2827 - accuracy: 0.9043 - val_loss: 0.4998 - val_accuracy: 0.8629\n",
            "Epoch 5850/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9019\n",
            "Epoch 5850: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2860 - accuracy: 0.9016 - val_loss: 0.5032 - val_accuracy: 0.8616\n",
            "Epoch 5851/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.9015\n",
            "Epoch 5851: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2902 - accuracy: 0.9016 - val_loss: 0.4922 - val_accuracy: 0.8600\n",
            "Epoch 5852/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9023\n",
            "Epoch 5852: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2878 - accuracy: 0.9022 - val_loss: 0.4949 - val_accuracy: 0.8624\n",
            "Epoch 5853/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9012\n",
            "Epoch 5853: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2876 - accuracy: 0.9011 - val_loss: 0.4991 - val_accuracy: 0.8625\n",
            "Epoch 5854/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9032\n",
            "Epoch 5854: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2861 - accuracy: 0.9032 - val_loss: 0.5062 - val_accuracy: 0.8607\n",
            "Epoch 5855/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9022\n",
            "Epoch 5855: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2915 - accuracy: 0.9020 - val_loss: 0.4961 - val_accuracy: 0.8635\n",
            "Epoch 5856/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9021\n",
            "Epoch 5856: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2839 - accuracy: 0.9021 - val_loss: 0.4955 - val_accuracy: 0.8615\n",
            "Epoch 5857/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9014\n",
            "Epoch 5857: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 15ms/step - loss: 0.2879 - accuracy: 0.9015 - val_loss: 0.4952 - val_accuracy: 0.8612\n",
            "Epoch 5858/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.9025\n",
            "Epoch 5858: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2831 - accuracy: 0.9026 - val_loss: 0.5032 - val_accuracy: 0.8629\n",
            "Epoch 5859/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9027\n",
            "Epoch 5859: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2867 - accuracy: 0.9027 - val_loss: 0.4995 - val_accuracy: 0.8617\n",
            "Epoch 5860/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9008\n",
            "Epoch 5860: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2885 - accuracy: 0.9008 - val_loss: 0.4998 - val_accuracy: 0.8619\n",
            "Epoch 5861/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9016\n",
            "Epoch 5861: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2844 - accuracy: 0.9017 - val_loss: 0.5056 - val_accuracy: 0.8621\n",
            "Epoch 5862/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9036\n",
            "Epoch 5862: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2860 - accuracy: 0.9035 - val_loss: 0.5073 - val_accuracy: 0.8592\n",
            "Epoch 5863/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2888 - accuracy: 0.9016\n",
            "Epoch 5863: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2888 - accuracy: 0.9016 - val_loss: 0.4972 - val_accuracy: 0.8617\n",
            "Epoch 5864/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9022\n",
            "Epoch 5864: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2870 - accuracy: 0.9020 - val_loss: 0.4977 - val_accuracy: 0.8616\n",
            "Epoch 5865/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.9043\n",
            "Epoch 5865: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2804 - accuracy: 0.9042 - val_loss: 0.5001 - val_accuracy: 0.8614\n",
            "Epoch 5866/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2806 - accuracy: 0.9041\n",
            "Epoch 5866: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2808 - accuracy: 0.9039 - val_loss: 0.4990 - val_accuracy: 0.8632\n",
            "Epoch 5867/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9039\n",
            "Epoch 5867: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2834 - accuracy: 0.9039 - val_loss: 0.5028 - val_accuracy: 0.8619\n",
            "Epoch 5868/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.9022\n",
            "Epoch 5868: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2884 - accuracy: 0.9022 - val_loss: 0.4944 - val_accuracy: 0.8618\n",
            "Epoch 5869/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.9045\n",
            "Epoch 5869: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2843 - accuracy: 0.9042 - val_loss: 0.4975 - val_accuracy: 0.8627\n",
            "Epoch 5870/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9028\n",
            "Epoch 5870: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2865 - accuracy: 0.9028 - val_loss: 0.5009 - val_accuracy: 0.8633\n",
            "Epoch 5871/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9056\n",
            "Epoch 5871: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2834 - accuracy: 0.9056 - val_loss: 0.4977 - val_accuracy: 0.8619\n",
            "Epoch 5872/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9026\n",
            "Epoch 5872: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2855 - accuracy: 0.9029 - val_loss: 0.5010 - val_accuracy: 0.8619\n",
            "Epoch 5873/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.9025\n",
            "Epoch 5873: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2863 - accuracy: 0.9024 - val_loss: 0.4990 - val_accuracy: 0.8626\n",
            "Epoch 5874/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.9033\n",
            "Epoch 5874: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2837 - accuracy: 0.9033 - val_loss: 0.4983 - val_accuracy: 0.8630\n",
            "Epoch 5875/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9028\n",
            "Epoch 5875: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2847 - accuracy: 0.9029 - val_loss: 0.4957 - val_accuracy: 0.8641\n",
            "Epoch 5876/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9036\n",
            "Epoch 5876: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2839 - accuracy: 0.9035 - val_loss: 0.4951 - val_accuracy: 0.8624\n",
            "Epoch 5877/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.9020\n",
            "Epoch 5877: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2858 - accuracy: 0.9019 - val_loss: 0.5003 - val_accuracy: 0.8616\n",
            "Epoch 5878/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9047\n",
            "Epoch 5878: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2823 - accuracy: 0.9046 - val_loss: 0.4987 - val_accuracy: 0.8625\n",
            "Epoch 5879/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.9005\n",
            "Epoch 5879: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2921 - accuracy: 0.9005 - val_loss: 0.4981 - val_accuracy: 0.8627\n",
            "Epoch 5880/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9018\n",
            "Epoch 5880: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2865 - accuracy: 0.9019 - val_loss: 0.5052 - val_accuracy: 0.8622\n",
            "Epoch 5881/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9038\n",
            "Epoch 5881: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2851 - accuracy: 0.9030 - val_loss: 0.5028 - val_accuracy: 0.8611\n",
            "Epoch 5882/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9012\n",
            "Epoch 5882: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2891 - accuracy: 0.9014 - val_loss: 0.4938 - val_accuracy: 0.8629\n",
            "Epoch 5883/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.9030\n",
            "Epoch 5883: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2835 - accuracy: 0.9028 - val_loss: 0.5013 - val_accuracy: 0.8618\n",
            "Epoch 5884/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9034\n",
            "Epoch 5884: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2858 - accuracy: 0.9033 - val_loss: 0.5032 - val_accuracy: 0.8631\n",
            "Epoch 5885/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.9002\n",
            "Epoch 5885: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2871 - accuracy: 0.9006 - val_loss: 0.5035 - val_accuracy: 0.8625\n",
            "Epoch 5886/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9013\n",
            "Epoch 5886: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.9013 - val_loss: 0.5023 - val_accuracy: 0.8632\n",
            "Epoch 5887/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9006\n",
            "Epoch 5887: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2861 - accuracy: 0.9009 - val_loss: 0.4957 - val_accuracy: 0.8632\n",
            "Epoch 5888/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9027\n",
            "Epoch 5888: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2819 - accuracy: 0.9027 - val_loss: 0.4981 - val_accuracy: 0.8635\n",
            "Epoch 5889/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9038\n",
            "Epoch 5889: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2821 - accuracy: 0.9037 - val_loss: 0.5042 - val_accuracy: 0.8630\n",
            "Epoch 5890/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.9042\n",
            "Epoch 5890: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2812 - accuracy: 0.9042 - val_loss: 0.5014 - val_accuracy: 0.8629\n",
            "Epoch 5891/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9004\n",
            "Epoch 5891: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2869 - accuracy: 0.9005 - val_loss: 0.4988 - val_accuracy: 0.8629\n",
            "Epoch 5892/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9013\n",
            "Epoch 5892: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2868 - accuracy: 0.9012 - val_loss: 0.4971 - val_accuracy: 0.8620\n",
            "Epoch 5893/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.9024\n",
            "Epoch 5893: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2849 - accuracy: 0.9024 - val_loss: 0.4943 - val_accuracy: 0.8629\n",
            "Epoch 5894/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9016\n",
            "Epoch 5894: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2872 - accuracy: 0.9014 - val_loss: 0.4920 - val_accuracy: 0.8620\n",
            "Epoch 5895/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9037\n",
            "Epoch 5895: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2854 - accuracy: 0.9037 - val_loss: 0.4973 - val_accuracy: 0.8624\n",
            "Epoch 5896/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9031\n",
            "Epoch 5896: val_accuracy did not improve from 0.86516\n",
            "92/92 [==============================] - 2s 20ms/step - loss: 0.2860 - accuracy: 0.9031 - val_loss: 0.5005 - val_accuracy: 0.8624\n",
            "Epoch 5897/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9023\n",
            "Epoch 5897: val_accuracy improved from 0.86516 to 0.86533, saving model to best_weights.h5\n",
            "92/92 [==============================] - 2s 19ms/step - loss: 0.2855 - accuracy: 0.9023 - val_loss: 0.4983 - val_accuracy: 0.8653\n",
            "Epoch 5898/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9032\n",
            "Epoch 5898: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2824 - accuracy: 0.9033 - val_loss: 0.5063 - val_accuracy: 0.8618\n",
            "Epoch 5899/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9016\n",
            "Epoch 5899: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2860 - accuracy: 0.9015 - val_loss: 0.5026 - val_accuracy: 0.8622\n",
            "Epoch 5900/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.9042\n",
            "Epoch 5900: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2813 - accuracy: 0.9042 - val_loss: 0.5056 - val_accuracy: 0.8627\n",
            "Epoch 5901/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9042\n",
            "Epoch 5901: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 16ms/step - loss: 0.2845 - accuracy: 0.9042 - val_loss: 0.4950 - val_accuracy: 0.8643\n",
            "Epoch 5902/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.9038\n",
            "Epoch 5902: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2820 - accuracy: 0.9039 - val_loss: 0.5000 - val_accuracy: 0.8652\n",
            "Epoch 5903/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9035\n",
            "Epoch 5903: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2870 - accuracy: 0.9031 - val_loss: 0.5018 - val_accuracy: 0.8640\n",
            "Epoch 5904/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9015\n",
            "Epoch 5904: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2877 - accuracy: 0.9015 - val_loss: 0.4982 - val_accuracy: 0.8647\n",
            "Epoch 5905/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2804 - accuracy: 0.9053\n",
            "Epoch 5905: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 21ms/step - loss: 0.2804 - accuracy: 0.9052 - val_loss: 0.5093 - val_accuracy: 0.8627\n",
            "Epoch 5906/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.9019\n",
            "Epoch 5906: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2902 - accuracy: 0.9020 - val_loss: 0.5006 - val_accuracy: 0.8646\n",
            "Epoch 5907/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.9034\n",
            "Epoch 5907: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.9029 - val_loss: 0.5003 - val_accuracy: 0.8637\n",
            "Epoch 5908/15000\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9032\n",
            "Epoch 5908: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2834 - accuracy: 0.9032 - val_loss: 0.5019 - val_accuracy: 0.8636\n",
            "Epoch 5909/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2890 - accuracy: 0.9000\n",
            "Epoch 5909: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 17ms/step - loss: 0.2880 - accuracy: 0.9005 - val_loss: 0.4951 - val_accuracy: 0.8631\n",
            "Epoch 5910/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9022\n",
            "Epoch 5910: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2837 - accuracy: 0.9021 - val_loss: 0.4966 - val_accuracy: 0.8628\n",
            "Epoch 5911/15000\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.9057\n",
            "Epoch 5911: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 1s 16ms/step - loss: 0.2842 - accuracy: 0.9057 - val_loss: 0.4984 - val_accuracy: 0.8606\n",
            "Epoch 5912/15000\n",
            "91/92 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9035\n",
            "Epoch 5912: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2839 - accuracy: 0.9033 - val_loss: 0.4999 - val_accuracy: 0.8636\n",
            "Epoch 5913/15000\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.9018\n",
            "Epoch 5913: val_accuracy did not improve from 0.86533\n",
            "92/92 [==============================] - 2s 18ms/step - loss: 0.2820 - accuracy: 0.9020 - val_loss: 0.5010 - val_accuracy: 0.8640\n",
            "Epoch 5914/15000\n",
            "33/92 [=========>....................] - ETA: 1s - loss: 0.2833 - accuracy: 0.9015"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# dropout=best_params['model__dropout']\n",
        "# best_params['batch_size']\n",
        "def create_model(dropout):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=256, activation='relu', input_dim=x_train.shape[1]))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=16, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=8, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create the model\n",
        "final_model = create_model(0.2)\n",
        "\n",
        "# Further split the training set into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
        "\n",
        "# Define a callback to save the best weights during training\n",
        "checkpoint = ModelCheckpoint('best_weights.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "history = final_model.fit(x_train, y_train, epochs=15000, batch_size=512,validation_data=(x_val, y_val), callbacks=[checkpoint])\n",
        "\n",
        "# Load the best weights after training\n",
        "final_model.load_weights('best_weights.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate results"
      ],
      "metadata": {
        "id": "jLc6fLFlFRrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Visualize the training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metrics')\n",
        "plt.title('Training and Validation History')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = final_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "predictions = final_model.predict(x_test)\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "\n",
        "# Compute various metrics\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
        "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
        "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
        "# Display the metrics in a table\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Value': [accuracy, precision, recall, f1]\n",
        "})\n",
        "\n",
        "print(\"Metrics Table:\")\n",
        "print(metrics_table)\n",
        "\n",
        "# plot a confusion matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "class_labels = np.unique(y_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels, labels=class_labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
        "disp.plot(cmap='viridis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_lJlIoxdFTt2",
        "outputId": "fa80f18b-7fc6-4287-b496-3e3465ea9dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABymUlEQVR4nO3dd3QUZd/G8e9uKqnUQIDQe0c6SEeqKDYQkSLNEhBULNgo+oqFB1FQQKWIiCgKiPTQizSlS+81dFIhZTPvHwsrkRCSkGSSzfU5Zw/M7Mzub7IJubjbWAzDMBARERFxElazCxARERFJTwo3IiIi4lQUbkRERMSpKNyIiIiIU1G4EREREaeicCMiIiJOReFGREREnIrCjYiIiDgVhRsRERFxKgo3IvepV69elChRIk3nDh8+HIvFkr4FZTHHjx/HYrEwbdq0TH9vi8XC8OHDHdvTpk3DYrFw/Pjxe55bokQJevXqla713M/3SmbLTrWK/JfCjTgti8WSosfq1avNLjXHe/nll7FYLBw+fPiux7zzzjtYLBZ27dqViZWl3tmzZxk+fDg7duwwuxSHWwFz9OjRST5/K2RfunTpvt5n7969DB8+PEXhUSQjuZpdgEhG+eGHHxJtT58+nZCQkDv2V6xY8b7e59tvvyUhISFN57777ru89dZb9/X+zqBbt26MGzeOmTNn8v777yd5zE8//UTVqlWpVq1amt+ne/fuPP3003h4eKT5Ne7l7NmzjBgxghIlSlCjRo1Ez93P90pmS0ute/fuZcSIETRr1kytPmIqhRtxWs8++2yi7U2bNhESEnLH/v+Kjo7Gy8srxe/j5uaWpvoAXF1dcXXVj2G9evUoU6YMP/30U5LhZuPGjRw7doyPP/74vt7HxcUFFxeX+3qN+3E/3yuZLSvVmtqfSRF1S0mO1qxZM6pUqcLff/9NkyZN8PLy4u233wbg999/p0OHDhQuXBgPDw9Kly7NBx98gM1mS/Qa/x2bcHsXwDfffEPp0qXx8PCgTp06bN26NdG5SY25sVgsDBgwgHnz5lGlShU8PDyoXLkyS5YsuaP+1atXU7t2bTw9PSldujSTJk1K8TiedevW8dRTT1GsWDE8PDwICgrilVde4fr163dcn4+PD2fOnKFTp074+PhQoEABhgwZcsfX4tq1a/Tq1Qt/f39y585Nz549uXbt2j1rAXvrzf79+9m2bdsdz82cOROLxULXrl2JjY3l/fffp1atWvj7++Pt7U3jxo1ZtWrVPd8jqTE3hmHw4YcfUrRoUby8vGjevDn//PPPHedeuXKFIUOGULVqVXx8fPDz86Ndu3bs3LnTcczq1aupU6cOAM8995yj6/PWeKOkxrFERUXx2muvERQUhIeHB+XLl2f06NEYhpHouNR8X6SHpGqdNWsWtWrVwtfXFz8/P6pWrcoXX3wB2L+2Tz31FADNmzdPstv366+/pnLlynh4eFC4cGGCg4Pv+P64289kz549yZ8/P3FxcXfU2rp1a8qXL5+u1y/Zm/7LKDne5cuXadeuHU8//TTPPvssBQsWBOz/WPv4+PDqq6/i4+PDypUref/99wkPD+ezzz675+vOnDmTiIgInn/+eSwWC59++imPP/44R48evef/itevX8+cOXN46aWX8PX15csvv+SJJ57g5MmT5MuXD4Dt27fTtm1bAgMDGTFiBDabjZEjR1KgQIEUXffs2bOJjo7mxRdfJF++fGzZsoVx48Zx+vRpZs+enehYm81GmzZtqFevHqNHj2b58uX873//o3Tp0rz44ouAPSQ8+uijrF+/nhdeeIGKFSsyd+5cevbsmaJ6unXrxogRI5g5cyYPPPBAovf+5ZdfaNy4McWKFePSpUt89913dO3alX79+hEREcHkyZNp06YNW7ZsuaMr6F7ef/99PvzwQ9q3b0/79u3Ztm0brVu3JjY2NtFxR48eZd68eTz11FOULFmS8+fPM2nSJJo2bcrevXspXLgwFStWZOTIkbz//vv079+fxo0bA9CwYcMk39swDB555BFWrVpFnz59qFGjBkuXLuX111/nzJkzfP7554mOT8n3RXKio6OTHFcTHR19z3NDQkLo2rUrLVu25JNPPgFg3759bNiwgUGDBtGkSRNefvllvvzyS95++21Hd++tP4cPH86IESNo1aoVL774IgcOHGDChAls3bqVDRs2JPqZSOpn0tvbm+nTp7N06VIefvhhx7GhoaGsXLmSYcOG3fMaJAcxRHKI4OBg47/f8k2bNjUAY+LEiXccHx0dfce+559/3vDy8jJu3Ljh2NezZ0+jePHiju1jx44ZgJEvXz7jypUrjv2///67ARh//PGHY9+wYcPuqAkw3N3djcOHDzv27dy50wCMcePGOfZ17NjR8PLyMs6cOePYd+jQIcPV1fWO10xKUtc3atQow2KxGCdOnEh0fYAxcuTIRMfWrFnTqFWrlmN73rx5BmB8+umnjn3x8fFG48aNDcCYOnXqPWuqU6eOUbRoUcNmszn2LVmyxACMSZMmOV4zJiYm0XlXr141ChYsaPTu3TvRfsAYNmyYY3vq1KkGYBw7dswwDMO4cOGC4e7ubnTo0MFISEhwHPf2228bgNGzZ0/Hvhs3biSqyzDsn7WHh0eir83WrVvver3//V659TX78MMPEx335JNPGhaLJdH3QEq/L5Jy63vyXo+LFy/etdZBgwYZfn5+Rnx8/F3fZ/bs2QZgrFq1KtH+W1/n1q1bJ/oajh8/3gCMKVOmOPbd7WfSZrMZRYsWNbp06ZJo/5gxYwyLxWIcPXo02a+B5CzqlpIcz8PDg+eee+6O/bly5XL8PSIigkuXLtG4cWOio6PZv3//PV+3S5cu5MmTx7F963/xR48evee5rVq1onTp0o7tatWq4efn5zjXZrOxfPlyOnXqROHChR3HlSlThnbt2t3z9SHx9UVFRXHp0iUaNmyIYRhs3779juNfeOGFRNuNGzdOdC2LFi3C1dXV0ZID9jEuAwcOTFE9YB8ndfr0adauXevYN3PmTNzd3R1dHi4uLri7uwOQkJDAlStXiI+Pp3bt2kl2aSVn+fLlxMbGMnDgwERdeYMHD77jWA8PD6xW+z+ZNpuNy5cv4+PjQ/ny5VP9vrcsWrQIFxcXXn755UT7X3vtNQzDYPHixYn23+v74l769+9PSEjIHY/u3bvf89zcuXMTFRVFSEhIit7rdre+zoMHD3Z8DQH69euHn58fCxcuTHR8Uj+TVquVbt26MX/+fCIiIhz7f/zxRxo2bEjJkiVTXZc4L4UbyfGKFCni+GV5u3/++YfHHnsMf39//Pz8KFCggGMwclhY2D1ft1ixYom2bwWdq1evpvrcW+ffOvfChQtcv36dMmXK3HFcUvuScvLkSXr16kXevHkd42iaNm0K3Hl9np6ed3R33V4PwIkTJwgMDMTHxyfRcakZC/H000/j4uLCzJkzAbhx4wZz586lXbt2iYLi999/T7Vq1fD09CRfvnwUKFCAhQsXpuhzud2JEycAKFu2bKL9BQoUSPR+YA9Sn3/+OWXLlsXDw4P8+fNToEABdu3aler3vf39CxcujK+vb6L9t7pybtV3y72+L+6lbNmytGrV6o5HqVKl7nnuSy+9RLly5WjXrh1Fixald+/eKR7vc+s6/vu94O7uTqlSpe64zrv9TPbo0YPr168zd+5cAA4cOMDff/+donAmOYvCjeR4t7dg3HLt2jWaNm3Kzp07GTlyJH/88QchISGOsQYpmSJ7t1k5xn8Giqb3uSlhs9l46KGHWLhwIW+++Sbz5s0jJCTEMfD1v9eXWTOMAgICeOihh/jtt9+Ii4vjjz/+ICIigm7dujmOmTFjBr169aJ06dJMnjyZJUuWEBISQosWLTJ0mvVHH33Eq6++SpMmTZgxYwZLly4lJCSEypUrZ9r07oz+vkhOQEAAO3bsYP78+Y5xQu3atUvxmKrUSOpnEqBSpUrUqlWLGTNmAPbvBXd3dzp37pzuNUj2pgHFIklYvXo1ly9fZs6cOTRp0sSx/9ixYyZW9a+AgAA8PT2TXPQuuYXwbtm9ezcHDx7k+++/p0ePHo79aelyuKV48eKsWLGCyMjIRK03Bw4cSNXrdOvWjSVLlrB48WJmzpyJn58fHTt2dDz/66+/UqpUKebMmZOoKyktA0qLFy8OwKFDhxK1Xly8ePGO1pBff/2V5s2bM3ny5ET7r127Rv78+R3bqVlxunjx4ixfvpyIiIhErTe3uj1v1ZdVuLu707FjRzp27EhCQgIvvfQSkyZN4r333qNMmTJ3vfZb13HgwIFEX+fY2FiOHTtGq1atUlxDjx49ePXVVzl37hwzZ86kQ4cOd7SyiajlRiQJt/6HfPv/iGNjY/n666/NKikRFxcXWrVqxbx58zh79qxj/+HDh+8Yp3G38yHx9RmG4ZjWmxbt27cnPj6eCRMmOPbZbDbGjRuXqtfp1KkTXl5efP311yxevJjHH38cT0/PZGvfvHkzGzduTHXNrVq1ws3NjXHjxiV6vbFjx95xrIuLyx0tJLNnz+bMmTOJ9nl7ewOkaAp8+/btsdlsjB8/PtH+zz//HIvFkuLxU5nh8uXLibatVqtjQcWYmBjg7tfeqlUr3N3d+fLLLxN9DSdPnkxYWBgdOnRIcR1du3bFYrEwaNAgjh49es91qyRnUsuNSBIaNmxInjx56Nmzp+PWAD/88EOmNP+n1PDhw1m2bBmNGjXixRdfdPySrFKlyj2X/q9QoQKlS5dmyJAhnDlzBj8/P3777bcUj91ISseOHWnUqBFvvfUWx48fp1KlSsyZMyfV41F8fHzo1KmTY9zN7V1SAA8//DBz5szhscceo0OHDhw7doyJEydSqVIlIiMjU/Vet9brGTVqFA8//DDt27dn+/btLF68OFFrzK33HTlyJM899xwNGzZk9+7d/Pjjj3eMVyldujS5c+dm4sSJ+Pr64u3tTb169ZIc8NqxY0eaN2/OO++8w/Hjx6levTrLli3j999/Z/DgwYkGD5utb9++XLlyhRYtWlC0aFFOnDjBuHHjqFGjhmOMUI0aNXBxceGTTz4hLCwMDw8PWrRoQUBAAEOHDmXEiBG0bduWRx55hAMHDvD1119Tp06dVAWUAgUK0LZtW2bPnk3u3LlTFYwk51DLjUgS8uXLx4IFCwgMDOTdd99l9OjRPPTQQ3z66adml+ZQq1YtFi9eTJ48eXjvvfeYPHkyI0eOpGXLlolaOpLi5ubGH3/8QY0aNRg1ahQjRoygbNmyTJ8+Pc31WK1W5s+fT7du3ZgxYwbvvPMORYoU4fvvv0/1a90KNIGBgbRo0SLRc7169eKjjz5i586dvPzyyyxdupQZM2ZQu3btNNX94YcfMmLECLZv387rr7/OkSNHWLZsmaMV4pa3336b1157jaVLlzJo0CC2bdvGwoULCQoKSnScm5sb33//PS4uLrzwwgt07dqVNWvWJPnet75mgwcPZsGCBQwePJi9e/fy2WefMWbMmDRdT0Z59tln8fT05Ouvv+all17i+++/p0uXLixevNgxA6pQoUJMnDiRCxcu0KdPH7p27crevXsBexgfP348J0+e5JVXXuGXX36hf//+LFu2LNWrId/qSu3cuXOG3kpDsi+LkZX+Kyoi961Tp078888/HDp0yOxSRDLE77//TqdOnVi7dq1jiQWR26nlRiQb+++tEg4dOsSiRYto1qyZOQWJZIJvv/2WUqVK8eCDD5pdimRRGnMjko2VKlWKXr16OdYKmTBhAu7u7rzxxhtmlyaS7mbNmsWuXbtYuHAhX3zxRapmpknOom4pkWzsueeeY9WqVYSGhuLh4UGDBg346KOPEt2bScRZWCwWfHx86NKlCxMnTsTVVf8/l6Qp3IiIiIhT0ZgbERERcSoKNyIiIuJUclyHZUJCAmfPnsXX11eD0URERLIJwzCIiIigcOHCie4un5QcF27Onj17x6JbIiIikj2cOnWKokWLJntMjgs3t25Od+rUKfz8/EyuRkRERFIiPDycoKCgRDeZvZscF25udUX5+fkp3IiIiGQzKRlSogHFIiIi4lQUbkRERMSpKNyIiIiIU8lxY25EROT+2Ww24uLizC5DnIy7u/s9p3mnhMKNiIikmGEYhIaGcu3aNbNLESdktVopWbIk7u7u9/U6CjciIpJit4JNQEAAXl5eWgxV0s2tRXbPnTtHsWLF7ut7S+FGRERSxGazOYJNvnz5zC5HnFCBAgU4e/Ys8fHxuLm5pfl1NKBYRERS5NYYGy8vL5MrEWd1qzvKZrPd1+so3IiISKqoK0oySnp9bynciIiIiFNRuBEREUmlEiVKMHbs2BQfv3r1aiwWi2aZZRKFGxERcVoWiyXZx/Dhw9P0ulu3bqV///4pPr5hw4acO3cOf3//NL1fSilE2Wm2VDracyaMPN7uFMmdy+xSREQEOHfunOPvP//8M++//z4HDhxw7PPx8XH83TAMbDYbrq73/tVYoECBVNXh7u5OoUKFUnWOpJ1abtLJwfMRdJ+8mc4TN3L8UpTZ5YiICFCoUCHHw9/fH4vF4tjev38/vr6+LF68mFq1auHh4cH69es5cuQIjz76KAULFsTHx4c6deqwfPnyRK/7324pi8XCd999x2OPPYaXlxdly5Zl/vz5juf/26Iybdo0cufOzdKlS6lYsSI+Pj60bds2URiLj4/n5ZdfJnfu3OTLl48333yTnj170qlTpzR/Pa5evUqPHj3IkycPXl5etGvXjkOHDjmeP3HiBB07diRPnjx4e3tTuXJlFi1a5Di3W7duFChQgFy5clG2bFmmTp2a5loyksJNOvHxcCWPlztnrl3nqUkbORAaYXZJIiIZzjAMomPjM/1hGEa6XcNbb73Fxx9/zL59+6hWrRqRkZG0b9+eFStWsH37dtq2bUvHjh05efJksq8zYsQIOnfuzK5du2jfvj3dunXjypUrdz0+Ojqa0aNH88MPP7B27VpOnjzJkCFDHM9/8skn/Pjjj0ydOpUNGzYQHh7OvHnz7utae/XqxV9//cX8+fPZuHEjhmHQvn17xzT/4OBgYmJiWLt2Lbt37+aTTz5xtG6999577N27l8WLF7Nv3z4mTJhA/vz576uejKJuqXRSOHcufn6+Ad0nb2Z/aARdvtnI9N51qVY0t9mliYhkmOtxNiq9vzTT33fvyDZ4uafPr7CRI0fy0EMPObbz5s1L9erVHdsffPABc+fOZf78+QwYMOCur9OrVy+6du0KwEcffcSXX37Jli1baNu2bZLHx8XFMXHiREqXLg3AgAEDGDlypOP5cePGMXToUB577DEAxo8f72hFSYtDhw4xf/58NmzYQMOGDQH48ccfCQoKYt68eTz11FOcPHmSJ554gqpVqwJQqlQpx/knT56kZs2a1K5dG7C3XmVVarlJRwV8PZjVvz41gnJzLTqOZ77dzJZjd0/tIiJivlu/rG+JjIxkyJAhVKxYkdy5c+Pj48O+ffvu2XJTrVo1x9+9vb3x8/PjwoULdz3ey8vLEWwAAgMDHceHhYVx/vx56tat63jexcWFWrVqperabrdv3z5cXV2pV6+eY1++fPkoX748+/btA+Dll1/mww8/pFGjRgwbNoxdu3Y5jn3xxReZNWsWNWrU4I033uDPP/9Mcy0ZTS036Sy3lzsz+taj7/db2XT0Cj2mbGZS99o0LZe6wWciItlBLjcX9o5sY8r7phdvb+9E20OGDCEkJITRo0dTpkwZcuXKxZNPPklsbGyyr/Pf2wVYLBYSEhJSdXx6drelRd++fWnTpg0LFy5k2bJljBo1iv/9738MHDiQdu3aceLECRYtWkRISAgtW7YkODiY0aNHm1pzUtRykwF8PFyZ9lxdmpcvwI24BPp+v5Ule87d+0QRkWzGYrHg5e6a6Y+MXCV5w4YN9OrVi8cee4yqVatSqFAhjh8/nmHvlxR/f38KFizI1q1bHftsNhvbtm1L82tWrFiR+Ph4Nm/e7Nh3+fJlDhw4QKVKlRz7goKCeOGFF5gzZw6vvfYa3377reO5AgUK0LNnT2bMmMHYsWP55ptv0lxPRlLLTQbxdHNhUvfavPLzDhbuPkfwzO189qSNxx8oanZpIiKSjLJlyzJnzhw6duyIxWLhvffeS7YFJqMMHDiQUaNGUaZMGSpUqMC4ceO4evVqioLd7t278fX1dWxbLBaqV6/Oo48+Sr9+/Zg0aRK+vr689dZbFClShEcffRSAwYMH065dO8qVK8fVq1dZtWoVFStWBOD999+nVq1aVK5cmZiYGBYsWOB4LqtRuMlA7q5WvuxaEy93F2b/fZpXf9lJVKyN7vWLm12aiIjcxZgxY+jduzcNGzYkf/78vPnmm4SHh2d6HW+++SahoaH06NEDFxcX+vfvT5s2bXBxuXeXXJMmTRJtu7i4EB8fz9SpUxk0aBAPP/wwsbGxNGnShEWLFjm6yGw2G8HBwZw+fRo/Pz/atm3L559/DtjX6hk6dCjHjx8nV65cNG7cmFmzZqX/hacDi2F2B18mCw8Px9/fn7CwMPz8/DLlPRMSDEYu2Mu0P48DMLRdBZ5vWjr5k0REspgbN25w7NgxSpYsiaenp9nl5DgJCQlUrFiRzp0788EHH5hdToZI7nssNb+/1XKTCaxWC8M6VsLbw4WvVh1h1OL9RMbE8+pD5XR3XRERSdKJEydYtmwZTZs2JSYmhvHjx3Ps2DGeeeYZs0vL8jSgOJNYLBZeb1OBN9qWB2DcysOMXLDX9JHxIiKSNVmtVqZNm0adOnVo1KgRu3fvZvny5Vl2nEtWopabTPZSszL4eLjy/u//MHXDcaJjbHz0eFVcrGrBERGRfwUFBbFhwwazy8iW1HJjgh4NSjD6qepYLfDzX6cYNGs7cbbMH4kvIiLijBRuTPJkraKMf+YB3FwsLNh1jhd++JsbcTazyxIREcn2FG5M1L5qIN/0qI2Hq5UV+y/Qe9pWomLizS5LREQkW1O4MVnz8gF837su3u4u/HnkMs9O3kxYdJzZZYmIiGRbCjdZQP1S+fixX338c7mx/eQ1un67iUuRMWaXJSIiki0p3GQRNYJy8/Pz9cnv48Hec+F0mbSRc2HXzS5LREQk21G4yUIqFPLjl+frU9jfkyMXo3hq4kZOXo42uywRkRyvWbNmDB482LFdokQJxo4dm+w5FouFefPm3fd7p9fr5CQKN1lMqQI+/PJCA0rk8+L01es8OfFPDp2PMLssEZFsqWPHjrRt2zbJ59atW4fFYmHXrl2pft2tW7fSv3//+y0vkeHDh1OjRo079p87d4527dql63v917Rp08idO3eGvkdmMjXcjBo1ijp16uDr60tAQACdOnXiwIEDKT5/1qxZWCwWOnXqlHFFmqBoHi9+eb4B5Qv6ciEihi7fbGLPmTCzyxIRyXb69OlDSEgIp0+fvuO5qVOnUrt2bapVq5bq1y1QoABeXl7pUeI9FSpUCA8Pj0x5L2dharhZs2YNwcHBbNq0iZCQEOLi4mjdujVRUVH3PPf48eMMGTKExo0bZ0KlmS/Az5NZ/etTrag/V6Ji6frNJv46fsXsskREspWHH36YAgUKMG3atET7IyMjmT17Nn369OHy5ct07dqVIkWK4OXlRdWqVfnpp5+Sfd3/dksdOnSIJk2a4OnpSaVKlQgJCbnjnDfffJNy5crh5eVFqVKleO+994iLs8+OnTZtGiNGjGDnzp1YLBYsFouj5v92S+3evZsWLVqQK1cu8uXLR//+/YmMjHQ836tXLzp16sTo0aMJDAwkX758BAcHO94rLU6ePMmjjz6Kj48Pfn5+dO7cmfPnzzue37lzJ82bN8fX1xc/Pz9q1arFX3/9BdjvkdWxY0fy5MmDt7c3lStXZtGiRWmuJSVMvf3CkiVLEm1PmzaNgIAA/v777ztu1347m81Gt27dGDFiBOvWrePatWsZXKk58ni782PfevSZ9hdbjl+h++QtfNujNg+WzW92aSIidoYBcSaMDXTzghTceNjV1ZUePXowbdo03nnnHcfNimfPno3NZqNr165ERkZSq1Yt3nzzTfz8/Fi4cCHdu3endOnS1K1b957vkZCQwOOPP07BggXZvHkzYWFhicbn3OLr68u0adMoXLgwu3fvpl+/fvj6+vLGG2/QpUsX9uzZw5IlS1i+fDkA/v7+d7xGVFQUbdq0oUGDBmzdupULFy7Qt29fBgwYkCjArVq1isDAQFatWsXhw4fp0qULNWrUoF+/fve8nqSu71awWbNmDfHx8QQHB9OlSxdWr14NQLdu3ahZsyYTJkzAxcWFHTt24ObmBkBwcDCxsbGsXbsWb29v9u7di4+PT6rrSI0sdW+psDB710vevHmTPW7kyJEEBATQp08f1q1blxmlmcbX043ve9fl+Rl/s/bgRXpP28r4Z2rSunIhs0sTEbEHm48KZ/77vn0W3L1TdGjv3r357LPPWLNmDc2aNQPsXVJPPPEE/v7++Pv7M2TIEMfxAwcOZOnSpfzyyy8pCjfLly9n//79LF26lMKF7V+Ljz766I5xMu+++67j7yVKlGDIkCHMmjWLN954g1y5cuHj44OrqyuFCt393/eZM2dy48YNpk+fjre3/frHjx9Px44d+eSTTyhYsCAAefLkYfz48bi4uFChQgU6dOjAihUr0hRuVqxYwe7duzl27BhBQUEATJ8+ncqVK7N161bq1KnDyZMnef3116lQoQIAZcuWdZx/8uRJnnjiCapWrQpAqVKlUl1DamWZAcUJCQkMHjyYRo0aUaVKlbset379eiZPnsy3336boteNiYkhPDw80SO7yeXuwrc9atG2ciFibQm8+OM2ft9xxuyyRESyhQoVKtCwYUOmTJkCwOHDh1m3bh19+vQB7L0BH3zwAVWrViVv3rz4+PiwdOlSTp48maLX37dvH0FBQY5gA9CgQYM7jvv5559p1KgRhQoVwsfHh3fffTfF73H7e1WvXt0RbAAaNWpEQkJCojGrlStXxsXFxbEdGBjIhQsXUvVet79nUFCQI9gAVKpUidy5c7Nv3z4AXn31Vfr27UurVq34+OOPOXLkiOPYl19+mQ8//JBGjRoxbNiwNA3gTq0s03ITHBzMnj17WL9+/V2PiYiIoHv37nz77bfkz5+yrplRo0YxYsSI9CrTNB6uLox/piZv/LqLOdvPMPjnHUTH2uhat5jZpYlITubmZW9FMeN9U6FPnz4MHDiQr776iqlTp1K6dGmaNm0KwGeffcYXX3zB2LFjqVq1Kt7e3gwePJjY2Nh0K3fjxo2O4RRt2rTB39+fWbNm8b///S/d3uN2t7qEbrFYLCQkZNwNmocPH84zzzzDwoULWbx4McOGDWPWrFk89thj9O3blzZt2rBw4UKWLVvGqFGj+N///sfAgQMzrJ4s0XIzYMAAFixYwKpVqyhatOhdjzty5AjHjx+nY8eOuLq64urqyvTp05k/fz6urq6JkuItQ4cOJSwszPE4depURl5KhnJ1sTL6qeo8W78YhgFD5+zmu3VHzS5LRHIyi8XePZTZjxSMt7ld586dsVqtzJw5k+nTp9O7d2/H+JsNGzbw6KOP8uyzz1K9enVKlSrFwYMHU/zaFStW5NSpU5w7d86xb9OmTYmO+fPPPylevDjvvPMOtWvXpmzZspw4cSLRMe7u7thsyd9AuWLFiuzcuTPRxJsNGzZgtVopX758imtOjVvXd/vvz71793Lt2jUqVark2FeuXDleeeUVli1bxuOPP87UqVMdzwUFBfHCCy8wZ84cXnvttRT3vqSVqS03hmEwcOBA5s6dy+rVqylZsmSyx1eoUIHdu3cn2vfuu+8SERHBF198kajJ7BYPDw+nmkJntVr44NEqeHu4MmnNUT5cuI/ImHgGtSzr+EEVEZHEfHx86NKlC0OHDiU8PJxevXo5nitbtiy//vorf/75J3ny5GHMmDGcP38+0S/u5LRq1Ypy5crRs2dPPvvsM8LDw3nnnXcSHVO2bFlOnjzJrFmzqFOnDgsXLmTu3LmJjilRogTHjh1jx44dFC1aFF9f3zt+f3Xr1o1hw4bRs2dPhg8fzsWLFxk4cCDdu3d3jLdJK5vNxo4dOxLt8/DwoFWrVlStWpVu3boxduxY4uPjeemll2jatCm1a9fm+vXrvP766zz55JOULFmS06dPs3XrVp544gkABg8eTLt27ShXrhxXr15l1apVVKxY8b5qvRdTW26Cg4OZMWMGM2fOxNfXl9DQUEJDQ7l+/d/bDvTo0YOhQ4cC4OnpSZUqVRI9cufOja+vL1WqVMHd3d2sS8lUFouFt9pWYEjrcgCMXX6IjxbtwzAMkysTEcm6+vTpw9WrV2nTpk2i8THvvvsuDzzwAG3atKFZs2YUKlQoVeunWa1W5s6dy/Xr16lbty59+/bl//7v/xId88gjj/DKK68wYMAAatSowZ9//sl7772X6JgnnniCtm3b0rx5cwoUKJDkdHQvLy+WLl3KlStXqFOnDk8++SQtW7Zk/PjxqftiJCEyMpKaNWsmenTs2BGLxcLvv/9Onjx5aNKkCa1ataJUqVL8/PPPALi4uHD58mV69OhBuXLl6Ny5M+3atXMMCbHZbAQHB1OxYkXatm1LuXLl+Prrr++73uRYDBN/I96tpWHq1KmOVN2sWTNKlChxxxoFt/Tq1Ytr166leGnq8PBw/P39CQsLw8/PLw1VZy1T1h9j5IK9AHStW4wPO1XBxaoWHBFJfzdu3ODYsWOULFkST09Ps8sRJ5Tc91hqfn+b3i11L7fm0N/N3UJPTtH7wZL4eLjy5pxd/LTlJNGx8Yx+qjpuLlliOJWIiEim029AJ9C5ThBfPl0TV6uF33ec5aUftxETn/ygNBEREWelcOMkOlYvzKTutXB3tRKy9zx9v/+L6Nh4s8sSERHJdAo3TqRlxYJM61UHL3cX1h26RI/JWwi/kfZ7iYiIiGRHCjdOpmGZ/MzoWw8/T1f+OnGVZ77dxJWo9FuISkREMzMlo6TX95bCjRN6oFgefupfn3ze7uw5E06XSRs5H37D7LJEJJu7teptdLQJN8qUHOHWqtC33zoiLbLM7RckfVUu7M/Pzzfg2e82c+hCJE9N3MiPfesRlDd1S5aLiNzi4uJC7ty5Hfco8vLy0uKhkm4SEhK4ePEiXl5euLreXzwxdZ0bMzjbOjf3cupKNN2+28zJK9EU8vNkRt96lAnI2FvNi4jzMgyD0NBQrl27ZnYp4oSsVislS5ZMclHe1Pz+VrjJAc6H33C04OTzdueHPvWoVDhnXLuIZAybzUZcnCYsSPpyd3fHak16xIzCTTJyYrgBuBIVS48pm9lzJhw/T1em9a7LA8XymF2WiIhIiqTm97cGFOcQeb3dmdmvPrWL5yH8RjzPfreZPw9fMrssERGRdKdwk4P4eboxvU9dHiyTn+hYG72mbWXl/vNmlyUiIpKuFG5yGC93V77rWZuHKhUkNj6B/tP/ZsGus2aXJSIikm4UbnIgTzcXvu72AI/WKEx8gsHLP23nl62nzC5LREQkXSjc5FBuLlbGdK5B17rFSDDgjd92MXXDMbPLEhERuW8KNzmYi9XCR49VoV/jkgCM+GMv41ce0tLqIiKSrSnc5HAWi4W321dkcKuyAIxedpBPlhxQwBERkWxL4UawWCwMblWOdztUBGDimiO8//s/JCQo4IiISPajcCMOfRuX4qPHqmKxwA+bTjDk153E2xLMLktERCRVFG4kkWfqFWNslxq4WC3M2XaGATO3ExNvM7ssERGRFFO4kTs8WqMIE7o9gLuLlSX/hNJv+t9cj1XAERGR7EHhRpLUunIhJveqTS43F9YevEjPKVuIuKGb5ImISNancCN31bhsAX7oUxdfD1e2HL9Ct+82czUq1uyyREREkqVwI8mqXSIvP/WvTx4vN3adDqPLNxu5EH7D7LJERETuSuFG7qlKEX9+eb4BAb4eHDwfSedJGzl9NdrsskRERJKkcCMpUragL7++0JCieXJx/HI0nSdu5OjFSLPLEhERuYPCjaRYsXxezH6hAaUKeHM27AadJ21i37lws8sSERFJROFGUiXQPxe/PN+AioF+XIqM4elvNrHj1DWzyxIREXFQuJFUy+/jwax+9alZLDdh1+Po9u0mNh29bHZZIiIigMKNpJG/lxsz+tSjYel8RMXa6DllC6sOXDC7LBEREYUbSTtvD1em9KpDywoBxMQn0H/6Xyzafc7sskREJIdTuJH74unmwsTutXi4WiBxNoMBM7fx69+nzS5LRERyMIUbuW9uLla+eLomnWsXJcGAIbN3Mn3jcbPLEhGRHErhRtKFi9XCx49X47lGJQB4//d/+Hr1YXOLEhGRHEnhRtKN1Wrh/YcrMbBFGQA+XXKAT5fsxzAMkysTEZGcROFG0pXFYuG11uV5q10FAL5efYQRf+wlIUEBR0REMofCjWSIF5qW5oNOVQCY9udx3vhtFzYFHBERyQQKN5JhutcvzpjO1bFa4Ne/T/PyT9uJjU8wuywREXFyCjeSoR5/oChfd3sANxcLC3ef4/kf/uJGnM3sskRExIkp3EiGa1slkO961sHTzcqqAxfpNXULkTHxZpclIiJOSuFGMkXTcgX4/rm6+Hi4sunoFbp9t5lr0bFmlyUiIk5I4UYyTb1S+fixbz1ye7mx89Q1nv5mExcjYswuS0REnIzCjWSq6kG5+bl/A/L7eLA/NIIukzZy9tp1s8sSEREnonAjma58IV9mv9CAIrlzcfRSFE9N3MjxS1FmlyUiIk5C4UZMUTK/N7+80ICS+b05c+06T03ayIHQCLPLEhERJ6BwI6YpkjsXPz9fnwqFfLkYEUOXbzay6/Q1s8sSEZFsTuFGTBXg68ms/vWpHpSba9FxPPPtZrYcu2J2WSIiko0p3Ijpcnu582PfetQrmZfImHh6TNnM2oMXzS5LRESyKYUbyRJ8PFyZ9lxdmpUvwI24BPp+/xdL9oSaXZaIiGRDCjeSZeRyd+Gb7rVpV6UQsbYEgmduY+7202aXJSIi2YzCjWQp7q5WxnWtyRMPFMWWYPDqLzuZsemE2WWJiEg2Ymq4GTVqFHXq1MHX15eAgAA6derEgQMHkj3n22+/pXHjxuTJk4c8efLQqlUrtmzZkkkVS2ZwdbHy2ZPV6NGgOIYB787bw6Q1R8wuS0REsglTw82aNWsIDg5m06ZNhISEEBcXR+vWrYmKuvuCbqtXr6Zr166sWrWKjRs3EhQUROvWrTlz5kwmVi4ZzWq1MOKRyrzYrDQAoxbvZ8yyAxiGYXJlIiKS1VmMLPTb4uLFiwQEBLBmzRqaNGmSonNsNht58uRh/Pjx9OjR457Hh4eH4+/vT1hYGH5+fvdbsmSCr1Yd5rOl9ha93o1K8t7DFbFYLCZXJSIimSk1v79dM6mmFAkLCwMgb968KT4nOjqauLi4u54TExNDTMy/N2cMDw+/vyIl0wU3L4O3uwvD/9jLlA3HiI6N5/8eq4qLVQFHRETulGUGFCckJDB48GAaNWpElSpVUnzem2++SeHChWnVqlWSz48aNQp/f3/HIygoKL1KlkzUq1FJPn2yGlYLzNp6isE/7yDOlmB2WSIikgVlmXATHBzMnj17mDVrVorP+fjjj5k1axZz587F09MzyWOGDh1KWFiY43Hq1Kn0KlkyWefaQYzr+gCuVgt/7DzLizP+5kaczeyyREQki8kS4WbAgAEsWLCAVatWUbRo0RSdM3r0aD7++GOWLVtGtWrV7nqch4cHfn5+iR6SfXWoFsi3PWrj4Wpl+b4L9Pl+K1Ex8WaXJSIiWYip4cYwDAYMGMDcuXNZuXIlJUuWTNF5n376KR988AFLliyhdu3aGVylZDXNKwQw7bm6eLu7sOHwZbpP3kzY9TizyxIRkSzC1HATHBzMjBkzmDlzJr6+voSGhhIaGsr169cdx/To0YOhQ4c6tj/55BPee+89pkyZQokSJRznREZGmnEJYpIGpfMxo289/Dxd2XbyGl2/2cTlyJh7nygiIk7P1HAzYcIEwsLCaNasGYGBgY7Hzz//7Djm5MmTnDt3LtE5sbGxPPnkk4nOGT16tBmXICaqWSwPPz/fgPw+7uw9F07nSRsJDbthdlkiImKyLLXOTWbQOjfO58jFSJ79bjPnwm4QlDcXP/apT7F8XmaXJSIi6Sg1v7+zxIBikftRuoAPvzzfgOL5vDh15TpPTfqTwxcizC5LRERMonAjTiEorxezn29AuYI+nA+PofOkTew5E2Z2WSIiYgKFG3EaAX6ezOrfgKpF/LkSFUvXbzfx94krZpclIiKZTOFGnEpeb3d+7FePOiXyEHEjnme/28L6Q5fMLktERDKRwo04HT9PN77vXZfGZfNzPc5G72lbCdl73uyyREQkkyjciFPycnflu561aVO5ILG2BF6Y8Te/7zhjdlkiIpIJFG7EaXm4uvDVMw/wWM0i2BIMBv+8g1lbTppdloiIZDCFG3Fqri5W/vdUdbrVK4ZhwFtzdvPduqNmlyUiIhlI4UacntVq4cNOVejfpBQAHy7cxxfLD5HD1q8UEckxFG4kR7BYLAxtV4FXHyoHwOfLDzJq8X4FHBERJ6RwIzmGxWLh5ZZlee/hSgB8s/Yo78zbQ0KCAo6IiDNRuJEcp8+DJfn48apYLDBz80le/WUH8bYEs8sSEZF0onAjOdLTdYvxxdM1cbVamLfjLC/9uI2YeJvZZYmISDpQuJEc65HqhZn4bC3cXa0s23uevt//RXRsvNlliYjIfVK4kRytVaWCTO1Vh1xuLqw7dImeU7YQfiPO7LJEROQ+KNxIjteoTH5m9K2Lr6crW49fpf0X61h78KLZZYmISBop3IgAtYrn5ad+9SmSOxenr16nx5QtvPLzDi5HxphdmoiIpJLCjchNVYr4s+yVJjzXqAQWC8zdfoZWY9YwZ9tprYcjIpKNKNyI3Mbbw5VhHSsz96VGVCjky9XoOF79ZSc9pmzh5OVos8sTEZEUULgRSUKNoNz8MfBBXm9THndXK+sOXaL12DV8s/aI1sQREcniFG5E7sLNxUpw8zIsHdyEBqXycSMugY8W7afT1xvYcybM7PJEROQuFG5E7qFkfm9m9qvHp09Uwz+XG3vOhPPoVxv4aNE+rsdq4T8RkaxG4UYkBSwWC53rBLH81aY8XC0QW4LBN2uP0nrsGtYd0rRxEZGsROFGJBUK+How/pkHmNKrNoX9PTl15TrdJ2/h1Z93cCUq1uzyREQEhRuRNGlRoSDLXm1Kr4b2aeNzbk4bn7td08ZFRMymcCOSRj4ergx/pDJzXmxIhUK+XImK5ZWfd9Jz6lZOXdG0cRERsyjciNynmsXyJJo2vvbgRVp/vpZv1x7VtHERERMo3Iikg1vTxpcMakz9Unm5Hmfj/xbt47Gv/9S0cRGRTKZwI5KOShXw4ad+9fnkiar4ebqy+0wYj361gVGaNi4ikmkUbkTSmcVioUudYix/rSkdbk4bn7T2KG3GrmX9oUtmlyci4vQUbkQySICvJ1898wDf9ahNoL8nJ69E8+zkzbz6yw6uatq4iEiGUbgRyWCtKhUk5PZp49vO0HLMGuZtP6Np4yIiGUDhRiQT3Jo2/tuLDSlX0IcrUbEM/nkHvTRtXEQk3SnciGSiB4rlYcHAxrz2UDncXaysuTlt/Lt1mjYuIpJeFG5EMpm7q5WBLcuyeHBj6pa0Txv/cKF92vg/ZzVtXETkfinciJikdAEfZvWrz6jHq+J7c9r4I+M3MGqxpo2LiNwPhRsRE1mtFrrWLcaKV5vSvmoh+7TxNUdp+8VaNhzWtHERkbRQuBHJAgL8PPm6Wy2+7VGbQn6enLgcTbfvNvPaLzs1bVxEJJUUbkSykIcqFSTk1Sb0aFAciwV+23aaVmPW8PsOTRsXEUkphRuRLMbX042Rj1bh1xfs08YvR8UyaNYOnpu2ldNXNW1cROReFG5Esqhaxe3Txl+9OW189QH7tPHJ649hS1ArjojI3SjciGRh7q5WXm5ZlkWDGlO3RF6iY218sGAvj3+9gb1nw80uT0QkS1K4EckGygT4MKt/fT56zD5tfOfpMDqOX88nS/ZzI07TxkVEbqdwI5JNWK0Wnqlnnzberop92viE1UdoO3Ytf2rauIiIg8KNSDYT4OfJhGdr8U33WhTy8+T45Wie+W4zr8/eybVoTRsXEVG4EcmmWlcuRMirTehe3z5tfPbf9mnj83ee1bRxEcnRFG5EsjFfTzc+6FSFX19oQNkAHy5FxvLyT9vpPW0rZ65dN7s8ERFTKNyIOIFaxfOy4OUHeaWVfdr4qgMXeWjMGqZo2riI5EAKNyJOwsPVhUGtyrJo0IPUKZGH6FgbIxfs5fEJf7LvnKaNi0jOYWq4GTVqFHXq1MHX15eAgAA6derEgQMH7nne7NmzqVChAp6enlStWpVFixZlQrUi2UOZAF9+7t+A/3usCr4eruw8dY2O49bzqaaNi0gOYWq4WbNmDcHBwWzatImQkBDi4uJo3bo1UVFRdz3nzz//pGvXrvTp04ft27fTqVMnOnXqxJ49ezKxcpGszWq10K1ecZa/1pS2lQsRn2Dw9a1p40c0bVxEnJvFyELTKi5evEhAQABr1qyhSZMmSR7TpUsXoqKiWLBggWNf/fr1qVGjBhMnTrzne4SHh+Pv709YWBh+fn7pVrtIVrb0n1De/30P58NjAOhcuyhvt69Ibi93kysTEUmZ1Pz+zlJjbsLCwgDImzfvXY/ZuHEjrVq1SrSvTZs2bNy4McnjY2JiCA8PT/QQyWnaVC5EyKtNebZ+MQB++cs+bfwPTRsXESeUZcJNQkICgwcPplGjRlSpUuWux4WGhlKwYMFE+woWLEhoaGiSx48aNQp/f3/HIygoKF3rFsku/Dzd+LBTVX59oQFlbk4bH/jTdvp8/5emjYuIU8ky4SY4OJg9e/Ywa9asdH3doUOHEhYW5nicOnUqXV9fJLupXSIvC19+kMGtyuLmYmHl/gu0HrOGqRs0bVxEnEOaws22bdvYvXu3Y/v333+nU6dOvP3228TGpn759wEDBrBgwQJWrVpF0aJFkz22UKFCnD9/PtG+8+fPU6hQoSSP9/DwwM/PL9FDJKfzcHVhcKtyLB7UmNrF8xAVa2PEH3t5YsKf7A9V162IZG9pCjfPP/88Bw8eBODo0aM8/fTTeHl5MXv2bN54440Uv45hGAwYMIC5c+eycuVKSpYsec9zGjRowIoVKxLtCwkJoUGDBqm7CBGhTIAvvzzfgA872aeN7zh1jYe/XM9nSzVtXESyrzSFm4MHD1KjRg3AvuZMkyZNmDlzJtOmTeO3335L8esEBwczY8YMZs6cia+vL6GhoYSGhnL9+r/9/z169GDo0KGO7UGDBrFkyRL+97//sX//foYPH85ff/3FgAED0nIpIjme1Wrh2frFCXm1KW0qFyQ+weCrVUdo98U6Nh65bHZ5IiKplqZwYxgGCQkJACxfvpz27dsDEBQUxKVLKV9DY8KECYSFhdGsWTMCAwMdj59//tlxzMmTJzl37pxju2HDhsycOZNvvvmG6tWr8+uvvzJv3rxkByGLyL0V8vdkUvfaTHz2AQJ8PTh2KYqu327izV93ERYdZ3Z5IiIplqZ1blq0aEFQUBCtWrWiT58+7N27lzJlyrBmzRp69uzJ8ePHM6DU9KF1bkTuLex6HJ8s2c/MzScByO/jwfBHKtGhaiAWi8Xk6kQkJ8rwdW7Gjh3Ltm3bGDBgAO+88w5lypQB4Ndff6Vhw4ZpeUkRyUL8c7nx0WNV+eX5BpQu4M2lyBgGzNxO3+//4qymjYtIFpeuKxTfuHEDFxcX3Nzc0usl051abkRSJybexlerjjBh9WHibAbe7i683qY83RuUwMWqVhwRyRwZ3nKzdetWNm/efMf+nTt3snPnzrS8ZPZnGHB8PVw+ArHRZlcjkm48XF149aFyLHy5MbVuThsf/sdenpz4JwdCI8wuT0TkDmlqualbty5vvPEGTz75ZKL9c+bM4ZNPPkky+GQVGdZyc/0qfFLi321Pf/AtDH6B4Hvz4Rd4277C4F0ArFlmHUWRe0pIMPhx8wk+WXKAyJh4XK0WXmhamgEtyuDp5mJ2eSLixFLz+ztN4cbHx4ddu3ZRqlSpRPuPHTtGtWrViIjIuv+by7Bwc/UE/PAYRJyDuBS23FhdwafQvwHIrzD4FkocgPwCwd07/eoUSQfnwq7z/u//ELLXvqBmqfzefPR4VeqXymdyZSLirFLz+9s1LW/g4eHB+fPn7wg3586dw9U1TS+Z/eUpDi9vs3dP3QiDiFCIOAvh52778xyEn7X/GXkBEuIh/LT9kRwP//8EoEB7CLr1d79brUD6n7NkjkD/XHzTvRZL9oTy/vx/OHopiqe/2UTXukG81a4i/rmy7rg7EXF+aWq56dq1K+fOneP333/H398fgGvXrtGpUycCAgL45Zdf0r3Q9JJlBhTb4iDyvD0E3Qo8d/x5DuKiUvZ6FpebrT5JdH/dHoQ8fDL2uiTHCbsex8eL9/PTFvu08QK+Hox4pDLtqhTStHERSTcZ3i115swZmjRpwuXLl6lZsyYAO3bsoGDBgoSEhGTpO29nmXCTEoYBMeH/tvr8N/jcahGKugBGQspe08MviQB0e4tQIPgEqBVIUm3z0csMnbuboxftgbxVxYJ80Kkygf65TK5MRJxBhocbgKioKH788Ud27txJrly5qFatGl27ds3S08Ahm4WblLLF/9sKdEdX2G1dYrGRKXs9iwv4FLwz+CT6sxB4+GbsdUm2cyPOxterDjNhzRHibAY+Hq680bY8z9YrjlXTxkXkPmRKuMmunDLcpNSN8Ntaf0LvMhbofMpbgdx9kwlAN1uG1AqUIx08H8Fbv+1i28lrADxQLDcfP1GNcgUViEUkbTIk3MyfP5927drh5ubG/Pnzkz32kUceSXm1mSxHh5uUsMXbu7lutfokOSboHMSmcEacxWpvBUoq+Nz+p1qBnE5CgsGMzSf49Oa0cTcXCy82Lc1LzTVtXERSL0PCjdVqJTQ0lICAAKzJrM1isViw2WypqzgTKdykk5iIewegyPNgpPB7wdEKlMRU+Ft/egeASw6djZeNnQu7znvz/mH5vpvTxgt48/Hj1ahbMq/JlYlIdqJuqWQo3GSiBJt9yvvt3V//HQwdcc4+aDolbm8FcrQAJdEl5qnPNasxDIPFe0IZNv8fLkbEANC1bjHealdB08ZFJEUyNNzExcXRtm1bJk6cSNmyZe+rUDMo3GRBMZFJTIX/z5igiNBUtAL5JO7++u+aQL6B9pCkVqBMFxYdx8dL9vHTllMABNycNt5W08ZF5B4yvOWmQIEC/Pnnnwo3knkSbBB18c7ur0RdYucgJixlr2ex2ru5EnV/FQK/IlCqmT0ESYbZdPQyb8/ZzdFL9mnjD1UqyAePVqGQv6fJlYlIVpXh4eaVV17Bw8ODjz/+OM1FmkXhxsnFRt19KrxjLFCofXXou7G6QsVHoN7zEFQP1KKQIW7E2fhq1WEmrD5CfIJ92vibbcvTTdPGRSQJGR5uBg4cyPTp0ylbtiy1atXC2zvxvY/GjBmT2pfMNAo3QkKCvRXojgAUChf3w5m//j02sDrUewEqPw5ualXICAdCI3hrzi6235w2Xqt4Hj5+vCplNW1cRG6T4eGmefPmyT6/atWq1L5kplG4kXsK3Q2bJ8Hu2RB/w77PKx/Ueg7q9FGXVQawJRjM2HSCT5fsJyrWZp823qwMwc1L4+GqaeMiotlSyVK4kRSLugzbvoetk/+9uam6rDLU2WvXef/3PSzfdwGA0gW8+fiJatQpoWnjIjldan5/333BmmT07t2biIg7F3GLioqid+/eaXlJkazHOx80fhUG7YTO06F4I/tYnX/mwJQ28E1T2DET4m6YXanTKJw7F9/2qM1XzzxAfh8PjlyM4qmJG3l77m7Cb8SZXZ6IZBNparlxcXHh3LlzBAQEJNp/6dIlChUqRHx8MoM1TaaWG7kv6rLKNGHRcYxavI9ZW/+dNj7y0cq0rRJocmUiYoYM65YKDw/HMAzy5MnDoUOHKFCggOM5m83GH3/8wVtvvcXZs2fTXn0GU7iRdHHXLquO9gHI6rJKNxuPXObtubs5dnPaeOtKBRmpaeMiOU6GhRur1ZrsQlsWi4URI0bwzjvvpLzaTKZwI+nKFg8HFtpbc05s+He/ZlmlqxtxNsavPMzENfZp474errzRrgLd6hbTtHGRHCLDws2aNWswDIMWLVrw22+/kTfvv4P83N3dKV68OIULZ+1meYUbyTDqsspw+0PDeeu33ew4dQ2A2sXzMErTxkVyhAyfLXXixAmKFSuWLZdLV7iRDBd9xd5lteU7dVllAFuCwfSNx/ls6QGib04bf6lZGV7StHERp5YpU8HXrVvHpEmTOHr0KLNnz6ZIkSL88MMPlCxZkgcffDBNhWcGhRvJNI4uq2/gxPp/96vLKl2cuXad9+btYeV++7TxMgE+fPx4VWpr2riIU8rwqeC//fYbbdq0IVeuXGzbto2YGPtdfsPCwvjoo4/S8pIizsfFFSo9Cs8thBfWQ83u4OoJ53bCvBfh80qw4gP7vbEk1YrkzsXknrUZ17Um+X3cOXwhkicnbuSdubsJi9a0cZGcLE0tNzVr1uSVV16hR48e+Pr6snPnTkqVKsX27dtp164doaGhGVFrulDLjZgqqS4riwtUekRdVvfhWnQsHy3axy9/2b+mub3cGNyyLN3qF8fNJU3/hxORLCbDu6W8vLzYu3cvJUqUSBRujh49SqVKlbhxI+suaqZwI1mCuqwyxJ9HLjF8/j8cPB8J2Fc4fqdDRZqXD8iWYwRF5F8Z3i1VqFAhDh8+fMf+9evXU6pUqbS8pEjOoi6rDNGwdH4WvdyYDztVIa+3O0cuRtF72l/0mLKFA6F3rqouIs4pTeGmX79+DBo0iM2bN2OxWDh79iw//vgjQ4YM4cUXX0zvGkWcW6Gq8Oh4eHUftBoOfkUh+jKsGw2fV4HZveDkJshZt4FLM1cXK8/WL87q15vxfJNSuLtYWXfoEu2+WMvbc3dzKTLG7BJFJIOlqVvKMAw++ugjRo0aRXR0NAAeHh4MGTKEDz74IN2LTE/qlpIszxYPBxbdXBjwP11WdZ+HKk+oyyoVTlyO4uPF+1m8xz4W0NfDleAWZXiuUQlNHRfJRjLtruCxsbEcPnyYyMhIKlWqhI+PT1pfKtMo3Ei2ooUB083mo5f5YOFe9pwJByAoby6GtqtIuyqFNB5HJBvIsHCT0jt+T5kyJaUvmekUbiRb0iyrdJGQYDBn+xk+W7qf8+H27qm6JfLy3sOVqFrU3+TqRCQ5GXpvqeLFi1OzZk2SO23u3LkprzaTKdxItqYuq3QRHRvPxDVH+WbtEW7EJQDwxANFeaNteQr66esnkhVlWLgJDg7mp59+onjx4jz33HM8++yzie4vlR0o3IjTUJfVfTt77TqfLT3A3O1nAMjl5sILTUvTv0kpcrlrPI5IVpKhY25iYmKYM2cOU6ZM4c8//6RDhw706dOH1q1bZ4t+a4UbcTrqsrpvO05d44MFe/n7xFUAAv09eaNteR6tXkR3HRfJIjJtQPGJEyeYNm0a06dPJz4+nn/++SfLDypWuBGndbcuq0LV7CFHXVbJMgyDBbvO8fHi/Zy5dh2A6kX9ee/hSrpflUgWkOGL+DlOtlqxWCwYhoHNZruflxKR++Xiam+t+e/CgKG74PeXbi4MOBLCzphdaZZksVjoWL0wK15ryuttyuPt7sLO02E8OXEjwTO3cepKtNklikgK3Ve31Pr163n44Yd57rnnaNu2LVZr1r+Hi1puJEdRl1WaXYi4wZhlB/n5r1MYBri7WunzYElealYaX083s8sTyXEyrFvqpZdeYtasWQQFBdG7d2+6detG/vz577vgzKRwIzmSuqzSbO/ZcD5cuJc/j1wGIL+PO6+1Lk/n2kG4aDyOSKbJ0KngxYoVo2bNmskOHp4zZ07Kq81kCjeS44XugS2TYNcv/5ll1Qtq9wH/IqaWlxUZhsHyfRf4aNE+jl2KAqBCIV/ee7gSjcpkr//giWRXGRZuevXqlaIZUVOnTk3pS2Y6hRuRm9RllWqx8Qn8sOkEXyw/SPiNeABaVQzg7fYVKVUga0+mEMnuMm22VHakcCPyH+qySrWrUbF8seIQP2w6gS3BwNVqoXuD4gxqWZbcXu5mlyfilBRukqFwI5IMdVmlyuELkXy0aB8r918AwD+XG4NbleXZ+sVxc8n6EyxEshOFm2Qo3IikwK0uq62TIeyUfZ+6rO5q3aGLfLhgHwfORwBQqoA377SvSIsKAdlicVOR7EDhJhkKNyKpoC6rFIu3JfDzX6cYs+wgl6NiAXiwTH7efbgiFQrp3xqR+6VwkwyFG5E0UpdVioTfiOOrVYeZuv44sbYErBboUqcYr7UuR34fD7PLE8m2FG6SoXAjcp+ir8C26bD1uzu7rOo+D8Xqq8sKOHk5mo+X7GPR7lAAfDxcCW5ehucalcDTTTflFEkthZtkKNyIpBN1WaXIlmNX+GDBXnafCQMgKG8u3mpbkfZVC2k8jkgqZNq9pe7X2rVr6dixI4ULF8ZisTBv3rx7nvPjjz9SvXp1vLy8CAwMpHfv3ly+fDnjixWRxBLdy2oDPNBD97JKQt2Sefk9uBH/e6o6Bf08OHXlOsEzt9F50kZ2nb5mdnkiTsnUcBMVFUX16tX56quvUnT8hg0b6NGjB3369OGff/5h9uzZbNmyhX79+mVwpSKSrEJV4JFx8Oo+aDUC/IMg+jKs+x+MrQqze8GJjZCzGoodrFYLT9QqyqohzRjUsiyebla2Hr/KI+M38OrPOzgXdt3sEkWcSpbplrJYLMydO5dOnTrd9ZjRo0czYcIEjhw54tg3btw4PvnkE06fPp2i91G3lEgmuNVlteUbOL7u3/3qsgLgXNh1PltygDnb7a1anm5Wnm9SmueblsLL3dXk6kSypmzTLZVaDRo04NSpUyxatAjDMDh//jy//vor7du3v+s5MTExhIeHJ3qISAa71WXVa4G6rJIQ6J+LMV1q8HtwI2oXz8ONuAS+WHGIFqPX8Nvfp0lIyBL/5xTJtrJVyw3A7Nmz6d27Nzdu3CA+Pp6OHTvy22+/4ebmluTxw4cPZ8SIEXfsV8uNSCa72yyrih3trTk5dJaVYRgs2h3KqMX7OH3V3j1Vrag/7z1ciTol8ppcnUjWkS1nS6Uk3Ozdu5dWrVrxyiuv0KZNG86dO8frr79OnTp1mDx5cpLnxMTEEBMT49gODw8nKChI4UbELOqyStKNOBtTNxznq1WHiYyx35SzfdVCDG1XkaC8XiZXJ2I+pw033bt358aNG8yePduxb/369TRu3JizZ88SGBh4z/fRmBuRLEQLA97hYkQMY0IO8vPWkyQY4O5ipfeDJQluXhpfz6RbqEVyAqcdcxMdHY3VmrhkFxf7YlhZJKOJSGrca5bVLz1z3CyrAr4ejHq8KgtfbsyDZfITa0tg4pojNB+9mpmbT2LTeByRezK15SYyMpLDhw8DULNmTcaMGUPz5s3JmzcvxYoVY+jQoZw5c4bp06cDMG3aNPr168eXX37p6JYaPHgwVquVzZs3p+g91XIjkoUl22X1PFR5Mkd1WRmGwcr9F/i/hfs4eikKgAqFfHm3QyUeLJvf5OpEMle26ZZavXo1zZs3v2N/z549mTZtGr169eL48eOsXr3a8dy4ceOYOHEix44dI3fu3LRo0YJPPvmEIkVS1nytcCOSTajLyiHOlsCMTScYu/wQYdfjAGhZIYC3O1SkdAEfk6sTyRzZJtyYQeFGJJvRLCuHa9GxfLHiED9sPEF8goGr1cKz9YszuFVZcnu5m12eSIZSuEmGwo1INqUuK4cjFyMZtWgfy/ddAMA/lxuDWpale4PiuLlkq6GUIimmcJMMhRsRJ6AuKwDWH7rEhwv3sj80AoBS+b15u31FWlYM0E05xeko3CRD4UbEidyty+qB7tD0LfC79/IQ2Z0tweCXv07xv2UHuBQZC0CjMvl4t0MlKgbq3zhxHgo3yVC4EXFCtng4uBg2TYQT6+37XHNB/Reg0SDIlcfc+jJBxI04vlp1hCnrjxFrS8BqgS51gnj1ofIU8PUwuzyR+6ZwkwyFGxEnd2IjLB8Gp24uD+GZGx58xT4uxy2XqaVlhlNXovl48X4W7j4HgI+HKy81L03vRiXxdHMxuTqRtFO4SYbCjUgOYBhwcAksHwEX99n3+RaGZm9BjW72G3s6ua3Hr/DBgr3sOh0GQNE8uXirXQU6VA3UeBzJlhRukqFwI5KDJNhg18+w6qN/x+TkKwst34OKjzj9FPKEBIN5O87w6ZIDhIbbB17XLp6H9x6uRPWg3OYWJ5JKCjfJULgRyYHiY2DrZFg32n57B4DCD0Cr4VCqqamlZYbo2Hi+WXuUSWuOcj3OBsBjNYvwRtvyBPo7f1edOAeFm2Qo3IjkYDfCYeN4+HM8xNlvZ0DpFtByGBSuYWppmSE07AafLt3PnG1nAPB0s9K/SWleaFoKL3fn76qT7E3hJhkKNyJC5AVYOxr+mgIJ9tsZUPlxaPEu5Cttbm2ZYNfpa3ywYC9bj18FoKCfB6+3qcDjNYtgtTp3V51kXwo3yVC4ERGHK8fs43F2zwYMsLrCAz2g6ZvgW8js6jKUYRgs3hPKqMX7OHXlOgBVi/jz3sOVqFsyr8nVidxJ4SYZCjcicofQ3bBiJBxaZt92zQX1X7y5Rk5uU0vLaDfibEz78zjjVx4mMiYegHZVCjG0XUWK5fMyuTqRfyncJEPhRkTu6vgGWD4cTm+xb3vmhsavQd1+Tr9GzqXIGMaEHGTWlpMkGODuYuW5RiUIblEGP083s8sTUbhJjsKNiCTLMOw36FwxEi7ut+/zK2JfI6f6M06/Rs7+0HD+b+E+1h26BEA+b3deeagcT9cJwlU35RQTKdwkQ+FGRFIkwQY7Z9nH5ISftu/LXw5avAcVOzr1GjmGYbDqwAU+XLiPoxfts8rKFfTh3Q6VaFKugMnVSU6lcJMMhRsRSZW4G/DXZPvsqutX7PuK1LKvkVOyiamlZbQ4WwI/bjrB2BWHuBZtn1XWvHwB3ulQiTIBPiZXJzmNwk0yFG5EJE1uhNnXx9n41W1r5LSEVsMgsLq5tWWwa9GxfLniMNM3Hic+wcDFauHZesUY3KocebzdzS5PcgiFm2Qo3IjIfYm8AGs+hb+nQoJ9dhFVnoDm7zj9GjlHL0by0aL9LN93HgA/T1deblmWHg1K4O6q8TiSsRRukqFwIyLp4srR29bIwb5GTq1e0OQN8C1oamkZbcPhS3ywYC/7QyMAKJnfm6HtKvBQpYK6KadkGIWbZCjciEi6OrfLPrPqcIh9280L6r8EjV4GT39za8tAtgSD2X+dYvSyg1yKjAGgYel8vNuhEpUK699WSX8KN8lQuBGRDHF8PYQMgzN/2bdz5bGvkVOnH7h5mltbBoqMiefrVYf5bv0xYuMTsFigc60gXmtTjgBf571uyXwKN8lQuBGRDGMYsH+hvSXn0gH7Pr8i0GwoVO/q1GvknLoSzSdL9rNg1zkAvN1deKl5Gfo8WBJPNxeTqxNnoHCTDIUbEclwtnjYNQtWjbptjZzy0PJ9qNDBqdfI+fvEFUYu2MfOU9cAKJI7F2+2q0DHaoEajyP3ReEmGQo3IpJp4m7A1m9h3f/guv0O3BStY18jp8SDppaWkRISDH7feYZPlxzgXNgNAB4olpv3Hq5EzWJ5TK5OsiuFm2Qo3IhIprsRBhu+hE1fQ1y0fV+Zh+wtOYHVzK0tA12PtfHN2qNMXHOE63E2AB6tUZg321agcG7nvleXpD+Fm2Qo3IiIaSJC7WvkbPv+3zVyqj5lXyMnb0lza8tAoWE3+GzpAX7bZu+i83C18nyTUjzftDTeHs47DknSl8JNMhRuRMR0l4/Aqv+DPb/Zt62uUOs5aPoG+ASYW1sG2n06jA8W7GXLcfttLAJ8PXi9TXmeeKAoVqvG40jyFG6SoXAjIlnGuZ2wfAQcWWHfdvOGBsHQcCB4Oue/T4ZhsGRPKKMW7+fkFXsXXZUifrzXoRL1SuUzuTrJyhRukqFwIyJZzrG1sHw4nPnbvp0rLzQZArX7OO0aOTHxNqZtOM74lYeJiLF30bWtXIih7StQPJ+3ydVJVqRwkwyFGxHJkgwD9v1hXyPn8iH7Pv+gm2vkPA1W51wr5lJkDJ+HHOSnLSdJMMDdxUrPhsV5oWlp8vl4mF2eZCEKN8lQuBGRLM0WDztn2tfIiThr31eggn1mVfn2TrtGzoHQCD5cuJd1hy4B4OXuQo8GJejXuKRCjgAKN8lSuBGRbCHuOmy5uUbOjWv2fUXr3lwjp5GZlWUYwzBYffAiY5YdZPeZMEAhR/6lcJMMhRsRyVauX4MNX8CmCRB/3b6vbGtoOQwKVTG1tIxiGAYr919g7PJDd4Sc/k1Kkdfb3eQKxQwKN8lQuBGRbCkiFNZ8An9/D4YNsEC1ztD8bchTwuzqMoRhGKzYd4GxKw6y50w4YA85PRuWoF9jhZycRuEmGQo3IpKtXT4CKz+Ef+bYt61uULs3NHkdfAqYW1sGUcgRULhJlsKNiDiFs9vtM6uOrLRvu3lDwwHQYIBTr5Hz35DjfTPk9FXIcXoKN8lQuBERp3J0jX2NnLPb7Nte+aDxEKjTB1ydcwCuYRgs33eBscsP8s9ZhZycQuEmGQo3IuJ0DAP2/g4rP4DLh+37/IvZx+NU6+y0a+QkF3L6NS5FHoUcp6JwkwyFGxFxWrZ42DEDVn8MEefs+wIq2dfIKdfWadfIuVvI6dWoBH0fVMhxFgo3yVC4ERGnF3cdNk+C9WPghn0qNUH17WvkFG9gamkZyTAMQvaeZ+zyQ+w9p5DjbBRukqFwIyI5xvWrN9fImfjvGjnl2tpbcgpWNre2DKSQ45wUbpKhcCMiOU74OVjzMWz74bY1crrcXCOnuNnVZZikQo6Phyu9Gpagb+OS5PZSyMlOFG6SoXAjIjnWpcP2Qcd759m3rW72WVWNhzjtGjlgDznLboacfQo52ZbCTTIUbkQkxzuzDVaMgKOr7dvuPvb1cRoOAA9fU0vLSAkJBiH7FHKyK4WbZCjciIjcdGSVfY2cczvs21757Ssd137OadfIAXvIWbb3PF+sSBxynmtUgj4PKuRkVQo3yVC4ERG5TUIC7PsdVnwAV47Y9+UuBs3fgapPOe0aOfBvyBm7/CD7QyMAhZysTOEmGQo3IiJJsMXB9ptr5ESG2vcFVIZWw+x3IXfSNXIg6ZDjezPk9FbIyTIUbpKhcCMikozYaNg8EdaPhZiba+QUa2BfI6dYfTMry3D2kBPK2OWH7gg5fR4shb+Xm8kV5mwKN8lQuBERSYHoK7BhrH0xwPgb9n3l2t1cI6eSqaVlNIWcrCk1v7+tmVRTktauXUvHjh0pXLgwFouFefPm3fOcmJgY3nnnHYoXL46HhwclSpRgypQpGV+siEhO4pUXHhoJA7fBAz3B4gIHF8OEhjD3Rbh20uwKM4zVaqFtlUAWvdyYic8+QIVCvkTExPPlysM8+MlKxoQcJCw6zuwyJRmmhpuoqCiqV6/OV199leJzOnfuzIoVK5g8eTIHDhzgp59+onz58hlYpYhIDuZfBB75El7aBBUfAQzYORPG1YIlQyHqktkVZpjbQ86EbreFnBWHFHKyuCzTLWWxWJg7dy6dOnW66zFLlizh6aef5ujRo+TNmzdN76NuKRGR+3Dmb/v08WNr7dvuvtBwIDQIBg8fU0vLaAkJBkv/CeWLFf/prnqwJH0alVR3VQbLNt1SqTV//nxq167Np59+SpEiRShXrhxDhgzh+vXrdz0nJiaG8PDwRA8REUmjIrWgx3zoPhcCq0NsBKz+CL6sAZu/gfhYsyvMMFarhXZV/23JKV/wtpacT1fyechBwq6rJScryFbh5ujRo6xfv549e/Ywd+5cxo4dy6+//spLL71013NGjRqFv7+/4xEUFJSJFYuIOCGLBUq3gH6r4ckpkLcURF2Exa/D+Nqw6xf7+jlO6lbIWTyoMV/fCjk34vniZneVQo75slW3VOvWrVm3bh2hoaH4+/sDMGfOHJ588kmioqLIlSvXHefExMQQExPj2A4PDycoKEjdUiIi6cUWB9umw5pPIPK8fV/BKtByGJR9yKnXyAF7d9WSf0L5YvkhDpy/2V3l6UqfB0vyXKOS+OdSd1V6cNpuqcDAQIoUKeIINgAVK1bEMAxOnz6d5DkeHh74+fkleoiISDpyuXkDzpe326eKe/jD+T0w8ymY1gFObTG7wgxltVpof7Ml56tnHqBcQR8ibsQzdrm9JWfscrXkZLZsFW4aNWrE2bNniYyMdOw7ePAgVquVokWLmliZiIjg7g2NX4NBO6Dhy+DiASc2wOSH4Kdn4MJ+syvMUFarhQ7VAlkyqIlCjslM7ZaKjIzk8OHDANSsWZMxY8bQvHlz8ubNS7FixRg6dChnzpxh+vTpjuMrVqxI/fr1GTFiBJcuXaJv3740bdqUb7/9NkXvqdlSIiKZJOwMrB4FO34EIwEsVqjeFZoNhdzOP/4xIcFg8Z5QvlhxkIPn7f8p9/N0pc+DpXjuwRL4eaq7KjWyzQrFq1evpnnz5nfs79mzJ9OmTaNXr14cP36c1atXO57bv38/AwcOZMOGDeTLl4/OnTvz4YcfJjneJikKNyIimeziQVg5Evb9Yd92cYc6/eytPN75zK0tEyQkGCzac44vlh/i0AWFnLTKNuHGDAo3IiImOf2XfY2c4+vs2x5+9u6r+i86/Ro5oJBzvxRukqFwIyJiIsOAIyvsISd0t32fdwA0fcN+mwdX578D991CTt/GpejVSCHnbhRukqFwIyKSBSQkwD9zYOWHcPWYfV+eEtD8XajyBFiz1XyXNLElGCzafY4vVhzisELOPSncJEPhRkQkC4mPhe3TYfUnEHXBvq9QVWg1HEq3dPo1ckAhJ6UUbpKhcCMikgXFRsGmr2HDlxBz8zY5JZvYQ06RWqaWllmSCjn+udzo+2BJejUqgW8ODzkKN8lQuBERycKir8C6/8GWb8B28z5VlR+DFu9BvtLm1pZJbAkGC3ef40uFnEQUbpKhcCMikg1cOwmrPoKdswADrK5Qqxc0eQN8C5pdXaa4W8jp17gkPRvmvJCjcJMMhRsRkWwkdA+sGAGHltm33byh4QBoMAA8c8a/4bdCzhfLD3LkYhSQM0OOwk0yFG5ERLKhY+tg+TA487d92yu/ffp4redyxPRxsIecBbvO8uWKQzky5CjcJEPhRkQkmzIM2DcfVoyEy/Zb95C7uP1mnZUfzxHTxyHpkJPby41+jUvRo0Fxpw05CjfJULgREcnmbHGwfYb9vlWR5+37ClWDh0ZA6Rbm1paJboWcL1Yc4mgOCDkKN8lQuBERcRKxUbBpAmz44rbp403tIadwTXNry0TJhZyeDUvg4+FqcoXpQ+EmGQo3IiJOJuqyffr41m9vmz7+OLR4N8dMHwfnDzkKN8lQuBERcVJXT9inj+/6mX+njz9nH3jsE2B2dZnGlmDwx077mJyjl5wn5CjcJEPhRkTEyYXuhuUj4HCIfdvNGxoOtE8h9/A1t7ZMlFTIyePlRr8mpejRIPuFHIWbZCjciIjkEMfWQsgwOLvNvu2VH5q+aV8MMIdMHwfnCTkKN8lQuBERyUEMA/b+bp8+fuWIfV+eEvbbOeSg6eMA8bYE/th1li9XHOZYNgw5CjfJULgREcmBbHGwbTqs/vjfu48HVodWI6B0c3Nry2R3Czn9m5SmR4PieGfRkKNwkwyFGxGRHCwm8t/p47ER9n2lmtvvPl64hpmVZbrsFnIUbpKhcCMiIkRdgrWjYet3kBBn31flSfv08bwlza0tk8XbEpi/8yzjVmbtkKNwkwyFGxERcbh6HFb+H+z+xb5tdYPavaHJ6+BTwNTSMtutkPPlikMcvxwNQF5vd/o3KUX3+uaHHIWbZCjciIjIHc7ttE8fP7LCvu3uAw1fhgbB4OFjbm2ZLN6WwO87zjJuZdYKOQo3yVC4ERGRuzq62j59/NwO+7Z3gX+nj7s4172a7iWrhRyFm2Qo3IiISLISEmDvPPv08avH7PvylrJPH6/UKUdNH4e7h5znm5Sie4PieLlnTshRuEmGwo2IiKSILQ7+ngZrPoGoi/Z9hWvap4+XampqaWaItyUw72bIOXEz5OS71ZKTCSFH4SYZCjciIpIqMZGw8Sv480uIjbTvK93SPn08sJqppZnhbiHn+aaleLZ+xoUchZtkKNyIiEiaRF6EtZ/BX1P+nT5etTO0eMe+6nEOk9khR+EmGQo3IiJyX64ctU8f3/OrfdvqBnX6QpMh4J3f3NpMEG9LYO72M4xbeZiTV+whJ7+PO0sHNyGfj0e6vY/CTTIUbkREJF2c3QErRsCRlfZtd19oNAgavATu3qaWZoY4WwLzboackvm9+b533XR9fYWbZCjciIhIujqyCpYPs6+VA+AdAM3ehAd65rjp42APOdei4yjgm36tNpC63985az6biIhIeivdHPqthien2MfeRF2Aha/BV/Xgn7n2O5PnIG4u1nQPNqmlcCMiInK/rFao8gQEb4X2o8ErP1w5ArN7wbct4NhasyvMURRuRERE0ourO9TtB4N2QLOh4OYNZ7fB9x1hxhMQutvsCnMEhRsREZH05uELzd6yh5y6/cHqCoeXw8TGMKc/XD1hdoVOTeFGREQko/gEQPvPIHiLvdsKA3b9DONrw5KhEHXZ7AqdksKNiIhIRstX2j7guP9qKNkUbLGw6Wv4soZ9YcDYKLMrdCoKNyIiIpmlcE3oOR+6z4VC1SAmHFZ+CF/WtK98bIszu0KnoHAjIiKS2Uq3gP5r4InJkLs4RJ6HBa/A1/Vh7+85bvp4elO4ERERMYPVClWfhAF/QbtPwSsfXD4Mv/SA71rC8fVmV5htKdyIiIiYydUd6j0PL++Apm/ap4+f+RumdYAfn4LQPWZXmO0o3IiIiGQFnn7Q/G14ebv9RpxWVzi0DCY+CHNfgGsnza4w21C4ERERyUp8C0KH/9mnj1d+DDBg508wrhYsfQeir5hdYZancCMiIpIV5SsNT02DfiuhRGP79PGN4+GL6rDufxAbbXaFWZbCjYiISFZWpBb0/AOe/Q0KVrVPH18x0j59/O9pYIs3u8IsR+FGREQkq7NYoEwreH4tPP4t5C4GkaHwxyD79PF9f2j6+G0UbkRERLILqxWqdbZPH2/7MeTKC5cPwc/PwuSH4PgGsyvMEhRuREREshtXD6j/IgzaCU3eADcvOL0VprWHmV3g/F6zKzSVwo2IiEh25ekHLd6xTx+v3QcsLnBwCUxoCHNfhGunzK7QFAo3IiIi2Z1vIXh4jH36eKVO2KePz8yx08cVbkRERJxF/jLQ+Xvoe2v6eMzN6eM1YN2YHDN93NRws3btWjp27EjhwoWxWCzMmzcvxedu2LABV1dXatSokWH1iYiIZEtFb04f7/YbFKwCMWGwYgSMewD+/t7pp4+bGm6ioqKoXr06X331VarOu3btGj169KBly5YZVJmIiEg2Z7FA2Vbw/Dp47BvwLwYR5+CPl2FCA9i3wGmnj1sMI2tcmcViYe7cuXTq1Omexz799NOULVsWFxcX5s2bx44dO1L8PuHh4fj7+xMWFoafn1/aCxYREclO4mNg62RY+xlcvzkGp2hdeGgkFG9gbm0pkJrf39luzM3UqVM5evQow4YNM7sUERGR7MPVAxq8BIN2QOMh4JoLTm+BqW1h5tNwYZ/ZFaabbBVuDh06xFtvvcWMGTNwdXVN0TkxMTGEh4cneoiIiORYnv7Q8j17yKnd++b08cX26ePzgiHstNkV3rdsE25sNhvPPPMMI0aMoFy5cik+b9SoUfj7+zseQUFBGViliIhINuFbCB7+HII3Q6VHwUiAHTPgywdg2XvZevp4thlzc+3aNfLkyYOLi4tjX0JCAoZh4OLiwrJly2jRosUd58XExBATE+PYDg8PJygoSGNuREREbnf6LwgZBifW27c9/eHBV6He8+CWy9zaSN2Ym5T17WQBfn5+7N69O9G+r7/+mpUrV/Lrr79SsmTJJM/z8PDAw8MjM0oUERHJvorWhl4L4PBye8i58A8sHwabJ0Hzt6F6V3DJHrHB1CojIyM5fPiwY/vYsWPs2LGDvHnzUqxYMYYOHcqZM2eYPn06VquVKlWqJDo/ICAAT0/PO/aLiIhIGlgsUPYhKN0Cds+GlR9C2CmYP8C+GGDLYVC+nf24LMzUMTd//fUXNWvWpGbNmgC8+uqr1KxZk/fffx+Ac+fOcfLkSTNLFBERyXmsLlD9afvdx9t8BLnywMX9MKsrTGkLJzeZXWGyssyYm8yidW5ERERS6UYYbPgCNn4N8dft+8p3gJbvQ0CFTCnBqde5ERERkUzm6W8PMi9vh1q97NPHDyy0r3T8+wAIO2N2hYko3IiIiEjK+AVCxy/gpU1QsaN9+vj2H+z3rAoZBtevml0hoHAjIiIiqVWgHHSZAX2WQ7GGEH8DNoy13318w5cQd8PU8hRuREREJG2C6sBzi+CZXyCgEty4BiHv2VtyIi+YVpbCjYiIiKSdxQLl2sAL66HTBPArCvnLgU+AaSVlj9V4REREJGuzukCNZ6Dy46aPvVG4ERERkfTj5glugaaWoG4pERERcSoKNyIiIuJUFG5ERETEqSjciIiIiFNRuBERERGnonAjIiIiTkXhRkRERJyKwo2IiIg4FYUbERERcSoKNyIiIuJUFG5ERETEqSjciIiIiFNRuBERERGnkuPuCm4YBgDh4eEmVyIiIiIpdev39q3f48nJceEmIiICgKCgIJMrERERkdSKiIjA398/2WMsRkoikBNJSEjg7Nmz+Pr6YrFY0vW1w8PDCQoK4tSpU/j5+aXra2cFzn594PzXqOvL/pz9GnV92V9GXaNhGERERFC4cGGs1uRH1eS4lhur1UrRokUz9D38/Pyc9psWnP/6wPmvUdeX/Tn7Ner6sr+MuMZ7tdjcogHFIiIi4lQUbkRERMSpKNykIw8PD4YNG4aHh4fZpWQIZ78+cP5r1PVlf85+jbq+7C8rXGOOG1AsIiIizk0tNyIiIuJUFG5ERETEqSjciIiIiFNRuBERERGnonCTSl999RUlSpTA09OTevXqsWXLlmSPnz17NhUqVMDT05OqVauyaNGiTKo0bVJzfdOmTcNisSR6eHp6ZmK1qbN27Vo6duxI4cKFsVgszJs3757nrF69mgceeAAPDw/KlCnDtGnTMrzO+5Haa1y9evUdn6HFYiE0NDRzCk6FUaNGUadOHXx9fQkICKBTp04cOHDgnudlp5/BtFxjdvo5nDBhAtWqVXMs7tagQQMWL16c7DnZ6fOD1F9jdvr8kvLxxx9jsVgYPHhwssdl9ueocJMKP//8M6+++irDhg1j27ZtVK9enTZt2nDhwoUkj//zzz/p2rUrffr0Yfv27XTq1IlOnTqxZ8+eTK48ZVJ7fWBfgfLcuXOOx4kTJzKx4tSJioqievXqfPXVVyk6/tixY3To0IHmzZuzY8cOBg8eTN++fVm6dGkGV5p2qb3GWw4cOJDocwwICMigCtNuzZo1BAcHs2nTJkJCQoiLi6N169ZERUXd9Zzs9jOYlmuE7PNzWLRoUT7++GP+/vtv/vrrL1q0aMGjjz7KP//8k+Tx2e3zg9RfI2Sfz++/tm7dyqRJk6hWrVqyx5nyORqSYnXr1jWCg4Md2zabzShcuLAxatSoJI/v3Lmz0aFDh0T76tWrZzz//PMZWmdapfb6pk6davj7+2dSdekLMObOnZvsMW+88YZRuXLlRPu6dOlitGnTJgMrSz8pucZVq1YZgHH16tVMqSk9XbhwwQCMNWvW3PWY7PYz+F8pucbs/HNoGIaRJ08e47vvvkvyuez++d2S3DVm188vIiLCKFu2rBESEmI0bdrUGDRo0F2PNeNzVMtNCsXGxvL333/TqlUrxz6r1UqrVq3YuHFjkuds3Lgx0fEAbdq0uevxZkrL9QFERkZSvHhxgoKC7vm/k+wmO31+96tGjRoEBgby0EMPsWHDBrPLSZGwsDAA8ubNe9djsvtnmJJrhOz5c2iz2Zg1axZRUVE0aNAgyWOy++eXkmuE7Pn5BQcH06FDhzs+n6SY8Tkq3KTQpUuXsNlsFCxYMNH+ggUL3nV8QmhoaKqON1Narq98+fJMmTKF33//nRkzZpCQkEDDhg05ffp0ZpSc4e72+YWHh3P9+nWTqkpfgYGBTJw4kd9++43ffvuNoKAgmjVrxrZt28wuLVkJCQkMHjyYRo0aUaVKlbsel51+Bv8rpdeY3X4Od+/ejY+PDx4eHrzwwgvMnTuXSpUqJXlsdv38UnON2e3zA5g1axbbtm1j1KhRKTrejM8xx90VXNJPgwYNEv1vpGHDhlSsWJFJkybxwQcfmFiZpFT58uUpX768Y7thw4YcOXKEzz//nB9++MHEypIXHBzMnj17WL9+vdmlZJiUXmN2+zksX748O3bsICwsjF9//ZWePXuyZs2au/7yz45Sc43Z7fM7deoUgwYNIiQkJEsPfFa4SaH8+fPj4uLC+fPnE+0/f/48hQoVSvKcQoUKpep4M6Xl+v7Lzc2NmjVrcvjw4YwoMdPd7fPz8/MjV65cJlWV8erWrZulQ8OAAQNYsGABa9eupWjRoskem51+Bm+Xmmv8r6z+c+ju7k6ZMmUAqFWrFlu3buWLL75g0qRJdxybXT+/1Fzjf2X1z+/vv//mwoULPPDAA459NpuNtWvXMn78eGJiYnBxcUl0jhmfo7qlUsjd3Z1atWqxYsUKx76EhARWrFhx177UBg0aJDoeICQkJNm+V7Ok5fr+y2azsXv3bgIDAzOqzEyVnT6/9LRjx44s+RkahsGAAQOYO3cuK1eupGTJkvc8J7t9hmm5xv/Kbj+HCQkJxMTEJPlcdvv87ia5a/yvrP75tWzZkt27d7Njxw7Ho3bt2nTr1o0dO3bcEWzApM8xw4YqO6FZs2YZHh4exrRp04y9e/ca/fv3N3Lnzm2EhoYahmEY3bt3N9566y3H8Rs2bDBcXV2N0aNHG/v27TOGDRtmuLm5Gbt37zbrEpKV2usbMWKEsXTpUuPIkSPG33//bTz99NOGp6en8c8//5h1CcmKiIgwtm/fbmzfvt0AjDFjxhjbt283Tpw4YRiGYbz11ltG9+7dHccfPXrU8PLyMl5//XVj3759xldffWW4uLgYS5YsMesS7im11/j5558b8+bNMw4dOmTs3r3bGDRokGG1Wo3ly5ebdQl39eKLLxr+/v7G6tWrjXPnzjke0dHRjmOy+89gWq4xO/0cvvXWW8aaNWuMY8eOGbt27TLeeustw2KxGMuWLTMMI/t/foaR+mvMTp/f3fx3tlRW+BwVblJp3LhxRrFixQx3d3ejbt26xqZNmxzPNW3a1OjZs2ei43/55RejXLlyhru7u1G5cmVj4cKFmVxx6qTm+gYPHuw4tmDBgkb79u2Nbdu2mVB1ytya9vzfx61r6tmzp9G0adM7zqlRo4bh7u5ulCpVypg6dWqm150aqb3GTz75xChdurTh6elp5M2b12jWrJmxcuVKc4q/h6SuC0j0mWT3n8G0XGN2+jns3bu3Ubx4ccPd3d0oUKCA0bJlS8cvfcPI/p+fYaT+GrPT53c3/w03WeFztBiGYWRcu5CIiIhI5tKYGxEREXEqCjciIiLiVBRuRERExKko3IiIiIhTUbgRERERp6JwIyIiIk5F4UZEREScisKNiORIFouFefPmmV2GiGQAhRsRyXS9evXCYrHc8Wjbtq3ZpYmIE9BdwUXEFG3btmXq1KmJ9nl4eJhUjYg4E7XciIgpPDw8KFSoUKJHnjx5AHuX0YQJE2jXrh25cuWiVKlS/Prrr4nO3717Ny1atCBXrlzky5eP/v37ExkZmeiYKVOmULlyZTw8PAgMDGTAgAGJnr906RKPPfYYXl5elC1blvnz5zueu3r1Kt26daNAgQLkypWLsmXL3hHGRCRrUrgRkSzpvffe44knnmDnzp1069aNp59+mn379gEQFRVFmzZtyJMnD1u3bmX27NksX748UXiZMGECwcHB9O/fn927dzN//nzKlCmT6D1GjBhB586d2bVrF+3bt6dbt25cuXLF8f579+5l8eLF7Nu3jwkTJpA/f/7M+wKISNpl6G05RUSS0LNnT8PFxcXw9vZO9Pi///s/wzDsd8d+4YUXEp1Tr14948UXXzQMwzC++eYbI0+ePEZkZKTj+YULFxpWq9UIDQ01DMMwChcubLzzzjt3rQEw3n33Xcd2ZGSkARiLFy82DMMwOnbsaDz33HPpc8Eikqk05kZETNG8eXMmTJiQaF/evHkdf2/QoEGi5xo0aMCOHTsA2LdvH9WrV8fb29vxfKNGjUhISODAgQNYLBbOnj1Ly5Ytk62hWrVqjr97e3vj5+fHhQsXAHjxxRd54okn2LZtG61bt6ZTp040bNgwTdcqIplL4UZETOHt7X1HN1F6yZUrV4qOc3NzS7RtsVhISEgAoF27dpw4cYJFixYREhJCy5YtCQ4OZvTo0eler4ikL425EZEsadOmTXdsV6xYEYCKFSuyc+dOoqKiHM9v2LABq9VK+fLl8fX1pUSJEqxYseK+aihQoAA9e/ZkxowZjB07lm+++ea+Xk9EModabkTEFDExMYSGhiba5+rq6hi0O3v2bGrXrs2DDz7Ijz/+yJYtW5g8eTIA3bp1Y9iwYfTs2ZPhw4dz8eJFBg4cSPfu3SlYsCAAw4cP54UXXiAgIIB27doRERHBhg0bGDhwYIrqe//996lVqxaVK1cmJiaGBQsWOMKViGRtCjciYoolS5YQGBiYaF/58uXZv38/YJ/JNGvWLF566SUCAwP56aefqFSpEgBeXl4sXbqUQYMGUadOHby8vHjiiScYM2aM47V69uzJjRs3+PzzzxkyZAj58+fnySefTHF97u7uDB06lOPHj5MrVy4aN27MrFmz0uHKRSSjWQzDMMwuQkTkdhaLhblz59KpUyezSxGRbEhjbkRERMSpKNyIiIiIU9GYGxHJctRbLiL3Qy03IiIi4lQUbkRERMSpKNyIiIiIU1G4EREREaeicCMiIiJOReFGREREnIrCjYiIiDgVhRsRERFxKgo3IiIi4lT+H1nZURZUtFNCAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423/423 [==============================] - 1s 3ms/step - loss: 1.2941 - accuracy: 0.7677\n",
            "Test Accuracy: 76.77%\n",
            "Test Loss: 1.2941\n",
            "423/423 [==============================] - 1s 2ms/step\n",
            "Metrics Table:\n",
            "      Metric     Value\n",
            "0   Accuracy  0.767655\n",
            "1  Precision  0.589294\n",
            "2     Recall  0.767655\n",
            "3   F1-Score  0.666753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x786dc82b0a90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the model, training, validation and testing datasets"
      ],
      "metadata": {
        "id": "2BOyke0IGXPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "final_model.save('final_model.h5')\n",
        "\n",
        "# Merge features and labels for each dataset\n",
        "train_data = x_train.copy()\n",
        "train_data['status'] = y_train\n",
        "val_data = x_val.copy()\n",
        "val_data['status'] = y_val\n",
        "test_data = x_test.copy()\n",
        "test_data['status'] = y_test\n",
        "\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "val_data.to_csv('val_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLADPAvZIKCu",
        "outputId": "03bcf692-cb7e-4b02-e013-84c3f51fe4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}